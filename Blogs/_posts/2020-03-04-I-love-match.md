---
layout: post
title: I Love Learning and Applying Mathematics, Statistics!
description: >
  
#image: /assets/img/blog/steve-harvey.jpg
comment: true
---



0. [Linear algebra](#linear-algebra)
0. [Probabilistic view of the world](#probabilistic-view-of-the-world)
0. [Optimisation](#optimisation)
{:.message}


### Linear algebra
* [Properties of the Covariance Matrix](http://www.robots.ox.ac.uk/~davidc/pubs/tt2015_dac1.pdf)
* [Positive semi-definite]()
* [Eigen vectors and Diagonalisation]()
* [Eigen values and Determinant]()
{:.message}

### Probabilistic view of the world
* [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance)
    * Mahalanobis distance is proportional, for a normal distribution, to the square root of the negative log likelihood (after adding a constant so the minimum is at zero).
    * This intuitive approach can be made quantitative by defining the normalized distance between the test point and the set to be $${\displaystyle {x-\mu } \over \sigma } $$ . By plugging this into the normal distribution we can derive the probability of the test point belonging to the set.

* [Maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)

* [Properties of the Covariance Matrix](http://www.robots.ox.ac.uk/~davidc/pubs/tt2015_dac1.pdf)

* [Differential entropy](https://en.wikipedia.org/wiki/Differential_entropy)

* [What does Determinant of Covariance Matrix give](https://math.stackexchange.com/questions/889425/what-does-determinant-of-covariance-matrix-give)

* [Why do we use the determinant of the covariance matrix when using the multivariate normal?](https://stats.stackexchange.com/questions/89952/why-do-we-use-the-determinant-of-the-covariance-matrix-when-using-the-multivaria)
{:.message}

### Optimisation
* [Concave function](https://en.wikipedia.org/wiki/Concave_function)
{:.message}