---
layout: post
title: Paper Summary on Noise, Anomalies, Adversaries
description: >
  
image: /assets/img/blog/steve-harvey.jpg
comment: true
---

:+1: means being highly related to my personal research interest. 
{:.message}


## [GAN, Adversary Examples, Adversary Machine Learning](../../my_docs/adversary.md)


## [Label Noise](../../my_docs/Label-Noise.md)


## [Stochastic-Gradient-Noise](../../my_docs/Stochastic-Gradient-Noise.md)


## [Denoiser, Noise Removal](../../my_docs/Denoiser.md)



## :+1:  [NeurIPS 2019-Noise-tolerant fair classification](https://arxiv.org/abs/1901.10837)
**NOTE**: Existing work on the problem operates **under the assumption that the sensitive feature available in one's training sample is perfectly reliable.** This assumption may be violated in many real-world cases: for example, respondents to a survey may choose to conceal or obfuscate their group identity out of fear of potential discrimination. This poses the question of whether one can still learn fair classifiers given noisy sensitive features.
{:.message}



## [NeurIPS 2019-Neural networks grown and self-organized by noise](https://arxiv.org/abs/1906.01039)
**NOTE**: **Living neural networks** emerge through a process of growth and self-organization that begins with a single cell and results in a brain, an organized and functional computational device. Artificial neural networks, however, rely on human-designed, hand-programmed architectures for their remarkable performance. **Can we develop artificial computational devices that can grow and self-organize without human intervention?** In this paper, we propose a biologically inspired developmental algorithm that can **'grow' a functional, layered neural network from a single initial cell.** The algorithm organizes inter-layer connections to construct a convolutional pooling layer, a key constituent of convolutional neural networks (CNN's). Our approach is inspired by the mechanisms employed by the early visual system to wire the retina to the lateral geniculate nucleus (LGN), days before animals open their eyes. The key ingredients for robust self-organization are an emergent spontaneous spatiotemporal activity wave in the first layer and a local learning rule in the second layer that 'learns' the underlying activity pattern in the first layer. The algorithm is adaptable to a wide-range of input-layer geometries, robust to malfunctioning units in the first layer, and so can be used to **successfully grow and self-organize pooling architectures of different pool-sizes and shapes.** The algorithm provides a primitive procedure for constructing layered neural networks through growth and self-organization. Broadly, our work shows that biologically inspired developmental algorithms can be applied to autonomously grow functional 'brains' in-silico.
{:.message}


