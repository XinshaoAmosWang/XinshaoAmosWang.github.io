<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v8.5.2 <https://hydejack.com/>
-->











<head>
  



<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">




  
<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>I Love Learning and Applying Mathematics, Statistics! | Xinshao (Amos) Wang</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="I Love Learning and Applying Mathematics, Statistics!" />
<meta name="author" content="Xinshao (Amos) Wang" />
<meta property="og:locale" content="en" />
<meta name="description" content="“Stay Hungry. Stay Foolish. – Steve Jobs 2005”. ML/DL/AI Research with applications to CV/NLP, etc" />
<meta property="og:description" content="“Stay Hungry. Stay Foolish. – Steve Jobs 2005”. ML/DL/AI Research with applications to CV/NLP, etc" />
<link rel="canonical" href="http://localhost:4000/blogs/2020-03-04-I-love-math/" />
<meta property="og:url" content="http://localhost:4000/blogs/2020-03-04-I-love-math/" />
<meta property="og:site_name" content="Xinshao (Amos) Wang" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-04T00:00:00+00:00" />
<script type="application/ld+json">
{"description":"“Stay Hungry. Stay Foolish. – Steve Jobs 2005”. ML/DL/AI Research with applications to CV/NLP, etc","author":{"@type":"Person","name":"Xinshao (Amos) Wang"},"@type":"BlogPosting","url":"http://localhost:4000/blogs/2020-03-04-I-love-math/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/icons/android-chrome-192x192.png"},"name":"Xinshao (Amos) Wang"},"headline":"I Love Learning and Applying Mathematics, Statistics!","dateModified":"2020-03-04T00:00:00+00:00","datePublished":"2020-03-04T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blogs/2020-03-04-I-love-math/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  

  
    <meta name="keywords" content="Xinshao (Amos) Wang; Machine Learning,Computer Vision,Robust Learning,Deep Metric Learning,Image Recognition,Video Recognition,Person ReID">
  


<meta name="mobile-web-app-capable" content="yes">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="Xinshao (Amos) Wang">
<meta name="apple-mobile-web-app-status-bar-style" content="black">

<meta name="application-name" content="Xinshao (Amos) Wang">
<meta name="msapplication-config" content="/assets/ieconfig.xml">


<meta name="theme-color" content="rgb(25,55,71)">


<meta name="generator" content="Hydejack v8.5.2" />

<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Xinshao (Amos) Wang" />



<link rel="alternate" href="http://localhost:4000/blogs/2020-03-04-I-love-math/" hreflang="en">

<link rel="shortcut icon" href="/assets/icons/icon.png">
<link rel="apple-touch-icon" href="/assets/icons/icon.png">

<link rel="manifest" href="/assets/manifest.json">


  <link rel="dns-prefetch" href="https://fonts.googleapis.com">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">




<link rel="dns-prefetch" href="/" id="_baseURL">
<link rel="dns-prefetch" href="/sw.js" id="_hrefSW">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/katex.min.js" id="_hrefKatexJS">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/katex.min.css" id="_hrefKatexCSS">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/contrib/copy-tex.min.js" id="_hrefKatexCopyJS">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/contrib/copy-tex.min.css" id="_hrefKatexCopyCSS">
<link rel="dns-prefetch" href="/assets/img/swipe.svg" id="_hrefSwipeSVG">




<script>
!function(e,t){"use strict";function n(e,t,n,o){e.addEventListener?e.addEventListener(t,n,o):e.attachEvent?e.attachEvent("on"+t,n):e["on"+t]=n}e.loadJS=function(e,o){var r=t.createElement("script");r.src=e,o&&n(r,"load",o,{once:!0});var a=t.scripts[0];return a.parentNode.insertBefore(r,a),r},e._loaded=!1,e.loadJSDeferred=function(o,r){function a(){e._loaded=!0,r&&n(c,"load",r,{once:!0});var o=t.scripts[0];o.parentNode.insertBefore(c,o)}var c=t.createElement("script");return c.src=o,e._loaded?a():n(e,"load",a,{once:!0}),c},e.setRel=e.setRelStylesheet=function(e){function o(){this.rel="stylesheet"}n(t.getElementById(e),"load",o,{once:!0})}}(window,document);
;
!function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this);
;
!function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this);
;
!function(w, d) {
  w._noPushState = false;
  w._noDrawer = false;
}(window, document);
</script>

<!--[if gt IE 8]><!---->











  <link rel="stylesheet" href="/assets/css/hydejack-8.5.2.css">
  <link rel="stylesheet" href="/assets/icomoon/style.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:400|Noto+Sans:400,400i,700,700i&display=swap">
  


  <style id="_pageStyle">

.content a:not(.btn){color:#4fb1ba;border-color:rgba(79,177,186,0.2)}.content a:not(.btn):hover{border-color:#4fb1ba}:focus{outline-color:#4fb1ba !important}.btn-primary{color:#fff;background-color:#4fb1ba;border-color:#4fb1ba}.btn-primary:focus,.btn-primary.focus,.form-control:focus,.form-control.focus{box-shadow:0 0 0 3px rgba(79,177,186,0.5)}.btn-primary:hover,.btn-primary.hover{color:#fff;background-color:#409ba3;border-color:#409ba3}.btn-primary:disabled,.btn-primary.disabled{color:#fff;background-color:#4fb1ba;border-color:#4fb1ba}.btn-primary:active,.btn-primary.active{color:#fff;background-color:#409ba3;border-color:#409ba3}::selection{color:#fff;background:#4fb1ba}::-moz-selection{color:#fff;background:#4fb1ba}

</style>


<!--<![endif]-->





  
<script data-ad-client="ca-pub-8231481254980115" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script data-ad-client="ca-pub-8231481254980115" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
</head>

<body class="no-color-transition">
  <div id="_navbar" class="navbar fixed-top">
  <div class="content">
    <div class="nav-btn-bar">
      <span class="sr-only">Jump to:</span>
      <a id="_menu" class="nav-btn no-hover fl" href="#_navigation">
        <span class="sr-only">Navigation</span>
        <span class="icon-menu"></span>
      </a>
      <!-- <a id="_search" class="nav-btn no-hover fl" href="#_search">
        <span class="sr-only">Search</span>
        <span class="icon-search"></span>
      </a>
      <form action="https://duckduckgo.com/" method="GET">
        <div class="form-group fr">
          <label class="sr-only" for="_search">Search</label>
          <input id="_search" name="q" class="form-control" type="search" />
        </div>
        <input type="hidden" name="q" value="site:hydejack.com" />
        <input type="hidden" name="ia" value="web" />
      </form> -->
    </div>
  </div>
</div>
<hr class="sr-only" hidden />


<hy-push-state
  replace-ids="_main"
  link-selector="a[href]:not([href*='/assets/']):not(.external):not(.no-push-state)"
  duration="250"
  script-selector="script:not([type^='math/tex'])"
  prefetch
>
  
    <main
  id="_main"
  class="content fade-in layout-post"
  role="main"
  data-color="rgb(79,177,186)"
  data-theme-color="rgb(25,55,71)"
  
    data-image="/assets/img/sidebar-bg.jpg"
    data-overlay
  
  >
  




<article id="post-blogs-I-love-math" class="page post mb6" role="article">
  <header>
    <h1 class="post-title">
      
        I Love Learning and Applying Mathematics, Statistics!
      
    </h1>

    <p class="post-date heading">
      
      <time datetime="2020-03-04T00:00:00+00:00">04 Mar 2020</time>
      
      
      
      
      









in <a href="/blogs/" class="flip-title">Blogs</a>

      











    </p>

    
    

    



  <div class="hr pb0"></div>


  </header>

  
    <ol class="message">
  <li><a href="#sgd--newtons-method--second-order-derivative-optimisation">SGD &amp; Newton’s Method &amp; Second-order Derivative Optimisation</a></li>
  <li><a href="#linear-algebra">Linear algebra</a></li>
  <li><a href="#probabilistic-view-of-the-world">Probabilistic view of the world</a></li>
  <li><a href="#optimisation">Optimisation</a></li>
</ol>

<h3 id="sgd--newtons-method--second-order-derivative-optimisation">SGD &amp; Newton’s Method &amp; Second-order Derivative Optimisation</h3>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Newton%27s_method">Newton’s Method</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton’s Method: Second-order Derivative Optimisation</a></li>
</ul>

<h3 id="linear-algebra">Linear algebra</h3>
<ul class="message">
  <li><a href="http://www.robots.ox.ac.uk/~davidc/pubs/tt2015_dac1.pdf">Properties of the Covariance Matrix</a></li>
  <li><a href="https://www.mathsisfun.com/data/correlation.html">Covariance and Correlation</a>
    <ul>
      <li>Interpreting: <strong>Variable = Axis</strong>
        <ul>
          <li>Variable = axis/component/one hyper-line</li>
          <li>Observations of this variable = points of this axis/hyper-line</li>
          <li>The observations of a variable = one vector of points in this line.</li>
          <li><code class="MathJax_Preview">E(XY)=</code><script type="math/tex">E(XY)=</script> the dot product of two variables’ observation vector (multiple points for each variable)</li>
          <li>If <code class="MathJax_Preview">X,Y</code><script type="math/tex">X,Y</script> are orthogonal, then for any point in <code class="MathJax_Preview">X</code><script type="math/tex">X</script> and any point in <code class="MathJax_Preview">Y</code><script type="math/tex">Y</script>, their dot product are zero, therefore, we have <code class="MathJax_Preview">E(XY) = 0</code><script type="math/tex">E(XY) = 0</script>.</li>
          <li>Diagonalisation (Orthogonal, Making them independent) =&gt;  Decorrelation
            <ul>
              <li>In this context, uncorrelation = independence.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Uncorrelation (Orthogonalisaion) using Eigen vectors
        <ul>
          <li><strong>Projection of a normal distribtion</strong> <code class="MathJax_Preview">X \sim \mathcal{N}(0,\,{\sigma_x}^2)</code><script type="math/tex">X \sim \mathcal{N}(0,\,{\sigma_x}^2)</script> to a standard normal distribution <code class="MathJax_Preview">\mathcal{N}(0,\,1)</code><script type="math/tex">\mathcal{N}(0,\,1)</script>:
            <ul>
              <li><strong>the projected distribution</strong> is <code class="MathJax_Preview">\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)</code><script type="math/tex">\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)</script></li>
              <li><code class="MathJax_Preview">\theta</code><script type="math/tex">\theta</script> determines their linear dependency: correlation coefficient=<code class="MathJax_Preview">\cos(\theta)</code><script type="math/tex">\cos(\theta)</script>.</li>
            </ul>
          </li>
          <li>
            <p>For each eigen vector: <strong>we project all the original variables to this eigen vector</strong> (an axis in the transformed orthogonal space) =&gt; summarise/accumulate those projected variables in this axis =&gt; New variable</p>

            <ul>
              <li>
                <p>Two accumulated projected new variables of two axises (eigen vectors) are uncorrelated (being independent now).</p>
              </li>
              <li>
                <p>Eigen value = sum of projected standard deviation.</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>The number of variables, the number of observations, the number of axises/components
        <ul>
          <li>The number of variables = the number of axis.</li>
          <li>When the number of variables (feature dim) &gt; the number of observations?</li>
        </ul>
      </li>
      <li>In a high-dimensional space with orthogonal axises:
        <ul>
          <li>Each axis is one independent event/variable. (Without losing generality, feel free to treat it as an unit normal distribution <code class="MathJax_Preview">~\sim \mathcal{N}(0,\,1)</code><script type="math/tex">~\sim \mathcal{N}(0,\,1)</script>)</li>
          <li><strong>The sum of two independent normal random variables is also normal.</strong> However, if the two normal random variables are not independent, then their sum is not necessarily normal.</li>
          <li>
            <p>Then, the whole space becomes the combination (summarisation) of multiple independent normal random variables.</p>
          </li>
          <li>In this context, <strong>uncorrelation = independence.</strong>
  Independent variables = indepdent axis/components = orthogonal components.</li>
        </ul>
      </li>
      <li>Covariance:
        <ul>
          <li>remove mean for each axis/variable</li>
          <li>projection (accumulation/expectation of dot product of observations from different axis/variable)</li>
          <li>In other words, dot product of two points (dim &gt;= 2) from two axises (out of multiple axises) =&gt; expectation/accumulation</li>
        </ul>
      </li>
      <li><strong>Dot product of two variable/axis</strong></li>
      <li>Correlation = <code class="MathJax_Preview">\frac{Covariance(X_i, X_j)} { \sigma_i \times \sigma_j } \in [-1, 1]</code><script type="math/tex">\frac{Covariance(X_i, X_j)} { \sigma_i \times \sigma_j } \in [-1, 1]</script></li>
      <li>For easier and intuitive understanding, looking at <code class="MathJax_Preview">\frac{(X_i - E(X_i))} { \sigma_i } \sim \mathcal{N}(0,\,1) = \mathbf{e}_i</code><script type="math/tex">\frac{(X_i - E(X_i))} { \sigma_i } \sim \mathcal{N}(0,\,1) = \mathbf{e}_i</script> and <code class="MathJax_Preview">\frac{(X_j - E(X_j))} { \sigma_j } \sim \mathcal{N}(0,\,1) = \lambda\mathbf{e}_i + \sqrt{(1-\lambda^2)} \mathbf{e}_j</code><script type="math/tex">\frac{(X_j - E(X_j))} { \sigma_j } \sim \mathcal{N}(0,\,1) = \lambda\mathbf{e}_i + \sqrt{(1-\lambda^2)} \mathbf{e}_j</script>
        <ul>
          <li><code class="MathJax_Preview">\mathbf{e}_i \sim \mathcal{N}(0,\,1)</code><script type="math/tex">\mathbf{e}_i \sim \mathcal{N}(0,\,1)</script>, <code class="MathJax_Preview">\mathbf{e}_j \sim \mathcal{N}(0,\,1)</code><script type="math/tex">\mathbf{e}_j \sim \mathcal{N}(0,\,1)</script></li>
          <li><code class="MathJax_Preview">\mathbf{e}_i \text{ and } \mathbf{e}_j</code><script type="math/tex">\mathbf{e}_i \text{ and } \mathbf{e}_j</script> are two variables in two orthogonal axises.</li>
          <li>Correlation = <code class="MathJax_Preview">\lambda \in [-1, 1]</code><script type="math/tex">\lambda \in [-1, 1]</script></li>
          <li>Covariance = <code class="MathJax_Preview">\lambda \sigma_i \sigma_j</code><script type="math/tex">\lambda \sigma_i \sigma_j</script></li>
          <li>
            <pre class="MathJax_Preview"><code>\begin{aligned}
E\{\frac{(X_i - E(X_i))} { \sigma_i }  \frac{(X_j - E(X_j))} { \sigma_j } \} &amp;= E\{ \mathbf{e}_i \cdot \lambda\mathbf{e}_i + \sqrt{(1-\lambda^2)} \mathbf{e}_j \} \\
&amp;= \lambda E\{ \mathbf{e}_i \cdot \mathbf{e}_i  \} + \sqrt{(1-\lambda^2)} E\{ \mathbf{e}_i \cdot \mathbf{e}_j \} \\
&amp;= \lambda .
\end{aligned}</code></pre>
            <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
E\{\frac{(X_i - E(X_i))} { \sigma_i }  \frac{(X_j - E(X_j))} { \sigma_j } \} &= E\{ \mathbf{e}_i \cdot \lambda\mathbf{e}_i + \sqrt{(1-\lambda^2)} \mathbf{e}_j \} \\
&= \lambda E\{ \mathbf{e}_i \cdot \mathbf{e}_i  \} + \sqrt{(1-\lambda^2)} E\{ \mathbf{e}_i \cdot \mathbf{e}_j \} \\
&= \lambda .
\end{aligned} %]]></script>
          </li>
          <li>
            <pre class="MathJax_Preview"><code>\begin{aligned}
E\{ {(X_i - E(X_i))}  \cdot {(X_j - E(X_j))}  \} =  \lambda  \sigma_i  \sigma_j.
  \end{aligned}</code></pre>
            <script type="math/tex; mode=display">\begin{aligned}
E\{ {(X_i - E(X_i))}  \cdot {(X_j - E(X_j))}  \} =  \lambda  \sigma_i  \sigma_j.
  \end{aligned}</script>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://en.wikipedia.org/wiki/Correlation_and_dependence#Correlation_and_independence">Correlation and dependence -1</a>
    <ul>
      <li>Correlation and dependencd are totally different concepts/terms for discribing two random variables.</li>
      <li><strong>In the special case</strong> when <code class="MathJax_Preview">{\displaystyle X}</code><script type="math/tex">{\displaystyle X}</script> and <code class="MathJax_Preview">{\displaystyle Y}</code><script type="math/tex">{\displaystyle Y}</script> are jointly normal, uncorrelatedness is equivalent to independence.</li>
    </ul>
  </li>
  <li><a href="https://www.probabilitycourse.com/chapter5/5_3_2_bivariate_normal_dist.php">Correlation and dependence-2-Bivariate Normal Distribution</a>
    <ul>
      <li>
        <p>The sum of two independent normal random variables is also normal. However, <strong>if the two normal random variables are not independent, then their sum is not necessarily normal.</strong></p>
      </li>
      <li>
        <p>If <code class="MathJax_Preview">X</code><script type="math/tex">X</script> and <code class="MathJax_Preview">Y</code><script type="math/tex">Y</script> are <strong>independent</strong>, <code class="MathJax_Preview">P(Y\|X)=P(Y) =&gt; E(XY)=E(X)E(Y) =&gt; Cov(X,Y)=E((X-E(X))(Y-E(Y)))=E(XY)-E(X)E(Y) = 0 =&gt;</code><script type="math/tex">P(Y\|X)=P(Y) => E(XY)=E(X)E(Y) => Cov(X,Y)=E((X-E(X))(Y-E(Y)))=E(XY)-E(X)E(Y) = 0 =></script><strong>Uncorrelated</strong>.</p>
      </li>
      <li>
        <p>If <code class="MathJax_Preview">X</code><script type="math/tex">X</script> and <code class="MathJax_Preview">Y</code><script type="math/tex">Y</script> are uncorrelated, <code class="MathJax_Preview">Cov(X,Y)=0 =&gt; E(XY) = E(X)E(Y) =&gt; ?</code><script type="math/tex">Cov(X,Y)=0 => E(XY) = E(X)E(Y) => ?</script></p>
        <ul>
          <li><a href="http://home.iitk.ac.in/~zeeshan/pdf/The%20Bivariate%20Normal%20Distribution.pdf">Zero Correlation Implies Independence</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://www.actuaries.org.uk/system/files/documents/pdf/correlation.pdf">Why are we interested in
correlation/dependency?</a></li>
  <li>
    <p><a href="https://www.probabilitycourse.com/chapter5/5_2_3_conditioning_independence.php">Conditioning and Independence</a></p>
  </li>
  <li><a href="">Positive semi-definite</a></li>
  <li><a href="">Eigen vectors and Diagonalisation</a></li>
  <li>
    <p><a href="">Eigen values and Determinant</a></p>
  </li>
  <li><a href="">Mahalanobis distance</a>
    <ul>
      <li>Transformation/projection to the orthogonal space, where one axis is an independent normal distribution</li>
      <li>One each axis: compute the distance <code class="MathJax_Preview">\frac{(x-u)^T(x-u)}{\sigma^2}</code><script type="math/tex">\frac{(x-u)^T(x-u)}{\sigma^2}</script></li>
      <li>Summarise all the distances of all axises.</li>
    </ul>
  </li>
</ul>

<h3 id="probabilistic-view-of-the-world">Probabilistic view of the world</h3>
<ul class="message">
  <li>Basic concepts/terms
    <ul>
      <li>Covariance and correlation</li>
      <li>Bivariate Normal Distribution</li>
      <li>The sum of two <strong>independent</strong> normal distributions</li>
      <li>Distance = <code class="MathJax_Preview">num \times standard~deviation</code><script type="math/tex">num \times standard~deviation</script>
  =&gt; Mahalanobis distance</li>
      <li>Diagonalisation (Orthogonal, Making them independent) =&gt;  Decorrelation
        <ul>
          <li>In this context, uncorrelation = independence.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Uncorrelation (Orthogonalisaion) using Eigen vectors
    <ul>
      <li><strong>Projection of a normal distribtion</strong> <code class="MathJax_Preview">X \sim \mathcal{N}(0,\,{\sigma_x}^2)</code><script type="math/tex">X \sim \mathcal{N}(0,\,{\sigma_x}^2)</script> to a standard normal distribution <code class="MathJax_Preview">\mathcal{N}(0,\,1)</code><script type="math/tex">\mathcal{N}(0,\,1)</script>:
        <ul>
          <li><strong>the projected distribution</strong> is <code class="MathJax_Preview">\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)</code><script type="math/tex">\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)</script></li>
          <li><code class="MathJax_Preview">\theta</code><script type="math/tex">\theta</script> determines their linear dependency: correlation coefficient=<code class="MathJax_Preview">\cos(\theta)</code><script type="math/tex">\cos(\theta)</script>.</li>
        </ul>
      </li>
      <li>
        <p>For each eigen vector: <strong>we project all the original variables to this eigen vector</strong> (an axis in the transformed orthogonal space) =&gt; summarise/accumulate those projected variables in this axis =&gt; New variable</p>

        <ul>
          <li>
            <p>Two accumulated projected new variables of two axises (eigen vectors) are uncorrelated (being independent now).</p>
          </li>
          <li>
            <p>Eigen value = sum of projected standard deviation.</p>
          </li>
        </ul>
      </li>
      <li>Eigen vectors are orthogonal, serving as independent axises where independent variables lie in.
        <ul>
          <li>Diagonal covariance matrix: each entry is the square of eigen value.</li>
          <li>Eigen value = sum of projected standard deviation.</li>
          <li><strong>Projection of a normal distribtion</strong>
            <ul>
              <li><strong>the projected distribution</strong> is <code class="MathJax_Preview">\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)</code><script type="math/tex">\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)</script></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis distance</a>
    <ul>
      <li><a href="https://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance.html">Distance is not always what it seems</a></li>
      <li><a href="https://blogs.sas.com/content/iml/2012/02/02/detecting-outliers-in-sas-part-3-multivariate-location-and-scatter.html">Detecting outliers in SAS: Part 3: Multivariate location and scatter &amp; MCD: Robust estimation by subsampling</a></li>
      <li><a href="https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables">Sum of normally distributed random variables-Independent random variables</a></li>
      <li><a href="https://github.com.cnpmjs.org/XinshaoAmosWang/DerivativeManipulation/blob/master/OutlierDetection_RobustInference.pptx.pdf">Outlier, Anomaly, and Adversaries Detection using Mahalanobis distance</a></li>
      <li>The Mahalanobis distance is a measure of the <strong>distance between a point <code class="MathJax_Preview">\mathrm{P}</code><script type="math/tex">\mathrm{P}</script> and a distribution <code class="MathJax_Preview">\mathbf{D}</code><script type="math/tex">\mathbf{D}</script>.</strong>
  It is a multi-dimensional generalization of the idea of measuring how many standard deviations away P is from the mean of D. This distance is zero if P is at the mean of D, and grows as P moves away from the mean along <strong>each principal component axis.</strong>
        <ul>
          <li>If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space.</li>
          <li>The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.</li>
        </ul>
      </li>
      <li>Mahalanobis distance is proportional, for a normal distribution, to the square root of the negative log likelihood (after adding a constant so the minimum is at zero).</li>
      <li>This intuitive approach can be made quantitative by defining <strong>the normalized distance between the test point and the set to be <code class="MathJax_Preview">{\displaystyle {x-\mu } \over \sigma }</code><script type="math/tex">{\displaystyle {x-\mu } \over \sigma }</script></strong>. By plugging this into the normal distribution we can derive the probability of the test point belonging to the set.</li>
      <li>Were the distribution to be decidedly non-spherical, for instance ellipsoidal, then we would expect the probability of the test point belonging to the set to depend not only on the distance from the center of mass, but also on the direction. In those directions where the ellipsoid has a short axis the test point must be closer, while in those where the axis is long the test point can be further away from the center.</li>
      <li>The Mahalanobis distance is the distance of the test point from the center of mass <strong>divided by the width of the ellipsoid in the direction of the test point.</strong> (distance normalisation)</li>
    </ul>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum likelihood estimation</a></p>
  </li>
  <li>
    <p><a href="http://www.robots.ox.ac.uk/~davidc/pubs/tt2015_dac1.pdf">Properties of the Covariance Matrix</a></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Differential_entropy">Differential entropy</a></p>
  </li>
  <li>
    <p><a href="https://math.stackexchange.com/questions/889425/what-does-determinant-of-covariance-matrix-give">What does Determinant of Covariance Matrix give</a></p>
  </li>
  <li><a href="https://stats.stackexchange.com/questions/89952/why-do-we-use-the-determinant-of-the-covariance-matrix-when-using-the-multivaria">Why do we use the determinant of the covariance matrix when using the multivariate normal?</a></li>
</ul>

<h3 id="optimisation">Optimisation</h3>
<ul class="message">
  <li><a href="https://en.wikipedia.org/wiki/Concave_function">Concave function</a></li>
</ul>

  
</article>



<hr class="dingbat related" />










<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    },
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML">
</script>




<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <ins class="adsbygoogle"
      style="display:block"
      data-ad-format="fluid"
      data-ad-layout-key="-ef+6k-30-ac+ty"
      data-ad-client="ca-pub-8231481254980115"
      data-ad-slot="9596964208"></ins>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
  </script>




<div class="navigator">
    
        <span style="float:left"><a href="/paperlists/2020-02-24-ThesisRelatedPapers/">« Papers Related to My PhD Thesis-Example Weighting in Classification and Distance Metric Learning</a>
          · <a href="https://xinshaoamoswang.github.io/paperlists/2020-02-24-ThesisRelatedPapers/#disqus_thread"></a>
        </span>
    
    
        <span style="float:right"><a href="/blogs/2020-03-08-example-weighting/">Example Weighting, Importance Sampling? »</a>
          · <a href="https://xinshaoamoswang.github.io/blogs/2020-03-08-example-weighting/#disqus_thread"></a>
        </span>
    
</div>

<script data-ad-client="ca-pub-8231481254980115" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script data-ad-client="ca-pub-8231481254980115" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>





<div id="disqus_thread"></div>


<script>
    var disqus_config = function () {
            this.page.url = "https://xinshaoamoswang.github.io/blogs/2020-03-04-I-love-math/";
            this.page.identifier = "/blogs/I-love-math";
        }; 

    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://xinshaowang.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>


<noscript>Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>



  

  
<footer role="contentinfo">
  <hr/>
  
    <p><small class="copyright">© 2019-2020. All rights reserved.
</small></p>
  
  
  <p><small>Welcome to Xinshao Wang's Personal Website</small></p>
  <hr class="sr-only"/>
</footer>


</main>

    <hy-drawer
  class=""
  align="left"
  threshold="10"
  touch-events
  prevent-default
>
  <header id="_sidebar" class="sidebar" role="banner">
    
    <div class="sidebar-bg sidebar-overlay" style="background-color:rgb(25,55,71);background-image:url(/assets/img/sidebar-bg.jpg)"></div>

    <div class="sidebar-sticky">
      <div class="sidebar-about">
        
          <a class="no-hover" href="/" tabindex="-1">
            <img src="/assets/icons/android-chrome-192x192.png" class="avatar" alt="Xinshao (Amos) Wang" data-ignore />
          </a>
        
        <h2 class="h1"><a href="/">Xinshao (Amos) Wang</a></h2>
        
        
          <p class="fine">
            Machine Learning (Robust Learning under Adverse Conditions, Deep Metric Learning).
Computer Vision (Image/Video Recognition including retrieval and clustering, Person ReID).

          </p>
        
      </div>

      <nav class="sidebar-nav heading" role="navigation">
        <span class="sr-only">Navigation:</span>
<ul>
  
    
      
      <li>
        <a
          id="_navigation"
          href="/blogs/"
          class="sidebar-nav-item active"
          
        >
          Blog
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/paperlists/"
          class="sidebar-nav-item"
          
        >
          PaperReading
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/projects/"
          class="sidebar-nav-item"
          
        >
          Projects
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/Resume/"
          class="sidebar-nav-item"
          
        >
          Resume
        </a>
      </li>
    
      
      <li>
        <a
          
          href="/about/"
          class="sidebar-nav-item"
          
        >
          About ME
        </a>
      </li>
    
  
</ul>

      </nav>

      

      <div class="sidebar-social">
        <span class="sr-only">Social:</span>
<ul>
  
    
    

    
    

    
    
  
</ul>

      </div>
    </div>
  </header>
</hy-drawer>
<hr class="sr-only" hidden />

  
</hy-push-state>

<!--[if gt IE 10]><!---->

  <script nomodule>!function(){var e=document.createElement("script");if(!("noModule"in e)&&"onbeforeload"in e){var t=!1;document.addEventListener("beforeload",function(n){if(n.target===e)t=!0;else if(!n.target.hasAttribute("nomodule")||!t)return;n.preventDefault()},!0),e.type="module",e.src=".",document.head.appendChild(e),e.remove()}}();
</script>
  <script type="module" src="/assets/js/hydejack-8.5.2.js"></script>
  <script nomodule src="/assets/js/hydejack-legacy-8.5.2.js" defer></script>
  

  


<!--<![endif]-->




<h2 class="sr-only" hidden>Templates (for web app):</h2>

<template id="_animation-template" hidden>
  <div class="animation-main fixed-top">
    <div class="content">
      <div class="page"></div>
    </div>
  </div>
</template>

<template id="_loading-template" hidden>
  <div class="loading nav-btn fr">
    <span class="sr-only">Loading…</span>
    <span class="icon-cog"></span>
  </div>
</template>

<template id="_error-template" hidden>
  <div class="page">
    <h1 class="page-title">Error</h1>
    
    
    <p class="lead">
      Sorry, an error occurred while loading <a class="this-link" href=""></a>.

    </p>
  </div>
</template>

<template id="_forward-template" hidden>
  <button id="_forward" class="forward nav-btn no-hover fl">
    <span class="sr-only">Forward</span>
    <span class="icon-arrow-right2"></span>
  </button>
</template>

<template id="_back-template" hidden>
  <button id="_back" class="back nav-btn no-hover fl">
    <span class="sr-only">Back</span>
    <span class="icon-arrow-left2"></span>
  </button>
</template>

<template id="_permalink-template" hidden>
  <a href="#" class="permalink">
    <span class="sr-only">Permalink</span>
    <span class="icon-link"></span>
  </a>
</template>





  


  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <ins class="adsbygoogle"
      style="display:block"
      data-ad-format="fluid"
      data-ad-layout-key="-ef+6k-30-ac+ty"
      data-ad-client="ca-pub-8231481254980115"
      data-ad-slot="9596964208"></ins>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
  </script>

</body>



<script id="dsq-count-scr" src="//xinshaowang.disqus.com/count.js" async></script>

<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>

</html>
