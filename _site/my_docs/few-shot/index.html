<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v8.5.2 <https://hydejack.com/>
-->











<head>
  



<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">




  
<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Few-shot Learning | Xinshao Wang</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Few-shot Learning" />
<meta name="author" content="Xinshao Wang" />
<meta property="og:locale" content="en" />
<meta name="description" content="“Welcome to my personal website”. 3rd Year PhD Student, will graduate in Sep 2020." />
<meta property="og:description" content="“Welcome to my personal website”. 3rd Year PhD Student, will graduate in Sep 2020." />
<link rel="canonical" href="http://localhost:4000/my_docs/few-shot/" />
<meta property="og:url" content="http://localhost:4000/my_docs/few-shot/" />
<meta property="og:site_name" content="Xinshao Wang" />
<script type="application/ld+json">
{"description":"“Welcome to my personal website”. 3rd Year PhD Student, will graduate in Sep 2020.","author":{"@type":"Person","name":"Xinshao Wang"},"@type":"WebPage","url":"http://localhost:4000/my_docs/few-shot/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/icons/android-chrome-192x192.png"},"name":"Xinshao Wang"},"headline":"Few-shot Learning","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  

  
    <meta name="keywords" content="Xinshao Wang; Machine Learning,Computer Vision,Robust Learning,Deep Metric Learning,Image Recognition,Video Recognition,Person ReID">
  


<meta name="mobile-web-app-capable" content="yes">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="Xinshao Wang">
<meta name="apple-mobile-web-app-status-bar-style" content="black">

<meta name="application-name" content="Xinshao Wang">
<meta name="msapplication-config" content="/assets/ieconfig.xml">


<meta name="theme-color" content="rgb(25,55,71)">


<meta name="generator" content="Hydejack v8.5.2" />

<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Xinshao Wang" />



<link rel="alternate" href="http://localhost:4000/my_docs/few-shot/" hreflang="en">

<link rel="shortcut icon" href="/assets/icons/icon.png">
<link rel="apple-touch-icon" href="/assets/icons/icon.png">

<link rel="manifest" href="/assets/manifest.json">


  <link rel="dns-prefetch" href="https://fonts.googleapis.com">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">




<link rel="dns-prefetch" href="/" id="_baseURL">
<link rel="dns-prefetch" href="/sw.js" id="_hrefSW">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/katex.min.js" id="_hrefKatexJS">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/katex.min.css" id="_hrefKatexCSS">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/contrib/copy-tex.min.js" id="_hrefKatexCopyJS">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/contrib/copy-tex.min.css" id="_hrefKatexCopyCSS">
<link rel="dns-prefetch" href="/assets/img/swipe.svg" id="_hrefSwipeSVG">




<script>
!function(e,t){"use strict";function n(e,t,n,o){e.addEventListener?e.addEventListener(t,n,o):e.attachEvent?e.attachEvent("on"+t,n):e["on"+t]=n}e.loadJS=function(e,o){var r=t.createElement("script");r.src=e,o&&n(r,"load",o,{once:!0});var a=t.scripts[0];return a.parentNode.insertBefore(r,a),r},e._loaded=!1,e.loadJSDeferred=function(o,r){function a(){e._loaded=!0,r&&n(c,"load",r,{once:!0});var o=t.scripts[0];o.parentNode.insertBefore(c,o)}var c=t.createElement("script");return c.src=o,e._loaded?a():n(e,"load",a,{once:!0}),c},e.setRel=e.setRelStylesheet=function(e){function o(){this.rel="stylesheet"}n(t.getElementById(e),"load",o,{once:!0})}}(window,document);
;
!function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this);
;
!function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this);
;
!function(w, d) {
  w._noPushState = false;
  w._noDrawer = false;
}(window, document);
</script>

<!--[if gt IE 8]><!---->











  <link rel="stylesheet" href="/assets/css/hydejack-8.5.2.css">
  <link rel="stylesheet" href="/assets/icomoon/style.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:400|Noto+Sans:400,400i,700,700i&display=swap">
  


  <style id="_pageStyle">

.content a:not(.btn){color:#4fb1ba;border-color:rgba(79,177,186,0.2)}.content a:not(.btn):hover{border-color:#4fb1ba}:focus{outline-color:#4fb1ba !important}.btn-primary{color:#fff;background-color:#4fb1ba;border-color:#4fb1ba}.btn-primary:focus,.btn-primary.focus,.form-control:focus,.form-control.focus{box-shadow:0 0 0 3px rgba(79,177,186,0.5)}.btn-primary:hover,.btn-primary.hover{color:#fff;background-color:#409ba3;border-color:#409ba3}.btn-primary:disabled,.btn-primary.disabled{color:#fff;background-color:#4fb1ba;border-color:#4fb1ba}.btn-primary:active,.btn-primary.active{color:#fff;background-color:#409ba3;border-color:#409ba3}::selection{color:#fff;background:#4fb1ba}::-moz-selection{color:#fff;background:#4fb1ba}

</style>


<!--<![endif]-->





  
<script data-ad-client="ca-pub-8231481254980115" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
</head>

<body class="no-color-transition">
  <div id="_navbar" class="navbar fixed-top">
  <div class="content">
    <div class="nav-btn-bar">
      <span class="sr-only">Jump to:</span>
      <a id="_menu" class="nav-btn no-hover fl" href="#_navigation">
        <span class="sr-only">Navigation</span>
        <span class="icon-menu"></span>
      </a>
      <!-- <a id="_search" class="nav-btn no-hover fl" href="#_search">
        <span class="sr-only">Search</span>
        <span class="icon-search"></span>
      </a>
      <form action="https://duckduckgo.com/" method="GET">
        <div class="form-group fr">
          <label class="sr-only" for="_search">Search</label>
          <input id="_search" name="q" class="form-control" type="search" />
        </div>
        <input type="hidden" name="q" value="site:hydejack.com" />
        <input type="hidden" name="ia" value="web" />
      </form> -->
    </div>
  </div>
</div>
<hr class="sr-only" hidden>


<hy-push-state replace-ids="_main" link-selector="a[href]:not([href*='/assets/']):not(.external):not(.no-push-state)" duration="250" script-selector="script:not([type^='math/tex'])" prefetch>
  
    <main id="_main" class="content fade-in layout-page" role="main" data-color="rgb(79,177,186)" data-theme-color="rgb(25,55,71)" data-image="/assets/img/sidebar-bg.jpg" data-overlay>
  <article class="page" role="article">
  <header>
    <h1 class="page-title">Few-shot Learning</h1>
    



  <div class="hr pb0"></div>


  </header>

  <p class="message"><img class="emoji" title=":+1:" alt=":+1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png" height="20" width="20"> means being highly related to my personal research interest.</p>

<h2 id="1-iclr-2018-meta-learning-for-semi-supervised-few-shot-classification">
<img class="emoji" title=":+1:" alt=":+1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png" height="20" width="20"> <a href="https://openreview.net/pdf?id=HJcSzz-CZ">ICLR 2018-Meta-Learning for Semi-Supervised Few-Shot Classification</a>
</h2>
<p class="message"><strong>NOTE</strong>: 
In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. <strong>Recent progress in few-shot classification has featured meta-learning,</strong> in which a parameterized model for a learning algorithm is defined and trained on <strong>episodes representing different classification problems, each with a small labeled training set and its corresponding test set.</strong> In this work, <strong>we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode.</strong> We consider <strong>two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided.</strong> To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We <strong>evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples.</strong> We also propose <strong>a new split of ImageNet</strong>, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would. <br>
TL;DR: <strong>We propose novel extensions of Prototypical Networks that are augmented with the ability to use unlabeled examples when producing prototypes.</strong> <br>
Related: <a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/">Overview of learning to learn</a> <br>
<a href="https://github.com/cbfinn/maml">Code for “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks”</a></p>

<h2 id="1-neurips-2019-unsupervised-meta-learning-for-few-show-image-classification">
<img class="emoji" title=":+1:" alt=":+1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png" height="20" width="20"> <a href="https://arxiv.org/pdf/1811.11819.pdf">NeurIPS 2019-Unsupervised Meta Learning for Few-Show Image Classification</a>
</h2>
<p class="message"><strong>NOTE</strong>: 
Few-shot or one-shot learning of classifiers for images or videos is an important next frontier in computer vision. The extreme paucity of training data means that the learning must start with a significant inductive bias towards the type of task to be learned. One way to acquire this is by meta-learning on tasks similar to the target task. However, <strong>if the meta-learning phase requires labeled data for a large number of tasks closely related to the target task, it not only increases the difficulty and cost, but also conceptually limits the approach to variations of well-understood domains.</strong> <br>
In this paper, we propose UMTRA, an algorithm that performs meta-learning on an unlabeled dataset in an unsupervised fashion, without putting any constraint on the classifier network architecture. <strong>The only requirements towards the dataset are: sufficient size, diversity and number of classes, and relevance of the domain to the one in the target task. Exploiting this information, UMTRA generates synthetic training tasks for the meta-learning phase.</strong> <br>
We evaluate UMTRA on few-shot and one-shot learning on <strong>both image and video domains.</strong> To the best of our knowledge, we are the first to evaluate meta-learning approaches on UCF-101. On the Omniglot and Mini-Imagenet few-shot learning benchmarks, UMTRA outperforms every tested approach based on unsupervised learning of representations, while alternating for the best performance with the recent CACTUs algorithm. Compared to supervised model-agnostic meta-learning approaches, UMTRA trades off some classification accuracy for a vast decrease in the number of labeled data needed. For instance, on the five-way one-shot classification on the Omniglot, we retain 85% of the accuracy of MAML, a recently proposed supervised meta-learning algorithm, while reducing the number of required labels from 24005 to 5.</p>

<h2 id="1-neurips-2019-learning-to-self-train-for-semi-supervised-few-shot-classification">
<img class="emoji" title=":+1:" alt=":+1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png" height="20" width="20"> <a href="https://arxiv.org/pdf/1906.00562.pdf">NeurIPS 2019-Learning to Self-Train for Semi-Supervised Few-Shot Classification</a>
</h2>
<p class="message"><strong>NOTE</strong>: 
Few-shot classification (FSC) is challenging due to the scarcity of labeled training data (e.g. only one labeled data point per class). Meta-learning has shown to achieve promising results by learning to initialize a classification model for FSC. In this paper we propose a novel semi-supervised meta-learning method called learning to self-train (LST) that <strong>leverages unlabeled data and specifically meta-learns how to cherry-pick and label such unsupervised data to further improve performance.</strong> To this end, we train the LST model through a large number of semi-supervised few-shot tasks. <strong>On each task, we train a few-shot model to predict pseudo labels for unlabeled data, and then iterate the self-training steps on labeled and pseudo-labeled data with each step followed by fine-tuning.</strong> We additionally learn <strong>a soft weighting network (SWN) to optimize the self-training weights of pseudo labels so that better ones can contribute more to gradient descent optimization.</strong> We evaluate our LST method on two ImageNet benchmarks for semi-supervised few-shot classification and achieve large improvements over the state-of-the-art method. <br>
Code url: <a href="https://github.com/xinzheli1217/learning-to-self-train">https://github.com/xinzheli1217/learning-to-self-train</a></p>

<h2 id="neurips-2019-adaptive-cross-modal-few-shot-learning"><a href="https://arxiv.org/pdf/1902.07104.pdf">NeurIPS 2019-Adaptive Cross-Modal Few-shot Learning</a></h2>
<p class="message"><strong>NOTE</strong>: 
Metric-based meta-learning techniques have successfully been applied to few-shot classification problems. In this paper, we propose to leverage cross-modal information to enhance metric-based few-shot learning methods. Visual and semantic feature spaces have different structures by definition. <strong>For certain concepts, visual features might be richer and more discriminative than text ones. While for others, the inverse might be true. Moreover, when the support from visual information is limited in image classification, semantic representations (learned from unsupervised text corpora) can provide strong prior knowledge and context to help learning.</strong> Based on these two intuitions, we propose a mechanism that can adaptively combine information from both modalities according to new image categories to be learned. Through a series of experiments, we show that by this adaptive combination of the two modalities, our model outperforms current uni-modality few-shot learning methods and modality-alignment methods by a large margin on all benchmarks and few-shot scenarios tested. Experiments also show that <strong>our model can effectively adjust its focus on the two modalities. The improvement in performance is particularly large when the number of shots is very small.</strong> <br>
Code url: <a href="https://github.com/ElementAI/am3">https://github.com/ElementAI/am3</a></p>

<h2 id="neurips-2019-cross-attention-network-for-few-shot-classification"><a href="">NeurIPS 2019-Cross Attention Network for Few-shot Classification</a></h2>

<p class="message"><strong>NOTE</strong>: 
Not available yet.</p>

<h2 id="neurips-2019-incremental-few-shot-learning-with-attention-attractor-networks"><a href="https://arxiv.org/pdf/1810.07218.pdf">NeurIPS 2019-Incremental Few-Shot Learning with Attention Attractor Networks</a></h2>
<p class="message"><strong>NOTE</strong>: 
Machine learning classifiers are often trained to recognize a set of pre-defined classes. However, in many real applications, it is often desirable to have the flexibility of learning additional concepts, without re-training on the full training set. This paper addresses this problem, <strong>incremental few-shot learning, where a regular classification network has already been trained to recognize a set of base classes; and several extra novel classes are being considered, each with only a few labeled examples. After learning the novel classes, the model is then evaluated on the overall performance of both base and novel classes.</strong> To this end, we propose a meta-learning model, the Attention Attractor Network, <strong>which regularizes the learning of novel classes.</strong> In each episode, we train a set of new weights to recognize novel classes until they converge, and we show that <strong>the technique of recurrent back-propagation can back-propagate through the optimization process and facilitate the learning of the attractor network regularizer.</strong> We demonstrate that <strong>the learned attractor network can recognize novel classes while remembering old classes without the need to review the original training set</strong>, outperforming baselines that do not rely on an iterative optimization process.</p>

<h2 id="1-icml-2019-lgm-net-learning-to-generate-matching-networks-for-few-shot-learning">
<img class="emoji" title=":+1:" alt=":+1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png" height="20" width="20"> <a href="http://proceedings.mlr.press/v97/li19c/li19c.pdf">ICML 2019-LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning</a>
</h2>
<p class="message"><strong>NOTE</strong>: 
In this work, we propose <strong>a novel meta-learning
approach for few-shot classification</strong>, which learns
<strong>transferable prior knowledge across tasks</strong> and
<strong>directly produces network parameters for similar unseen tasks with training samples</strong>. Our approach,
called LGM-Net, includes two key modules,
namely, TargetNet and MetaNet. The <strong>TargetNet module is a neural network for solving a specific task</strong> and the <strong>MetaNet module aims at learning to generate functional weights for TargetNet by observing training samples.</strong> We also present an intertask normalization strategy for the training process to <strong>leverage common information shared across different tasks.</strong> The experimental results on Omniglot and miniImageNet datasets
demonstrate that LGM-Net can effectively adapt to similar unseen tasks and achieve competitive performance, and the results on synthetic datasets
show that transferable prior knowledge is learned
by the MetaNet module via <strong>mapping training data to functional weights.</strong> <strong>LGM-Net enables fast learning and adaptation</strong> since no further tuning steps are required compared to other metalearning approaches. <br>
Code url: <a href="https://github.com/likesiwell/LGM-Net/">https://github.com/likesiwell/LGM-Net/</a></p>


</article>










<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    },
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML">
</script>





<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <ins class="adsbygoogle" style="display:block" data-ad-format="fluid" data-ad-layout-key="-ef+6k-30-ac+ty" data-ad-client="ca-pub-8231481254980115" data-ad-slot="9596964208"></ins>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
  </script>

  


<div class="navigator">
    
    
</div>




<script data-ad-client="ca-pub-8231481254980115" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>





<div id="disqus_thread"></div>


<script>
    var disqus_config = function () {
            this.page.url = "https://xinshaoamoswang.github.io/my_docs/few-shot/";
            this.page.identifier = "";
        }; 

    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://xinshaowang.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>


<noscript>Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>




  

  
<footer role="contentinfo">
  <hr>
  
    <p><small class="copyright">© 2019. All rights reserved.
</small></p>
  
  
  <p><small>Welcome to Xinshao Wang's Personal Website</small></p>
  <hr class="sr-only">
</footer>


</main>

    <hy-drawer class="" align="left" threshold="10" touch-events prevent-default>
  <header id="_sidebar" class="sidebar" role="banner">
    
    <div class="sidebar-bg sidebar-overlay" style="background-color:rgb(25,55,71);background-image:url(/assets/img/sidebar-bg.jpg)"></div>

    <div class="sidebar-sticky">
      <div class="sidebar-about">
        
          <a class="no-hover" href="/" tabindex="-1">
            <img src="/assets/icons/android-chrome-192x192.png" class="avatar" alt="Xinshao Wang" data-ignore>
          </a>
        
        <h2 class="h1"><a href="/">Xinshao Wang</a></h2>
        
        
          <p class="fine">
            Machine Learning (Deep Metric Learning, Robust Learning under Arbitrary Anomalies).
Computer Vision (Image/Video Recognition, Person ReID).

          </p>
        
      </div>

      <nav class="sidebar-nav heading" role="navigation">
        <span class="sr-only">Navigation:</span>
<ul>
  
    
      
      <li>
        <a id="_navigation" href="/blogs/" class="sidebar-nav-item">
          Blog
        </a>
      </li>
    
      
      <li>
        <a href="/paperlists/" class="sidebar-nav-item">
          PaperReading
        </a>
      </li>
    
      
      <li>
        <a href="/projects/" class="sidebar-nav-item">
          Projects
        </a>
      </li>
    
      
      <li>
        <a href="/Resume/" class="sidebar-nav-item">
          Resume
        </a>
      </li>
    
      
      <li>
        <a href="/about/" class="sidebar-nav-item">
          About ME
        </a>
      </li>
    
  
</ul>

      </nav>

      

      <div class="sidebar-social">
        <span class="sr-only">Social:</span>
<ul>
  
    
    

    
    

    
    
  
</ul>

      </div>
    </div>
  </header>
</hy-drawer>
<hr class="sr-only" hidden>

  
</hy-push-state>

<!--[if gt IE 10]><!---->

  <script nomodule>!function(){var e=document.createElement("script");if(!("noModule"in e)&&"onbeforeload"in e){var t=!1;document.addEventListener("beforeload",function(n){if(n.target===e)t=!0;else if(!n.target.hasAttribute("nomodule")||!t)return;n.preventDefault()},!0),e.type="module",e.src=".",document.head.appendChild(e),e.remove()}}();
</script>
  <script type="module" src="/assets/js/hydejack-8.5.2.js"></script>
  <script nomodule src="/assets/js/hydejack-legacy-8.5.2.js" defer></script>
  

  


<!--<![endif]-->




<h2 class="sr-only" hidden>Templates (for web app):</h2>

<template id="_animation-template" hidden>
  <div class="animation-main fixed-top">
    <div class="content">
      <div class="page"></div>
    </div>
  </div>
</template>

<template id="_loading-template" hidden>
  <div class="loading nav-btn fr">
    <span class="sr-only">Loading…</span>
    <span class="icon-cog"></span>
  </div>
</template>

<template id="_error-template" hidden>
  <div class="page">
    <h1 class="page-title">Error</h1>
    
    
    <p class="lead">
      Sorry, an error occurred while loading <a class="this-link" href=""></a>.

    </p>
  </div>
</template>

<template id="_forward-template" hidden>
  <button id="_forward" class="forward nav-btn no-hover fl">
    <span class="sr-only">Forward</span>
    <span class="icon-arrow-right2"></span>
  </button>
</template>

<template id="_back-template" hidden>
  <button id="_back" class="back nav-btn no-hover fl">
    <span class="sr-only">Back</span>
    <span class="icon-arrow-left2"></span>
  </button>
</template>

<template id="_permalink-template" hidden>
  <a href="#" class="permalink">
    <span class="sr-only">Permalink</span>
    <span class="icon-link"></span>
  </a>
</template>





  


  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <ins class="adsbygoogle" style="display:block" data-ad-format="fluid" data-ad-layout-key="-ef+6k-30-ac+ty" data-ad-client="ca-pub-8231481254980115" data-ad-slot="9596964208"></ins>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
  </script>

</body>



<script id="dsq-count-scr" src="//xinshaowang.disqus.com/count.js" async></script>

<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>

</html>
