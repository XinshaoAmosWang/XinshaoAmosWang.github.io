<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v8.5.2 <https://hydejack.com/>
-->











<head>
  



<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">




  
<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Robust DL/ML | Postdoc@OxfordU</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Robust DL/ML" />
<meta name="author" content="XW" />
<meta property="og:locale" content="en" />
<meta name="description" content="“Stay Hungry. Stay Foolish. – Steve Jobs 2005”. A ML/DL/AI Researcher" />
<meta property="og:description" content="“Stay Hungry. Stay Foolish. – Steve Jobs 2005”. A ML/DL/AI Researcher" />
<link rel="canonical" href="http://localhost:4000/readingnotes/2020-09-08-RobustMLDL/" />
<meta property="og:url" content="http://localhost:4000/readingnotes/2020-09-08-RobustMLDL/" />
<meta property="og:site_name" content="Postdoc@OxfordU" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-08T00:00:00+01:00" />
<script type="application/ld+json">
{"description":"“Stay Hungry. Stay Foolish. – Steve Jobs 2005”. A ML/DL/AI Researcher","author":{"@type":"Person","name":"XW"},"@type":"BlogPosting","url":"http://localhost:4000/readingnotes/2020-09-08-RobustMLDL/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/icons/android-chrome-192x192.png"},"name":"XW"},"headline":"Robust DL/ML","dateModified":"2020-09-08T00:00:00+01:00","datePublished":"2020-09-08T00:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/readingnotes/2020-09-08-RobustMLDL/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  

  
    <meta name="keywords" content="Machine Learning,Computer Vision,Robust Learning,Deep Metric Learning,Image Recognition,Video Recognition,Person ReID">
  


<meta name="mobile-web-app-capable" content="yes">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="Postdoc@OxfordU">
<meta name="apple-mobile-web-app-status-bar-style" content="black">

<meta name="application-name" content="Postdoc@OxfordU">
<meta name="msapplication-config" content="/assets/ieconfig.xml">


<meta name="theme-color" content="rgb(25,55,71)">


<meta name="generator" content="Hydejack v8.5.2" />

<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Postdoc@OxfordU" />



<link rel="alternate" href="http://localhost:4000/readingnotes/2020-09-08-RobustMLDL/" hreflang="en">

<link rel="shortcut icon" href="/assets/icons/icon.png">
<link rel="apple-touch-icon" href="/assets/icons/icon.png">

<link rel="manifest" href="/assets/manifest.json">


  <link rel="dns-prefetch" href="https://fonts.googleapis.com">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">




<link rel="dns-prefetch" href="/" id="_baseURL">
<link rel="dns-prefetch" href="/sw.js" id="_hrefSW">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/katex.min.js" id="_hrefKatexJS">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/katex.min.css" id="_hrefKatexCSS">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/contrib/copy-tex.min.js" id="_hrefKatexCopyJS">
<link rel="dns-prefetch" href="/assets/bower_components/katex/dist/contrib/copy-tex.min.css" id="_hrefKatexCopyCSS">
<link rel="dns-prefetch" href="/assets/img/swipe.svg" id="_hrefSwipeSVG">




<script>
!function(e,t){"use strict";function n(e,t,n,o){e.addEventListener?e.addEventListener(t,n,o):e.attachEvent?e.attachEvent("on"+t,n):e["on"+t]=n}e.loadJS=function(e,o){var r=t.createElement("script");r.src=e,o&&n(r,"load",o,{once:!0});var a=t.scripts[0];return a.parentNode.insertBefore(r,a),r},e._loaded=!1,e.loadJSDeferred=function(o,r){function a(){e._loaded=!0,r&&n(c,"load",r,{once:!0});var o=t.scripts[0];o.parentNode.insertBefore(c,o)}var c=t.createElement("script");return c.src=o,e._loaded?a():n(e,"load",a,{once:!0}),c},e.setRel=e.setRelStylesheet=function(e){function o(){this.rel="stylesheet"}n(t.getElementById(e),"load",o,{once:!0})}}(window,document);
;
!function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this);
;
!function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this);
;
!function(w, d) {
  w._noPushState = false;
  w._noDrawer = false;
}(window, document);
</script>

<!--[if gt IE 8]><!---->











  <link rel="stylesheet" href="/assets/css/hydejack-8.5.2.css">
  <link rel="stylesheet" href="/assets/icomoon/style.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:400|Noto+Sans:400,400i,700,700i&display=swap">
  


  <style id="_pageStyle">

.content a:not(.btn){color:#4fb1ba;border-color:rgba(79,177,186,0.2)}.content a:not(.btn):hover{border-color:#4fb1ba}:focus{outline-color:#4fb1ba !important}.btn-primary{color:#fff;background-color:#4fb1ba;border-color:#4fb1ba}.btn-primary:focus,.btn-primary.focus,.form-control:focus,.form-control.focus{box-shadow:0 0 0 3px rgba(79,177,186,0.5)}.btn-primary:hover,.btn-primary.hover{color:#fff;background-color:#409ba3;border-color:#409ba3}.btn-primary:disabled,.btn-primary.disabled{color:#fff;background-color:#4fb1ba;border-color:#4fb1ba}.btn-primary:active,.btn-primary.active{color:#fff;background-color:#409ba3;border-color:#409ba3}::selection{color:#fff;background:#4fb1ba}::-moz-selection{color:#fff;background:#4fb1ba}

</style>


<!--<![endif]-->





  
<script data-ad-client="ca-pub-8231481254980115" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script data-ad-client="ca-pub-8231481254980115" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
</head>

<body class="no-color-transition">
  <div id="_navbar" class="navbar fixed-top">
  <div class="content">
    <div class="nav-btn-bar">
      <span class="sr-only">Jump to:</span>
      <a id="_menu" class="nav-btn no-hover fl" href="#_navigation">
        <span class="sr-only">Navigation</span>
        <span class="icon-menu"></span>
      </a>
      <!-- <a id="_search" class="nav-btn no-hover fl" href="#_search">
        <span class="sr-only">Search</span>
        <span class="icon-search"></span>
      </a>
      <form action="https://duckduckgo.com/" method="GET">
        <div class="form-group fr">
          <label class="sr-only" for="_search">Search</label>
          <input id="_search" name="q" class="form-control" type="search" />
        </div>
        <input type="hidden" name="q" value="site:hydejack.com" />
        <input type="hidden" name="ia" value="web" />
      </form> -->
    </div>
  </div>
</div>
<hr class="sr-only" hidden>


<hy-push-state replace-ids="_main" link-selector="a[href]:not([href*='/assets/']):not(.external):not(.no-push-state)" duration="250" script-selector="script:not([type^='math/tex'])" prefetch>
  
    <main id="_main" class="content fade-in layout-post" role="main" data-color="rgb(79,177,186)" data-theme-color="rgb(25,55,71)" data-image="/assets/img/sidebar-bg.jpg" data-overlay>
  




<article id="post-readingnotes-RobustMLDL" class="page post mb6" role="article">
  <header>
    <h1 class="post-title">
      
        Robust DL/ML
      
    </h1>

    <p class="post-date heading">
      
      <time datetime="2020-09-08T00:00:00+01:00">08 Sep 2020</time>
      
      
      
      
      









in <a href="/readingnotes/" class="flip-title">Reading Notes</a>

      











    </p>

    
    

    



  <div class="hr pb0"></div>


  </header>

  
    <p>In general, robust deep learning covers: missing labels (semisupervised learning); noisy labels (noise detection and correction); regularisation techniques; sample imbalance (long-tailed class distribution); adversarial learning; and so on.</p>

<ol class="message">
  <li><a href="#icml-20-papers-some-are-not-from-icml-but-cited-in-icml">ICML-20 papers</a></li>
  <li><a href="#the-design-of-loss-functions-ie-optimisation-objectives-or-output-regularistion">The design of loss functions (i.e., optimisation objectives or output regularistion)</a></li>
</ol>

<h3 id="icml-20-papers-some-are-not-from-icml-but-cited-in-icml">ICML-20 papers (some are not from ICML, but cited in ICML)</h3>

<ul class="message">
  <li>
<a href="https://arxiv.org/pdf/2007.00151.pdf">(arXiv-20-June) Early-Learning Regularization Prevents Memorization of Noisy Labels</a>
    <ul>
      <li>Their analysis is similar with <a href="https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/">DM and IMAE</a>, and they forgot to cite them in their first released version.
        <ul>
          <li>Early in training, the gradients corresponding to the correctly labeled examples dominate the dynamics—leading to early progress towards the true optimum—but that the gradients corresponding to wrong labels soon become dominant—at which point the classifier simply learns to fit the noisy labels.</li>
        </ul>
      </li>
      <li>There are two key elements to our approach. (A regularization term that incorporates target probabilities estimated from the model outputs using several semi-supervised learning techniques.)
        <ul>
          <li>First, we <strong>leverage semi-supervised learning techniques to produce target probabilities</strong> based on the model outputs.</li>
          <li>Second, we design a regularization term that steers the model towards these targets, implicitly preventing memorization of the false labels.</li>
        </ul>
      </li>
      <li>We also perform a systematic ablation study to evaluate the different alternatives to <strong>compute the target probabilities</strong>, and the effect of incorporating mixup data augmentation.
        <ul>
          <li>Temporal ensembeling: averaging label predictions;</li>
          <li>MeanTeachers: Weight-averaged consistency targets</li>
          <li>MixMatch =&gt; DivideMix</li>
          <li>Consistency regularisation: Interpolation Consistency Training</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<a href="https://www.ijcai.org/Proceedings/2019/0504.pdf">(IJCAI-19) Consistency regularisation: Interpolation Consistency Training for Semi-supervised Learning, Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz</a>
    <ul>
      <li>
        <p>ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points.</p>
      </li>
      <li>
        <p>In classification problems, ICT moves the decision boundary to low-density regions of the data distribution.</p>
      </li>
      <li>
        <p>MixUp is used for preprocessing/data augmentation.</p>
      </li>
    </ul>
  </li>
  <li>
<a href="https://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf">(NeurIPS-19) David Berthelot, Nicholas Carlini, Ian J. Goodfellow, Nicolas Papernot, Avital Oliver, and ColinRaffel. Mixmatch: A holistic approach to semi-supervised learning.</a>
    <ul>
      <li>This method is simple and effective. Their writing is also extremely naive to understand, instead of being fancy.</li>
      <li>Many recent approaches for semi-supervised learning add a loss term which is computed on unlabeled data and encourages the model to generalize better to unseen data:
        <ul>
          <li>entropy minimization – which encourages the model to output confident predictions on unlabeled data;</li>
          <li>consistency regularization – which encourages the model to produce the same output distribution when its inputs are perturbed;</li>
          <li>generic regularization – which encourages the model to generalize well and avoids overfitting the training data.</li>
        </ul>
      </li>
      <li>We introduce MixMatch, an SSL algorithm which introduces a single loss that gracefully unifies these dominant approaches to semi-supervised learning. We further show in an ablation study that MixMatch is greater than the sum of its parts;</li>
      <li><a href="https://github.com/google-research/mixmatch">https://github.com/google-research/mixmatch</a></li>
      <li>
        <p>In short, MixMatch introduces a unified loss term for unlabeled data that seamlessly reduces entropy while maintaining consistency and remaining compatible with traditional regularization techniques.</p>
      </li>
      <li>
        <p>Label guessing process used in MixMatch:
  <hy-img root-margin="512px" src="/imgs/MixMatch_label_guessing_proces.png" alt="Label guessing process used in MixMatch" class="lead figure" data-width="800" data-height="100">
  <noscript><img data-ignore src="/imgs/MixMatch_label_guessing_proces.png" alt="Label guessing process used in MixMatch" class="lead figure" data-width="800" data-height="100"></noscript>
  <span slot="loading" class="loading"><span class="icon-cog"></span></span>
</hy-img>
</p>
      </li>
      <li>
        <p>Question: if hyperparameters are not sensitive, why do we need it? Therefore, this writing is just for reviewers, not for readers. 
  <hy-img root-margin="512px" src="/imgs/MixMatch_hyperparameters.png" alt="" class="lead figure" data-width="800" data-height="100">
  <noscript><img data-ignore src="/imgs/MixMatch_hyperparameters.png" alt="" class="lead figure" data-width="800" data-height="100"></noscript>
  <span slot="loading" class="loading"><span class="icon-cog"></span></span>
</hy-img>
</p>
      </li>
      <li>
        <p>To set the stage for MixMatch, we first introduce existing methods for SSL. <strong>We focus mainly on those which are currently state-of-the-art and thatMixMatchbuilds on; there is a wide literature onSSL techniques that we do not discuss here</strong> (e.g., “transductive” models [14,22,21], graph-based methods [49,4,29], generative modeling [3,27,41,9,17,23,38,34,42], etc.). <strong>More comprehensive overviews are provided in [49,6].</strong></p>
      </li>
      <li>MixUp is used for preprocessing/data augmentation.</li>
    </ul>
  </li>
  <li>
<a href="https://openreview.net/forum?id=HJgExaVtwr&amp;noteId=keqS67sTCbi">(ICLR-20) Junnan Li, Richard Socher, and Steven C.H. Hoi. DivideMix: Learning with noisy labels as semi-supervised learning.</a>
    <ul>
      <li>
<a href="https://openreview.net/forum?id=HJgExaVtwr&amp;noteId=keqS67sTCbi">https://openreview.net/forum?id=HJgExaVtwr&amp;noteId=keqS67sTCbi</a>
        <ul>
          <li>
<strong>Exploiting MixMatch</strong>;</li>
          <li>The algorithm is complex. Instead, DM, IMAE, and ProSelfLC are much simpler.</li>
        </ul>
      </li>
      <li>
        <p>DivideMix models the per-sample loss distribution with a mixture model to <strong>dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples</strong>, and trains the model on both the labeled and unlabeled data in a semi-supervised manner.</p>
      </li>
      <li>
        <p>To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the <strong>MixMatch</strong> strategy by performing <strong>label co-refinement and label co-guessing on labeled and unlabeled samples</strong>, respectively.</p>
      </li>
      <li>
        <p>DivideMix discards the sample labels that are highly likely to be noisy, and leverages the noisy samples as unlabeled data to regularize the model from overfitting and improve generalization performance.</p>
      </li>
      <li>
        <p>For labeled samples, we refine their ground-truth labels using the network’s predictions guided by the GMM for the other network. For unlabeled samples,we use the ensemble of both networks to make reliable guesses for their labels.</p>
      </li>
      <li>Training two networks, Co-divide datasets, label co-refinement and co-guessing.
        <ul>
          <li>improving MixMatch with label co-refinement and co-guessing.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<a href="https://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf">(NeurIPS-17 Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. )</a>
    <ul>
      <li>Because the targets change only onceper epoch, Temporal Ensembling becomes unwieldy when learning large datasets.</li>
      <li>To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling.</li>
      <li>
        <p>Abstract: Without changing the network architecture, Mean Teacher achieves anerror rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucialto performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.</p>
      </li>
      <li>
        <p>Key algorithm:<br>
  <hy-img root-margin="512px" src="/imgs/MeanTeachers_KeyAlgorithm.png" alt="" class="lead figure" data-width="800" data-height="100">
  <noscript><img data-ignore src="/imgs/MeanTeachers_KeyAlgorithm.png" alt="" class="lead figure" data-width="800" data-height="100"></noscript>
  <span slot="loading" class="loading"><span class="icon-cog"></span></span>
</hy-img>
</p>
      </li>
      <li>
        <p>There are at least two ways to improve the target quality. One approach is to choose the perturbation of the representations carefully instead of barely applying additive or multiplicative noise. Another approach is to choose the teacher model carefully instead of barely replicating the student model.Concurrently to our research, <a href="https://arxiv.org/pdf/1704.03976.pdf">“Virtual Adversarial Training” (VAT)</a> has taken the first approach and shown that Virtual Adversarial Training can yield impressive results. We take the second approach and will show that it too provides significant benefits. To our understanding, these two approaches are compatible, andtheir combination may produce even better outcomes. However, the analysis of their combined effectsis outside the scope of this paper.</p>
      </li>
      <li>About <a href="https://openreview.net/pdf?id=BJ6oOfqge">TEMPORALENSEMBLING FORSEMI-SUPERVISEDLEARNING</a>: Each target is updated only once per epoch, the learned information is incorporated into the training process at a slow pace. The larger the dataset, the longer the span of the updates, and in the case of on-line learning, it is unclear how Temporal Ensembling can be used at all. (One could evaluate all the targets periodically more than once per epoch, but keeping the evaluation span constant would require \(O(n^2)\) evaluations per epoch where n is the number of training examples.)</li>
    </ul>
  </li>
  <li>
<a href="https://openreview.net/pdf?id=BJ6oOfqge">(ICLR-17) Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2017.</a>
    <ul>
      <li>We introduce <strong>self-ensembling</strong>, where we form <strong>a consensus prediction of the unknown labels</strong> using the outputs of the network-in-training on differentepochs, and most importantly, under different regularization and input augmentation conditions. <strong>This ensemble prediction can be expected to be a better predictorfor the unknown labels than the output of the network at the most recent training epoch</strong>, and can thus be used as a target for training;</li>
    </ul>
  </li>
  <li>
<a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2888-Paper.pdf">Unlabelled Data Improves Bayesian Uncertainty Calibration under Covariate Shift Alexander Chan, Ahmed Alaa, Zhaozhi Qian, Mihaela van der Schaar</a>
    <ul>
      <li>While existing variants of BNNs are able to produce reliable, albeit approximate, uncertainty estimates over in-distribution data, it has been shown that they tend to be overconfident in predictions made on target data whose distribution over features differs from the training data, i.e., <strong>the covariate shift setup</strong>.</li>
      <li>We develop <strong>an approximate Bayesian inference scheme</strong> based on posterior regularisation, where we use information from <strong>unlabelled target data to produce more appropriate uncertainty estimates for ‘‘covariate-shifted’’ predictions.</strong>
</li>
      <li>
        <p>Empirical evaluations demonstrate that our method performs competitively compared to Bayesian and frequentist approaches to <strong>uncertainty estimation in neural networks.</strong></p>

        <ul>
          <li>
            <p>uncertainty estimation:  quantifying confidence in their predictions — this is crucial in <strong>high-stakes applications that involve critical decision-making</strong>.</p>
          </li>
          <li>
            <p>We make the following observation: <strong>a point being in the target data is an indication that the model should output higher uncertainty</strong> because the target distribution is not well-represented by training data due to covariate shift.
  <strong>We use whether the data come from training or target set as a “pseudo-label”of model confidence.</strong></p>
          </li>
          <li>
            <p>BNN learns a posterior distribution over parametersthat encapsulates the model uncertainty. Due the complexityof deep neural networks, the exact posterior is usually intractable. Hence, much of the research in BNN literature is devoted to finding better approximate inference algorithms for the posterior.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>We note  that  most  existingworks in SSL focus entirely on using unlabelled data toimprove predictive performance (e.g. accuracy), but muchless thoughts have been given to improving the uncertainty estimate for those predictions, which is the focus of this paper.</p>
      </li>
      <li>
<strong>Semi-supervised Learning</strong>: Many recent works encourage the model to generalise better by using a regularisation term computed on the unlabelled data <a href="https://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf">MixMatch Berth-elot et al. (NeurIPS 2019)</a>.
        <ul>
          <li>
<strong>Consistency regularization</strong> applies data augmentation to semi-supervised learning by leveraging the idea that a classifier should output the same class distribution for an unlabeled example even after it has been augmented.
            <ul>
              <li>E.g. 1: <a href="https://arxiv.org/pdf/1704.03976.pdf">(TPAMI 2018) “Virtual Adversarial Training” (VAT)</a> addresses this by instead computing an additive perturbation to apply to the input which maximally changes the output class distribution;</li>
              <li>E.g. 2: <a href="https://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf">(NeurIPS 2017) “Mean Teacher”: a method that averages model weights instead of label predictions.</a> uses an exponential moving average of model parameter values. This provides a more stable target and was found empirically to significantly improve results. 
 Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling.
 Combining Mean Teacher and Residual Networks, we improvethe state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.</li>
              <li>E.g. 3: <a href="https://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf">MixMatch Berth-elot et al. (NeurIPS 2019)</a> utilizes a form of consistency regularization through the use of standard data augmentation for images (random horizontal flips and crops).</li>
              <li>
<a href="https://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf">Connections</a>: There are at least two ways to improve the target quality. One approach is to choose the perturbation of the representations carefully instead of barely applying additive or multiplicative noise. Another approach is to choose the teacher model carefully instead of barely replicating the student model.Concurrently to our research, <a href="https://arxiv.org/pdf/1704.03976.pdf">“Virtual Adversarial Training” (VAT)</a> has taken the first approach and shown that Virtual Adversarial Training can yield impressive results. We take the second approach and will show that it too provides significant benefits. To our understanding, these two approaches are compatible, andtheir combination may produce even better outcomes. However, the analysis of their combined effectsis outside the scope of this paper.</li>
              <li>About <a href="https://openreview.net/pdf?id=BJ6oOfqge">TEMPORALENSEMBLING FORSEMI-SUPERVISEDLEARNING</a>: Each target is updated only once per epoch, the learned information is incorporated into the training process at a slow pace. The larger the dataset, the longer the span of the updates, and in the case of on-line learning, it is unclear how Temporal Ensembling can be used at all. (One could evaluate all the targets periodically more than once per epoch, but keeping the evaluation span constant would require \(O(n^2)\) evaluations per epoch where n is the number of training examples.)</li>
            </ul>
          </li>
          <li>
            <p><strong>Entropy Minimization</strong>: CCE, <a href="https://xinshaoamoswang.github.io/blogs/2020-06-07-Progressive-self-label-correction/">ProSelfLC</a>, <a href="https://papers.nips.cc/paper/2740-semi-supervised-learning-by-entropy-minimization.pdf">(NeurIPS) Semi-supervised learning by entropy minimization</a>, <a href="http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf">(ICML workshop 2013) Pseudo-Label</a></p>
          </li>
          <li>
            <p><strong>Traditional Regularization</strong>: weight decay, MixUP, etc.</p>
          </li>
          <li>More work on semi-supervised learning:
            <ul>
              <li><a href="https://papers.nips.cc/paper/7778-semi-supervised-deep-kernel-learning-regression-with-unlabeled-data-by-minimizing-predictive-variance.pdf">(NeurIPS 2018) Entropy minimisation: Jean, N., Xie, S. M., and Ermon, S. Semi-supervised deep kernel learning: Regression with unlabeled data by minimizing predictive variance.</a></li>
              <li><a href="https://papers.nips.cc/paper/6333-regularization-with-stochastic-transformations-and-perturbations-for-deep-semi-supervised-learning.pdf">(NeurIPS 2016) Consistency regularisation: Sajjadi, M., Javanmardi, M., and Tasdizen, T.  Regulariza-tion with stochastic transformations and perturbations fordeep semi-supervised learning.</a></li>
              <li><a href="https://www.ijcai.org/Proceedings/2019/0504.pdf">(IJCAI 20019) Consistency regularisation: Interpolation Consistency Training for Semi-supervised Learning  Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<a href="https://proceedings.icml.cc/static/paper_files/icml/2020/705-Paper.pdf">SIGUA: Forgetting May Make Learning with Noisy Labels More Robust, Bo Han, Gang Niu, Xingrui Yu, QUANMING YAO, Miao Xu, Ivor Tsang, Masashi Sugiyama</a>
    <ul>
      <li>
        <p>We propose stochastic integrated gradient underweighted ascent (SIGUA): in a mini-batch, we adopt gradient descent on good data as usual, and learning-rate-reduced gradient ascent on bad data;</p>
      </li>
      <li>
        <p>Technically, SIGUA pulls optimization back for generalization when their goals conflict with each other;</p>
      </li>
      <li>Philosophically, SIGUA shows forgetting undesired memorization can reinforce desired memorization.
        <ul>
          <li><strong>The idea is similar with <a href="https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/">DM and IMAE</a></strong></li>
        </ul>
      </li>
      <li>Almost the same authors as <a href="https://proceedings.icml.cc/book/2020/hash/72b386224056bf940cd5b01341f65e9d">Searching to Exploit Memorization Effect in Learning with Noisy Labels QUANMING YAO, Hansi Yang, Bo Han, Gang Niu, James Kwok</a>
</li>
    </ul>
  </li>
  <li>
<a href="https://proceedings.icml.cc/paper/2020/file/87682805257e619d49b8e0dfdc14affa-Paper.pdf">Error-Bounded Correction of Noisy Labels, Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, Chao Chen</a>
    <ul>
      <li>
        <p>To be robust against label noise, many successful methods rely on the noisy classifiers (i.e., models trained on the noisy training data) to determine whether a label is trustworthy. However, it remains unknown why this heuristic works well in practice.</p>
      </li>
      <li>
        <p>In this paper, <strong>we provide the first theoretical explanation</strong> for these methods.</p>
      </li>
      <li>
        <p>We prove that the prediction of a noisy classifier can indeed be a good indicator of whether the label of a training data is clean.</p>
      </li>
      <li>
        <p>Based on the theoretical result, we propose a novel algorithm that corrects the labels based on the noisy classifier prediction. The corrected labels are <strong>consistent with the true Bayesian optimal classifier with high probability.</strong></p>
      </li>
      <li>
        <p>We prove that when the noisy classifier has <strong>low confidence on the label of a datum, such label is likely corrupted.</strong> In fact, we can quantify the threshold of confidence, below which the label is likely to be corrupted, and above which is it likely to be not. We also empirically show that the bound in our theorem is tight.</p>
      </li>
      <li>
        <p>We provide a theorem quantifying how a noisy classifier’s prediction correlates to the purity of a datum’s label. This provides theoretical explanation for data-recalibrating methods for noisy labels.</p>
      </li>
      <li>
        <p>Inspired by the theorem, we propose <strong>a new label correction algorithm with guaranteed success rate.</strong></p>
      </li>
      <li>
        <p><strong>A Bayes optimal classifier is the minimizer of the risk over all possible hypotheses.</strong></p>
      </li>
      <li>
        <p>We also have a burn-in stage in which we train the networkusing the original noisy labels for \(m\) epochs. During theburn-in stage, we use the original cross-entropy loss;</p>
      </li>
      <li>
        <p>After the burn-in stage, we want to avoid overfitting of theneural network. To achieve this goal, we introduce aretroactive loss term. The  idea  is  to  enforce  the consistency between \(f\) and the prediction of the model at a previous epoch.</p>
      </li>
      <li>
        <p>In all experiments, <strong>we use early stopping on validation set to tune hyperparameters and report theperformance on test set.</strong></p>
      </li>
      <li>
        <p><strong><a href="https://xinshaoamoswang.github.io/blogs/2020-06-07-Progressive-self-label-correction/">Simple and Effective ProSelfLC: Progressive Self Label Correction</a></strong></p>
      </li>
    </ul>
  </li>
  <li>
<a href="https://proceedings.icml.cc/paper/2020/file/f2d887e01a80e813d9080038decbbabb-Paper.pdf">Learning with Bounded Instance- and Label-dependent Label Noise, Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, Dacheng Tao</a>
    <ul>
      <li><strong>Binary classification =&gt; Not highly useful.</strong></li>
      <li>. Specifically, we introduce the concept of <strong>distilled examples</strong>, i.e. examples whose labels are identical with the labels assigned for them by the Bayes optimal classifier, and prove that <strong>under certain conditions classifiers learnt on distilled examples will converge to the Bayes optimal classifier</strong>.</li>
      <li>Inspired by the idea of learning with distilled examples, we then propose a learning algorithm with <strong>theoretical guarantees for its robustness to BILN</strong>.</li>
    </ul>
  </li>
  <li>
<a href="https://proceedings.icml.cc/book/2020/hash/77f959f119f4fb2321e9ce801e2f5163">Normalized Loss Functions for Deep Learning with Noisy Labels Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, James Bailey</a>
    <ul>
      <li>
        <p><strong>This work is motivated by <a href="https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/">DM and IMAE</a></strong></p>
      </li>
      <li>
        <p>We provide new theoretical insights into robust loss func-tions demonstrating that a simple normalization can makeany loss function robust to noisy labels.</p>
      </li>
      <li>
        <p>We identify that existing robust loss functions suffer from an underfitting problem.  To address this, we propose ageneric framework Active Passive Loss(APL) to build new loss functions with <strong>theoretically guaranteed robustness and sufficient learning properties.</strong></p>
      </li>
      <li>
        <p><strong>Robustness and Convergence?</strong></p>
      </li>
    </ul>
  </li>
  <li>
<a href="https://proceedings.icml.cc/static/paper_files/icml/2020/3285-Paper.pdf">Searching to Exploit Memorization Effect in Learning with Noisy Labels, QUANMING YAO, Hansi Yang, Bo Han, Gang Niu, James Kwok</a>
    <ul>
      <li>Sample selection approaches: select \(R(t)\) small-loss samples based on network’s predictions</li>
      <li>Formulation as an AutoML Problem  (complex algorithm personally);</li>
      <li>Bi-level optimisation</li>
      <li><strong>No sample selection is needed: <a href="https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/">DM and IMAE</a></strong></li>
      <li>Almost the same authors as <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/705-Paper.pdf">SIGUA: Forgetting May Make Learning with Noisy Labels More Robust Bo Han, Gang Niu, Xingrui Yu, QUANMING YAO, Miao Xu, Ivor Tsang, Masashi Sugiyama</a>
</li>
    </ul>
  </li>
  <li>
<a href="https://proceedings.icml.cc/static/paper_files/icml/2020/4950-Paper.pdf">Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates, Yang Liu, Hongyi Guo</a>
    <ul>
      <li>Overall, this method is complex due to <strong>peer samples</strong>.</li>
      <li>
<strong>The motivation/highlight is not novel</strong>: without Knowing Noise Rates.  Our main goal is to provide an al-ternative that does not require the specification of the noiserates, nor an additional estimation step for the noise.</li>
      <li>
        <p>Peer loss is invariant to label noise when optimizing with it. This effect helps us get rid of theestimation of noise rates.</p>
      </li>
      <li>
        <p>i) is robust to asymmetriclabel noise with <strong>formal theoretical guarantees</strong>  and  ii)  requires  no  prior  knowledge  or  estimationof the noise rates (<strong>no need for specifying noise rates</strong>).</p>
      </li>
      <li>
        <p>We also provide preliminary results on <strong>how peer loss generalizes to multi-class clas-sification problems.</strong></p>
      </li>
      <li>
        <p>Relevant work 1: <a href="https://papers.nips.cc/paper/8853-l_dmi-a-novel-information-theoretic-loss-function-for-training-deep-nets-robust-to-label-noise.pdf">neurips-19: \(L_{DMI}\): A Novel Information-theoretic Loss Functionfor Training Deep Nets Robust to Label Noise</a> To the best ofour knowledge, \(L_{DMI}\) is the first loss function that is provably robust to instance-independent label noise, regardless of noise pattern, and it can be applied to any existing classification neural networks straightforwardly without any auxiliary information. In addition to theoretical justification, we also empirically show that using \(L_{DMI}\) outperforms all other counterparts in the classification task on both image dataset and natural language dataset include Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a variety of synthesized noise patterns and noise amounts,as well as a real-world dataset Clothing1M.
  The core of \(L_{DMI}\) is a generalized version of mutual information, termed Determinant based Mutual Information (DMI), which is not only information-monotone but also relatively invariant.</p>
      </li>
      <li>
        <p>Relevant work 2: <a href="https://arxiv.org/pdf/1802.08887.pdf">Water from Two Rocks: Maximizing the Mutual Information</a></p>
      </li>
      <li><strong>No loss function is needed: <a href="https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/">DM and IMAE</a></strong></li>
    </ul>
  </li>
  <li>
    <p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Iscen_Label_Propagation_for_Deep_Semi-Supervised_Learning_CVPR_2019_paper.pdf">(CVPR-19) Ahmet Iscen et al Label Propagation for Deep Semi-supervised Learning</a></p>
  </li>
  <li>
<a href="https://proceedings.icml.cc/static/paper_files/icml/2020/6080-Paper.pdf">Progressive Identification of True Labels for Partial-Label Learning Jiaqi Lv, Miao Xu, LEI FENG, Gang Niu, Xin Geng, Masashi Sugiyama</a>
    <ul>
      <li>
<strong>Partial-label learning</strong> is one of the important <strong>weakly supervised learning problems</strong>, where each training example is equipped with <strong>a set of candidate labels</strong> that contains the true label.</li>
    </ul>
  </li>
  <li><a href="https://proceedings.icml.cc/book/2020/hash/2e2079d63348233d91cad1fa9b1361e9">Federated Learning with Only Positive Labels Felix Xinnan Yu, Ankit Singh Rawat, Aditya Menon, Sanjiv Kumar</a></li>
</ul>

<h3 id="the-design-of-loss-functions-ie-optimisation-objectives-or-output-regularistion">The design of loss functions (i.e., optimisation objectives or output regularistion)</h3>
<ul class="message">
  <li>
<img class="emoji" title=":+1:" alt=":+1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png" height="20" width="20"> <a href="https://arxiv.org/pdf/1905.11528.pdf">Improved Training Speed, Accuracy, and Data Utilization Through Loss Function Optimization</a>
    <ul>
      <li>Speed, Accuracy, Data Efficiency, etc;</li>
      <li>BAIKAL loss;</li>
      <li>Genetic Loss Function Optimization (GLO) builds loss functions hierarchically from a set of operators and leaf nodes;</li>
      <li>A general framework for loss function metalearning, covering both novel loss function discovery and optimization, is developed and evaluated experimentally.</li>
      <li><strong>No loss function is needed: <a href="https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/">DM and IMAE</a></strong></li>
    </ul>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1506.01186.pdf">Cyclical Learning Rates for Training Neural Networks</a></p>
  </li>
  <li>
<a href="https://arxiv.org/pdf/1702.05659.pdf">On loss functions for deep neural networks in classification =&gt; with theory of robustness and convergence</a>
    <ul>
      <li>We try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects;</li>
      <li>We present new insights into theoretical properties of a couple of these losses;</li>
      <li>We provide experimental evaluation of resulting models’ properties, including the effect on speed of learning, final performance, input data and label noise robustness as well as convergence.</li>
      <li>So why is using these two loss functions (\(L_1\), \(L_2\) losses) unpopular? Is there anything fundamentally wrong with this formulation from the mathematical perspective? While the following observation is not definitive, it shows an insight into what might be the issue causing slow convergence of such methods.</li>
      <li>
<strong>Lack of convexity</strong> comes from the same argument since <strong>second derivative wrt. to any weight in the final layer of the model changes sign (as it is equivalent to first derivative being non-monotonic)</strong>.</li>
    </ul>

    <p><strong>Proposition 2</strong>. \(L_1\), \(L_2\) losses applied to probabilities estimates coming
  from sigmoid (or softmax) have <strong>non-monotonic partial derivatives wrt. to the output of the final layer (and the loss is not convex nor concave wrt. to last layer weights)</strong>. Furthermore, <strong>they vanish in both infinities, which slows down learning of heavily misclassified examples</strong>.</p>

    <ul>
      <li><strong>No loss function is needed: <a href="https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/">DM and IMAE</a></strong></li>
    </ul>
  </li>
</ul>

  
</article>



<hr class="dingbat related">










<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    },
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML">
</script>




<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <ins class="adsbygoogle" style="display:block" data-ad-format="fluid" data-ad-layout-key="-ef+6k-30-ac+ty" data-ad-client="ca-pub-8231481254980115" data-ad-slot="9596964208"></ins>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
  </script>




<div class="navigator">
    
        <span style="float:left"><a href="/blogs/2020-07-03-deep-metric-learning-improvement/">« In deep metric learning, The improvements over time have been marginal?</a>
          · <a href="https://xinshaoamoswang.github.io/blogs/2020-07-03-deep-metric-learning-improvement/#disqus_thread"></a>
        </span>
    
    
</div>

#<script data-ad-client="ca-pub-8231481254980115" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
#
<script data-ad-client="ca-pub-8231481254980115" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>





<div id="disqus_thread"></div>


<script>
    var disqus_config = function () {
            this.page.url = "https://xinshaoamoswang.github.io/readingnotes/2020-09-08-RobustMLDL/";
            this.page.identifier = "/readingnotes/RobustMLDL";
        }; 

    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://xinshaowang.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>


<noscript>Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>



  

  
<footer role="contentinfo">
  <hr>
  
    <p><small class="copyright">© 2019-2020. All rights reserved.
</small></p>
  
  
  <p><small>Welcome to Xinshao Wang's Personal Website</small></p>
  <hr class="sr-only">
</footer>


</main>

    <hy-drawer class="" align="left" threshold="10" touch-events prevent-default>
  <header id="_sidebar" class="sidebar" role="banner">
    
    <div class="sidebar-bg sidebar-overlay" style="background-color:rgb(25,55,71);background-image:url(/assets/img/sidebar-bg.jpg)"></div>

    <div class="sidebar-sticky">
      <div class="sidebar-about">
        
          <a class="no-hover" href="/" tabindex="-1">
            <img src="/assets/icons/android-chrome-192x192.png" class="avatar" alt="Postdoc@OxfordU" data-ignore>
          </a>
        
        <h2 class="h1"><a href="/">Postdoc@OxfordU</a></h2>
        
        
          <p class="">
            Core Machine Learning.
Fundamental Deep Learning.

          </p>
        
      </div>

      <nav class="sidebar-nav heading" role="navigation">
        <span class="sr-only">Navigation:</span>
<ul>
  
    
      
      <li>
        <a id="_navigation" href="/blogs/" class="sidebar-nav-item">
          Blogs
        </a>
      </li>
    
      
      <li>
        <a href="/readingnotes/" class="sidebar-nav-item active">
          ReadingNotes
        </a>
      </li>
    
      
      <li>
        <a href="/projects/" class="sidebar-nav-item">
          Projects
        </a>
      </li>
    
      
      <li>
        <a href="/about/" class="sidebar-nav-item">
          About ME
        </a>
      </li>
    
  
</ul>

      </nav>

      

      <div class="sidebar-social">
        <span class="sr-only">Social:</span>
<ul>
  
    
    

    
    

    
    
  
</ul>

      </div>
    </div>
  </header>
</hy-drawer>
<hr class="sr-only" hidden>

  
</hy-push-state>

<!--[if gt IE 10]><!---->

  <script nomodule>!function(){var e=document.createElement("script");if(!("noModule"in e)&&"onbeforeload"in e){var t=!1;document.addEventListener("beforeload",function(n){if(n.target===e)t=!0;else if(!n.target.hasAttribute("nomodule")||!t)return;n.preventDefault()},!0),e.type="module",e.src=".",document.head.appendChild(e),e.remove()}}();
</script>
  <script type="module" src="/assets/js/hydejack-8.5.2.js"></script>
  <script nomodule src="/assets/js/hydejack-legacy-8.5.2.js" defer></script>
  

  


<!--<![endif]-->




<h2 class="sr-only" hidden>Templates (for web app):</h2>

<template id="_animation-template" hidden>
  <div class="animation-main fixed-top">
    <div class="content">
      <div class="page"></div>
    </div>
  </div>
</template>

<template id="_loading-template" hidden>
  <div class="loading nav-btn fr">
    <span class="sr-only">Loading…</span>
    <span class="icon-cog"></span>
  </div>
</template>

<template id="_error-template" hidden>
  <div class="page">
    <h1 class="page-title">Error</h1>
    
    
    <p class="lead">
      Sorry, an error occurred while loading <a class="this-link" href=""></a>.

    </p>
  </div>
</template>

<template id="_forward-template" hidden>
  <button id="_forward" class="forward nav-btn no-hover fl">
    <span class="sr-only">Forward</span>
    <span class="icon-arrow-right2"></span>
  </button>
</template>

<template id="_back-template" hidden>
  <button id="_back" class="back nav-btn no-hover fl">
    <span class="sr-only">Back</span>
    <span class="icon-arrow-left2"></span>
  </button>
</template>

<template id="_permalink-template" hidden>
  <a href="#" class="permalink">
    <span class="sr-only">Permalink</span>
    <span class="icon-link"></span>
  </a>
</template>





  


  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <ins class="adsbygoogle" style="display:block" data-ad-format="fluid" data-ad-layout-key="-ef+6k-30-ac+ty" data-ad-client="ca-pub-8231481254980115" data-ad-slot="9596964208"></ins>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
  </script>

</body>



<script id="dsq-count-scr" src="//xinshaowang.disqus.com/count.js" async></script>

<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>

</html>
