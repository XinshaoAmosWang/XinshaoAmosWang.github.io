<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2020-04-03T09:16:46+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Xinshao (Amos) Wang</title><subtitle>&quot;Stay Hungry. Stay Foolish. -- Steve Jobs 2005&quot;. ML/DL/AI Research with applications to CV/NLP, etc
</subtitle><author><name>Xinshao (Amos) Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><entry><title type="html">Paper Summary on Distance Metric, Representation Learning</title><link href="http://localhost:4000/blogs/2020-03-29-papers-summary-metric/" rel="alternate" type="text/html" title="Paper Summary on Distance Metric, Representation Learning" /><published>2020-03-29T00:00:00+00:00</published><updated>2020-03-29T00:00:00+00:00</updated><id>http://localhost:4000/blogs/papers-summary-metric</id><content type="html" xml:base="http://localhost:4000/blogs/2020-03-29-papers-summary-metric/">&lt;p&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;
&lt;ol class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#iccv-2019-cvpr-2020-deep-metric-learning&quot;&gt;ICCV 2019, CVPR 2020 Deep Metric Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#cvpr-2019-deep-metric-learning&quot;&gt;CVPR 2019 Deep Metric Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#few-shot-learning&quot;&gt;Few-shot Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#large-output-spaces&quot;&gt;Large Output Spaces&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#poincaré-hyperbolic-curvilinear&quot;&gt;Poincaré, Hyperbolic, Curvilinear&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#wasserstein&quot;&gt;Wasserstein&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#semi-supervised-or-unsupervised-learning&quot;&gt;Semi-supervised or Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#neurips-2019-stochastic-shared-embeddings-data-driven-regularization-of-embedding-layers&quot;&gt;NeurIPS 2019-Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;iccv-2019-cvpr-2020-deep-metric-learning&quot;&gt;ICCV 2019, CVPR 2020 Deep Metric Learning&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.10857.pdf&quot;&gt;Circle Loss: A Unified Perspective of Pair Similarity Optimization&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Motivation: aiming to maximize the within-class similarity &lt;code class=&quot;MathJax_Preview&quot;&gt;s_p&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;s_p&lt;/script&gt; and minimize the between-class similarity &lt;code class=&quot;MathJax_Preview&quot;&gt;s_n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;s_n&lt;/script&gt;. We find a majority of loss functions, including the triplet loss and the softmax plus cross-entropy loss, embed &lt;code class=&quot;MathJax_Preview&quot;&gt;s_n&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;s_n&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;s_p&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;s_p&lt;/script&gt; into similarity pairs and seek to reduce &lt;code class=&quot;MathJax_Preview&quot;&gt;(s_n − s_p)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;(s_n − s_p)&lt;/script&gt;. Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal.&lt;/li&gt;
      &lt;li&gt;Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized.&lt;/li&gt;
      &lt;li&gt;we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary.&lt;/li&gt;
      &lt;li&gt;Circle loss offers a more flexible optimization approach towards a more definite convergence target,
compared with the loss functions optimizing &lt;code class=&quot;MathJax_Preview&quot;&gt;(s_n − s_p)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;(s_n − s_p)&lt;/script&gt;.&lt;/li&gt;
      &lt;li&gt;(1)  a unified loss function; (2) flexible optimization; (3) definite convergence status.&lt;/li&gt;
      &lt;li&gt;Evaluation:
        &lt;ul&gt;
          &lt;li&gt;Tasks:
            &lt;ul&gt;
              &lt;li&gt;Face recognition&lt;/li&gt;
              &lt;li&gt;Person re-identification (Market-1501,MSMT17)&lt;/li&gt;
              &lt;li&gt;Fine-grained image retrieval (CUB-100-2011, CARS-196, SOP-11318)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Net architecture-1: ResNet50 (globla) + MGN (local features) for person reid (. Our implementation concatenates all the part features into a single feature vector for simplici);&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;Net architecture-2: GoogLeNet (BN-Inception) for CUB, CARS, SOP, 512-D embeddings;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The performance is not better than Ranked List Loss on SOP.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_Sampling_Wisely_Deep_Image_Embedding_by_Top-K_Precision_Optimization_ICCV_2019_paper.pdf&quot;&gt;Sampling Wisely: Deep Image Embedding by Top-k Precision Optimization&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;In contrast, in this paper, we propose a novel deep image embedding algorithm with end-to-end optimization to top-k precision, the evaluation metric that is &lt;strong&gt;closely related to user experience.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Specially, our loss function is constructed with &lt;strong&gt;Wisely Sampled “misplaced” images along the top-k nearest neighbor decision boundary,&lt;/strong&gt; so that the gradient descent update directly
promotes the concerned metric, top-k precision.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Our theoretical analysis&lt;/strong&gt; on the upper bounding and consistency properties of the proposed loss supports that minimizing our proposed loss is equivalent to maximizing top-k precision&lt;/li&gt;
      &lt;li&gt;Evaluation:
        &lt;ul&gt;
          &lt;li&gt;Datasets: CUB-200-2011, CARS-196, SOP&lt;/li&gt;
          &lt;li&gt;PyTorch + Adam&lt;/li&gt;
          &lt;li&gt;Net architecture: Densenet 201, GoogLeNet V2 (Inception with BN)&lt;/li&gt;
          &lt;li&gt;Finetuning&lt;/li&gt;
          &lt;li&gt;Embedding size: 64, 512?&lt;/li&gt;
          &lt;li&gt;Input size: warp (256x256) =&amp;gt; crop (227x227)&lt;/li&gt;
          &lt;li&gt;Testing: only center crop&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The performance is not better than Ranked List Loss&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cvpr-2019-deep-metric-learning&quot;&gt;CVPR 2019 Deep Metric Learning&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Sanakoyeu_Divide_and_Conquer_the_Embedding_Space_for_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Divide and Conquer the Embedding Space for Metric Learning&lt;/a&gt; 
:+1:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Each learner will learn a separate distance metric using only a subspace of the original embedding space and &lt;strong&gt;a part of the data&lt;/strong&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Natural hard negatives mining: Finally, &lt;strong&gt;the splitting and sampling connect to hard negative mining&lt;/strong&gt;, which is verified by them. (I appreciate this ablation study in Table 6 )&lt;/li&gt;
      &lt;li&gt;Divide means: 
  (1) Splitting the training data into K Clusters; 
  (2) Splitting the embedding into K Slices.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Cakir_Deep_Metric_Learning_to_Rank_CVPR_2019_paper.pdf&quot;&gt;Deep Metric Learning to Rank&lt;/a&gt; :+1:
    &lt;ul&gt;
      &lt;li&gt;Our main contribution is a novel solution to optimizing Average Precision under the Euclidean metric, based on the probabilistic interpretation of AP as the area under precision-recall curve, as well as distance quantization.&lt;/li&gt;
      &lt;li&gt;We also propose a category-based minibatch sampling strategy and a large-batch training heuristic.&lt;/li&gt;
      &lt;li&gt;On three &lt;strong&gt;few-shot image retrieval datasets&lt;/strong&gt;, FastAP consistently outperforms competing methods, which often involve complex optimization heuristics or costly model ensembles.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Multi-Similarity Loss With General Pair Weighting for Deep Metric Learning&lt;/a&gt; :+1:
    &lt;ul&gt;
      &lt;li&gt;Objective of the proposed multi-similarity loss, which aims to collect informative pairs, and weight these pairs through their own and relative similarities.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1903.03238.pdf&quot;&gt;Ranked List Loss for Deep Metric Learning&lt;/a&gt; :+1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Suh_Stochastic_Class-Based_Hard_Example_Mining_for_Deep_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Stochastic Class-Based Hard Example Mining for Deep Metric Learning&lt;/a&gt; :+1:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Scale linearly to the number of classes.&lt;/li&gt;
      &lt;li&gt;The methods proposed by Movshovitz-Attias et al. [14] and Wen et al. [34] are related to ours in a sense that class representatives are jointly trained with the feature extractor. 
However, their goal is to formulate new losses using the class representatives whereas we use them for hard negative mining.&lt;/li&gt;
      &lt;li&gt;Given an anchor instance, our algorithm first selects a few hard negative classes based on the class-to-sample distances and then performs a refined search in an instance-level only from the selected classes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Theoretically Sound Upper Bound on the Triplet Loss for Improving the Efficiency of Deep Distance Metric Learning :+1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unsupervised&lt;/strong&gt; Embedding Learning via Invariant and Spreading Instance Feature :+1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Yuan_Signal-To-Noise_Ratio_A_Robust_Distance_Metric_for_Deep_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Signal-To-Noise Ratio: A Robust Distance Metric for Deep Metric Learning&lt;/a&gt; :+1:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;We propose a robust SNR distance metric based on Signal-to-Noise Ratio (SNR) for measuring the similarity of image pairs for deep metric learning. Compared with Euclidean distance metric, our SNR distance metric can further jointly reduce the intra-class distances and enlarge the inter-class distances for learned features.&lt;/li&gt;
      &lt;li&gt;SNR in signal processing is used to measure the level of a desired signal to the level of noise, and a larger SNR value means a higher signal quality.
  For similarity measurement in deep metric learning, a pair of learned features x and y can be given as y = x + n, where n can be treated as a noise. Then, the SNR is the ratio of the feature variance and the noise variance.&lt;/li&gt;
      &lt;li&gt;To show the generality of our SNR-based metric, we also extend our approach to hashing retrieval learning.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Branchaud-Charron_Spectral_Metric_for_Dataset_Complexity_Assessment_CVPR_2019_paper.pdf&quot;&gt;Spectral Metric for Dataset Complexity Assessment&lt;/a&gt; :+1:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Related work: &lt;a href=&quot;https://openreview.net/forum?id=ryup8-WCW&quot;&gt;Measuring the Intrinsic Dimension of Objective Landscapes ICLR 2018&lt;/a&gt;, 
  &lt;a href=&quot;https://arxiv.org/abs/1808.03591&quot;&gt;How Complex is your classification problem? A survey on measuring classification complexity Survey on complexity measures&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Deep Asymmetric Metric Learning via Rich Relationship Mining :+1:
    &lt;ul&gt;
      &lt;li&gt;DAMLRRM relaxes the constraint on positive pairs to extend the generalization capability. We build positive pairs training pool by constructing a minimum connected tree for each category instead of considering all positive pairs within a mini-batch. As a result, there will exist a direct or indirect path between any positive pair, which ensures the relevance being bridged to each other. The inspiration comes from ranking on manifold [58] that spreads the relevance to their nearby neighbors one by one.&lt;/li&gt;
      &lt;li&gt;Idea is novel. The results on SOP are not good, only 69.7 with GoogLeNet&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Hybrid-Attention_Based_Decoupled_Metric_Learning_for_Zero-Shot_Image_Retrieval_CVPR_2019_paper.pdf&quot;&gt;Hybrid-Attention Based Decoupled Metric Learning for Zero-Shot Image Retrieval&lt;/a&gt; :-1:
    &lt;ul&gt;
      &lt;li&gt;Very complex: object attention, spatial attention, random walk graph, etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.09626.pdf&quot;&gt;Deep Metric Learning Beyond Binary Supervision&lt;/a&gt; :-1:
    &lt;ul&gt;
      &lt;li&gt;Binary supervision indicating whether a pair of images are of the same class or not.&lt;/li&gt;
      &lt;li&gt;Using continuous labels&lt;/li&gt;
      &lt;li&gt;Learn the degree of similarity rather than just the order.&lt;/li&gt;
      &lt;li&gt;A triplet mining strategy adapted to metric learning with continuous labels.&lt;/li&gt;
      &lt;li&gt;Image retrieval tasks with continuous labels in terms of human poses, room layouts and image captions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hardness-aware deep metric learning 
:-1: : data augmentation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensemble Deep Manifold Similarity Learning using Hard Proxies :-1: random walk algorithm, ensemble models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Re-Ranking via Metric Fusion for Object Retrieval and Person Re-Identification :-1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Deep Embedding Learning With Discriminative Sampling Policy :-1:&lt;/li&gt;
  &lt;li&gt;Point Cloud Oversegmentation With Graph-Structured Deep Metric Learning :-1:&lt;/li&gt;
  &lt;li&gt;Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval :-1:&lt;/li&gt;
  &lt;li&gt;A Compact Embedding for Facial Expression Similarity :-1:&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Karlinsky_RepMet_Representative-Based_Metric_Learning_for_Classification_and_Few-Shot_Object_Detection_CVPR_2019_paper.pdf&quot;&gt;RepMet: Representative-Based Metric Learning for Classification and Few-Shot Object Detection&lt;/a&gt; :-1:&lt;/li&gt;
  &lt;li&gt;Eliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking :-1:&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;few-shot-learning&quot;&gt;&lt;a href=&quot;/my_docs/few-shot/&quot;&gt;Few-shot Learning&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;ICLR 2018-Meta-Learning for Semi-Supervised Few-Shot Classification&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Unsupervised Meta Learning for Few-Show Image Classification&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Learning to Self-Train for Semi-Supervised Few-Shot Classification&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Adaptive Cross-Modal Few-shot Learning&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Cross Attention Network for Few-shot Classification&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Incremental Few-Shot Learning with Attention Attractor Networks&lt;/li&gt;
  &lt;li&gt;ICML 2019-LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;large-output-spaces&quot;&gt;&lt;a href=&quot;/my_docs/large-output-spaces/&quot;&gt;Large Output Spaces&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;NeurIPS 2019-Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces&lt;/li&gt;
  &lt;li&gt;AISTATS 2019-Stochastic Negative Mining for Learning with Large Output Spaces&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;poincaré-hyperbolic-curvilinear&quot;&gt;&lt;a href=&quot;/my_docs/Poincare-Hyperbolic-Curvilinear/&quot;&gt;Poincaré, Hyperbolic, Curvilinear&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;NeurIPS 2019-Multi-relational Poincaré Graph Embeddings&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Numerically Accurate Hyperbolic Embeddings Using Tiling-Based Models&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Curvilinear Distance Metric Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;wasserstein&quot;&gt;&lt;a href=&quot;/my_docs/wasserstein/&quot;&gt;Wasserstein&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;NeurIPS 2019-Generalized Sliced Wasserstein Distances&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Tree-Sliced Variants of Wasserstein Distances&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Sliced Gromov-Wasserstein&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Wasserstein Dependency Measure for Representation Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semi-supervised-or-unsupervised-learning&quot;&gt;&lt;a href=&quot;/my_docs/Semi-Un-Supervised-Learning/&quot;&gt;Semi-supervised or Unsupervised Learning&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;CVPR 2019-Label Propagation for Deep Semi-supervised Learning&lt;/li&gt;
  &lt;li&gt;NeurIPS 2017-Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results&lt;/li&gt;
  &lt;li&gt;ICLR 2019-Unsupervised Learning via Meta-Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;neurips-2019-stochastic-shared-embeddings-data-driven-regularization-of-embedding-layers&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.10630.pdf&quot;&gt;NeurIPS 2019-Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers&lt;/a&gt;&lt;/h2&gt;
&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: 
In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters.&lt;strong&gt;Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope to reduce statistical complexity.&lt;/strong&gt; Alternatively, we propose stochastically shared embeddings (SSE), a data-driven approach to regularizing embedding layers, which stochastically transitions between embeddings during stochastic gradient descent (SGD). Because SSE integrates seamlessly with existing SGD algorithms, it can be used with only minor modifications when training large scale neural networks. We develop two versions of SSE: SSE-Graph using knowledge graphs of embeddings; SSE-SE using no prior information. We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages. &lt;strong&gt;We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed SSE can further reduce overfitting, which often leads to more favorable generalization results.&lt;/strong&gt; &lt;br /&gt;
We conducted &lt;strong&gt;experiments for a total of 6 tasks from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages.&lt;/strong&gt;&lt;/p&gt;</content><author><name>Xinshao (Amos) Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest. ICCV 2019, CVPR 2020 Deep Metric Learning CVPR 2019 Deep Metric Learning Few-shot Learning Large Output Spaces Poincaré, Hyperbolic, Curvilinear Wasserstein Semi-supervised or Unsupervised Learning NeurIPS 2019-Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">Example Weighting, Importance Sampling?</title><link href="http://localhost:4000/blogs/2020-03-08-example-weighting/" rel="alternate" type="text/html" title="Example Weighting, Importance Sampling?" /><published>2020-03-08T00:00:00+00:00</published><updated>2020-03-08T00:00:00+00:00</updated><id>http://localhost:4000/blogs/example-weighting</id><content type="html" xml:base="http://localhost:4000/blogs/2020-03-08-example-weighting/">&lt;ol class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#importance-samplinggradient&quot;&gt;Importance sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;importance-samplinggradient&quot;&gt;Importance sampling–Gradient?&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v80/katharopoulos18a.html&quot;&gt;Not All Samples Are Created Equal: Deep Learning with Importance Sampling-Angelos Katharopoulos et al&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v37/zhaoa15.pdf&quot;&gt;Stochastic Optimization with Importance Sampling for Regularized Loss Minimization-Peilin Zhao, Tong Zhang&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1401.2753.pdf&quot;&gt;Stochastic Optimization with Importance Sampling-Peilin Zhao et al&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume19/16-241/16-241.pdf&quot;&gt;Importance Sampling for Minibatches-Dominik Csiba et al&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/7025-safe-adaptive-importance-sampling.pdf&quot;&gt;Safe Adaptive Importance Sampling-Sebastian U. Stich et al&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao (Amos) Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">Importance sampling</summary></entry><entry><title type="html">I Love Learning and Applying Mathematics, Statistics!</title><link href="http://localhost:4000/blogs/2020-03-04-I-love-math/" rel="alternate" type="text/html" title="I Love Learning and Applying Mathematics, Statistics!" /><published>2020-03-04T00:00:00+00:00</published><updated>2020-03-04T00:00:00+00:00</updated><id>http://localhost:4000/blogs/I-love-math</id><content type="html" xml:base="http://localhost:4000/blogs/2020-03-04-I-love-math/">&lt;ol class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#sgd--newtons-method--second-order-derivative-optimisation&quot;&gt;SGD &amp;amp; Newton’s Method &amp;amp; Second-order Derivative Optimisation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#linear-algebra&quot;&gt;Linear algebra&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#probabilistic-view-of-the-world&quot;&gt;Probabilistic view of the world&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#optimisation&quot;&gt;Optimisation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;sgd--newtons-method--second-order-derivative-optimisation&quot;&gt;SGD &amp;amp; Newton’s Method &amp;amp; Second-order Derivative Optimisation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Newton%27s_method&quot;&gt;Newton’s Method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization&quot;&gt;Newton’s Method: Second-order Derivative Optimisation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;linear-algebra&quot;&gt;Linear algebra&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.robots.ox.ac.uk/~davidc/pubs/tt2015_dac1.pdf&quot;&gt;Properties of the Covariance Matrix&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mathsisfun.com/data/correlation.html&quot;&gt;Covariance and Correlation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Interpreting: &lt;strong&gt;Variable = Axis&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Variable = axis/component/one hyper-line&lt;/li&gt;
          &lt;li&gt;Observations of this variable = points of this axis/hyper-line&lt;/li&gt;
          &lt;li&gt;The observations of a variable = one vector of points in this line.&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;E(XY)=&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;E(XY)=&lt;/script&gt; the dot product of two variables’ observation vector (multiple points for each variable)&lt;/li&gt;
          &lt;li&gt;If &lt;code class=&quot;MathJax_Preview&quot;&gt;X,Y&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X,Y&lt;/script&gt; are orthogonal, then for any point in &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and any point in &lt;code class=&quot;MathJax_Preview&quot;&gt;Y&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;, their dot product are zero, therefore, we have &lt;code class=&quot;MathJax_Preview&quot;&gt;E(XY) = 0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;E(XY) = 0&lt;/script&gt;.&lt;/li&gt;
          &lt;li&gt;Diagonalisation (Orthogonal, Making them independent) =&amp;gt;  Decorrelation
            &lt;ul&gt;
              &lt;li&gt;In this context, uncorrelation = independence.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Uncorrelation (Orthogonalisaion) using Eigen vectors
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Projection of a normal distribtion&lt;/strong&gt; &lt;code class=&quot;MathJax_Preview&quot;&gt;X \sim \mathcal{N}(0,\,{\sigma_x}^2)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X \sim \mathcal{N}(0,\,{\sigma_x}^2)&lt;/script&gt; to a standard normal distribution &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{N}(0,\,1)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(0,\,1)&lt;/script&gt;:
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;the projected distribution&lt;/strong&gt; is &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)&lt;/script&gt;&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; determines their linear dependency: correlation coefficient=&lt;code class=&quot;MathJax_Preview&quot;&gt;\cos(\theta)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\cos(\theta)&lt;/script&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;For each eigen vector: &lt;strong&gt;we project all the original variables to this eigen vector&lt;/strong&gt; (an axis in the transformed orthogonal space) =&amp;gt; summarise/accumulate those projected variables in this axis =&amp;gt; New variable&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;Two accumulated projected new variables of two axises (eigen vectors) are uncorrelated (being independent now).&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;Eigen value = sum of projected standard deviation.&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The number of variables, the number of observations, the number of axises/components
        &lt;ul&gt;
          &lt;li&gt;The number of variables = the number of axis.&lt;/li&gt;
          &lt;li&gt;When the number of variables (feature dim) &amp;gt; the number of observations?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;In a high-dimensional space with orthogonal axises:
        &lt;ul&gt;
          &lt;li&gt;Each axis is one independent event/variable. (Without losing generality, feel free to treat it as an unit normal distribution &lt;code class=&quot;MathJax_Preview&quot;&gt;~\sim \mathcal{N}(0,\,1)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;~\sim \mathcal{N}(0,\,1)&lt;/script&gt;)&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;The sum of two independent normal random variables is also normal.&lt;/strong&gt; However, if the two normal random variables are not independent, then their sum is not necessarily normal.&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Then, the whole space becomes the combination (summarisation) of multiple independent normal random variables.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;In this context, &lt;strong&gt;uncorrelation = independence.&lt;/strong&gt;
  Independent variables = indepdent axis/components = orthogonal components.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Covariance:
        &lt;ul&gt;
          &lt;li&gt;remove mean for each axis/variable&lt;/li&gt;
          &lt;li&gt;projection (accumulation/expectation of dot product of observations from different axis/variable)&lt;/li&gt;
          &lt;li&gt;In other words, dot product of two points (dim &amp;gt;= 2) from two axises (out of multiple axises) =&amp;gt; expectation/accumulation&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Dot product of two variable/axis&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Correlation = &lt;code class=&quot;MathJax_Preview&quot;&gt;\frac{Covariance(X_i, X_j)} { \sigma_i \times \sigma_j } \in [-1, 1]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{Covariance(X_i, X_j)} { \sigma_i \times \sigma_j } \in [-1, 1]&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;For easier and intuitive understanding, looking at &lt;code class=&quot;MathJax_Preview&quot;&gt;\frac{(X_i - E(X_i))} { \sigma_i } \sim \mathcal{N}(0,\,1) = \mathbf{e}_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{(X_i - E(X_i))} { \sigma_i } \sim \mathcal{N}(0,\,1) = \mathbf{e}_i&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;\frac{(X_j - E(X_j))} { \sigma_j } \sim \mathcal{N}(0,\,1) = \lambda\mathbf{e}_i + \sqrt{(1-\lambda^2)} \mathbf{e}_j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{(X_j - E(X_j))} { \sigma_j } \sim \mathcal{N}(0,\,1) = \lambda\mathbf{e}_i + \sqrt{(1-\lambda^2)} \mathbf{e}_j&lt;/script&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;\mathbf{e}_i \sim \mathcal{N}(0,\,1)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbf{e}_i \sim \mathcal{N}(0,\,1)&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathbf{e}_j \sim \mathcal{N}(0,\,1)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbf{e}_j \sim \mathcal{N}(0,\,1)&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;\mathbf{e}_i \text{ and } \mathbf{e}_j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbf{e}_i \text{ and } \mathbf{e}_j&lt;/script&gt; are two variables in two orthogonal axises.&lt;/li&gt;
          &lt;li&gt;Correlation = &lt;code class=&quot;MathJax_Preview&quot;&gt;\lambda \in [-1, 1]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\lambda \in [-1, 1]&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;Covariance = &lt;code class=&quot;MathJax_Preview&quot;&gt;\lambda \sigma_i \sigma_j&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\lambda \sigma_i \sigma_j&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;
            &lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
E\{\frac{(X_i - E(X_i))} { \sigma_i }  \frac{(X_j - E(X_j))} { \sigma_j } \} &amp;amp;= E\{ \mathbf{e}_i \cdot \lambda\mathbf{e}_i + \sqrt{(1-\lambda^2)} \mathbf{e}_j \} \\
&amp;amp;= \lambda E\{ \mathbf{e}_i \cdot \mathbf{e}_i  \} + \sqrt{(1-\lambda^2)} E\{ \mathbf{e}_i \cdot \mathbf{e}_j \} \\
&amp;amp;= \lambda .
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
            &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
E\{\frac{(X_i - E(X_i))} { \sigma_i }  \frac{(X_j - E(X_j))} { \sigma_j } \} &amp;= E\{ \mathbf{e}_i \cdot \lambda\mathbf{e}_i + \sqrt{(1-\lambda^2)} \mathbf{e}_j \} \\
&amp;= \lambda E\{ \mathbf{e}_i \cdot \mathbf{e}_i  \} + \sqrt{(1-\lambda^2)} E\{ \mathbf{e}_i \cdot \mathbf{e}_j \} \\
&amp;= \lambda .
\end{aligned} %]]&gt;&lt;/script&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
E\{ {(X_i - E(X_i))}  \cdot {(X_j - E(X_j))}  \} =  \lambda  \sigma_i  \sigma_j.
  \end{aligned}&lt;/code&gt;&lt;/pre&gt;
            &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
E\{ {(X_i - E(X_i))}  \cdot {(X_j - E(X_j))}  \} =  \lambda  \sigma_i  \sigma_j.
  \end{aligned}&lt;/script&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Correlation_and_dependence#Correlation_and_independence&quot;&gt;Correlation and dependence -1&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Correlation and dependencd are totally different concepts/terms for discribing two random variables.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;In the special case&lt;/strong&gt; when &lt;code class=&quot;MathJax_Preview&quot;&gt;{\displaystyle X}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;{\displaystyle X}&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;{\displaystyle Y}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;{\displaystyle Y}&lt;/script&gt; are jointly normal, uncorrelatedness is equivalent to independence.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.probabilitycourse.com/chapter5/5_3_2_bivariate_normal_dist.php&quot;&gt;Correlation and dependence-2-Bivariate Normal Distribution&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The sum of two independent normal random variables is also normal. However, &lt;strong&gt;if the two normal random variables are not independent, then their sum is not necessarily normal.&lt;/strong&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;If &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;Y&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; are &lt;strong&gt;independent&lt;/strong&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;P(Y\|X)=P(Y) =&amp;gt; E(XY)=E(X)E(Y) =&amp;gt; Cov(X,Y)=E((X-E(X))(Y-E(Y)))=E(XY)-E(X)E(Y) = 0 =&amp;gt;&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;P(Y\|X)=P(Y) =&gt; E(XY)=E(X)E(Y) =&gt; Cov(X,Y)=E((X-E(X))(Y-E(Y)))=E(XY)-E(X)E(Y) = 0 =&gt;&lt;/script&gt;&lt;strong&gt;Uncorrelated&lt;/strong&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;If &lt;code class=&quot;MathJax_Preview&quot;&gt;X&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;code class=&quot;MathJax_Preview&quot;&gt;Y&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; are uncorrelated, &lt;code class=&quot;MathJax_Preview&quot;&gt;Cov(X,Y)=0 =&amp;gt; E(XY) = E(X)E(Y) =&amp;gt; ?&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Cov(X,Y)=0 =&gt; E(XY) = E(X)E(Y) =&gt; ?&lt;/script&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;http://home.iitk.ac.in/~zeeshan/pdf/The%20Bivariate%20Normal%20Distribution.pdf&quot;&gt;Zero Correlation Implies Independence&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.actuaries.org.uk/system/files/documents/pdf/correlation.pdf&quot;&gt;Why are we interested in
correlation/dependency?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.probabilitycourse.com/chapter5/5_2_3_conditioning_independence.php&quot;&gt;Conditioning and Independence&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Positive semi-definite&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Eigen vectors and Diagonalisation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;&quot;&gt;Eigen values and Determinant&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Mahalanobis distance&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Transformation/projection to the orthogonal space, where one axis is an independent normal distribution&lt;/li&gt;
      &lt;li&gt;One each axis: compute the distance &lt;code class=&quot;MathJax_Preview&quot;&gt;\frac{(x-u)^T(x-u)}{\sigma^2}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{(x-u)^T(x-u)}{\sigma^2}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;Summarise all the distances of all axises.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;probabilistic-view-of-the-world&quot;&gt;Probabilistic view of the world&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;Basic concepts/terms
    &lt;ul&gt;
      &lt;li&gt;Covariance and correlation&lt;/li&gt;
      &lt;li&gt;Bivariate Normal Distribution&lt;/li&gt;
      &lt;li&gt;The sum of two &lt;strong&gt;independent&lt;/strong&gt; normal distributions&lt;/li&gt;
      &lt;li&gt;Distance = &lt;code class=&quot;MathJax_Preview&quot;&gt;num \times standard~deviation&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;num \times standard~deviation&lt;/script&gt;
  =&amp;gt; Mahalanobis distance&lt;/li&gt;
      &lt;li&gt;Diagonalisation (Orthogonal, Making them independent) =&amp;gt;  Decorrelation
        &lt;ul&gt;
          &lt;li&gt;In this context, uncorrelation = independence.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Uncorrelation (Orthogonalisaion) using Eigen vectors
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Projection of a normal distribtion&lt;/strong&gt; &lt;code class=&quot;MathJax_Preview&quot;&gt;X \sim \mathcal{N}(0,\,{\sigma_x}^2)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X \sim \mathcal{N}(0,\,{\sigma_x}^2)&lt;/script&gt; to a standard normal distribution &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{N}(0,\,1)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(0,\,1)&lt;/script&gt;:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;the projected distribution&lt;/strong&gt; is &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; determines their linear dependency: correlation coefficient=&lt;code class=&quot;MathJax_Preview&quot;&gt;\cos(\theta)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\cos(\theta)&lt;/script&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For each eigen vector: &lt;strong&gt;we project all the original variables to this eigen vector&lt;/strong&gt; (an axis in the transformed orthogonal space) =&amp;gt; summarise/accumulate those projected variables in this axis =&amp;gt; New variable&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Two accumulated projected new variables of two axises (eigen vectors) are uncorrelated (being independent now).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Eigen value = sum of projected standard deviation.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Eigen vectors are orthogonal, serving as independent axises where independent variables lie in.
        &lt;ul&gt;
          &lt;li&gt;Diagonal covariance matrix: each entry is the square of eigen value.&lt;/li&gt;
          &lt;li&gt;Eigen value = sum of projected standard deviation.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Projection of a normal distribtion&lt;/strong&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;the projected distribution&lt;/strong&gt; is &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)&lt;/script&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Mahalanobis_distance&quot;&gt;Mahalanobis distance&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance.html&quot;&gt;Distance is not always what it seems&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://blogs.sas.com/content/iml/2012/02/02/detecting-outliers-in-sas-part-3-multivariate-location-and-scatter.html&quot;&gt;Detecting outliers in SAS: Part 3: Multivariate location and scatter &amp;amp; MCD: Robust estimation by subsampling&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables&quot;&gt;Sum of normally distributed random variables-Independent random variables&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com.cnpmjs.org/XinshaoAmosWang/DerivativeManipulation/blob/master/OutlierDetection_RobustInference.pptx.pdf&quot;&gt;Outlier, Anomaly, and Adversaries Detection using Mahalanobis distance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;The Mahalanobis distance is a measure of the &lt;strong&gt;distance between a point &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathrm{P}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathrm{P}&lt;/script&gt; and a distribution &lt;code class=&quot;MathJax_Preview&quot;&gt;\mathbf{D}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbf{D}&lt;/script&gt;.&lt;/strong&gt;
  It is a multi-dimensional generalization of the idea of measuring how many standard deviations away P is from the mean of D. This distance is zero if P is at the mean of D, and grows as P moves away from the mean along &lt;strong&gt;each principal component axis.&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space.&lt;/li&gt;
          &lt;li&gt;The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Mahalanobis distance is proportional, for a normal distribution, to the square root of the negative log likelihood (after adding a constant so the minimum is at zero).&lt;/li&gt;
      &lt;li&gt;This intuitive approach can be made quantitative by defining &lt;strong&gt;the normalized distance between the test point and the set to be &lt;code class=&quot;MathJax_Preview&quot;&gt;{\displaystyle {x-\mu } \over \sigma }&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;{\displaystyle {x-\mu } \over \sigma }&lt;/script&gt;&lt;/strong&gt;. By plugging this into the normal distribution we can derive the probability of the test point belonging to the set.&lt;/li&gt;
      &lt;li&gt;Were the distribution to be decidedly non-spherical, for instance ellipsoidal, then we would expect the probability of the test point belonging to the set to depend not only on the distance from the center of mass, but also on the direction. In those directions where the ellipsoid has a short axis the test point must be closer, while in those where the axis is long the test point can be further away from the center.&lt;/li&gt;
      &lt;li&gt;The Mahalanobis distance is the distance of the test point from the center of mass &lt;strong&gt;divided by the width of the ellipsoid in the direction of the test point.&lt;/strong&gt; (distance normalisation)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;Maximum likelihood estimation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.robots.ox.ac.uk/~davidc/pubs/tt2015_dac1.pdf&quot;&gt;Properties of the Covariance Matrix&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_entropy&quot;&gt;Differential entropy&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://math.stackexchange.com/questions/889425/what-does-determinant-of-covariance-matrix-give&quot;&gt;What does Determinant of Covariance Matrix give&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/89952/why-do-we-use-the-determinant-of-the-covariance-matrix-when-using-the-multivaria&quot;&gt;Why do we use the determinant of the covariance matrix when using the multivariate normal?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimisation&quot;&gt;Optimisation&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Concave_function&quot;&gt;Concave function&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao (Amos) Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">SGD &amp;amp; Newton’s Method &amp;amp; Second-order Derivative Optimisation Linear algebra Probabilistic view of the world Optimisation</summary></entry><entry><title type="html">Papers Related to My PhD Thesis-Example Weighting in Classification and Distance Metric Learning</title><link href="http://localhost:4000/paperlists/2020-02-24-ThesisRelatedPapers/" rel="alternate" type="text/html" title="Papers Related to My PhD Thesis-Example Weighting in Classification and Distance Metric Learning" /><published>2020-02-24T00:00:00+00:00</published><updated>2020-02-24T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/ThesisRelatedPapers</id><content type="html" xml:base="http://localhost:4000/paperlists/2020-02-24-ThesisRelatedPapers/">&lt;ol class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#distance-metric-learning-learning-to-retrieve&quot;&gt;Learning to Retrieve&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#learning-to-classify&quot;&gt;Learning to Classify &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#others&quot;&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;distance-metric-learning-learning-to-retrieve&quot;&gt;Distance Metric Learning: Learning to Retrieve&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1910.04115.pdf&quot;&gt;Active Ordinal Querying for Tuplewise Similarity Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;&gt;Representation Learning with
Contrastive Predictive Coding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning-to-classify&quot;&gt;Learning to Classify&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;others&quot;&gt;Others&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;A general and Adaptive Robust Loss Function-CVPR 2019 Best Paper Finalist&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Unsupervised Embedding Learning via Invariant and Spreading Instance Feature-CVPR 2019&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Regularising Deep Neural Networks by Noise: Its Interpretation and Optimisation-NeurIPS 2017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks-NeurIPS 2017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Selfie: Refurbishing Unclean Samples for Robust Deep Learning-ICML 2019&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Unsupervised Label Noise Modeling and Loss Correction-ICML 2019&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise-NeurIPS 2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;example-weighting&quot;&gt;Example Weighting&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/publication/328731166_Weighted_Machine_Learning&quot;&gt;Weighted Machine Learning-Mahdi Hashemi∗, Hassan A. Karimi&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v80/katharopoulos18a.html&quot;&gt;Not All Samples Are Created Equal: Deep Learning with Importance Sampling-Angelos Katharopoulos, Franc¸ois Fleuret, ICML 2018&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;computing the importance score for the whole
dataset is still prohibitive and would render the method
unsuitable for online learning.&lt;/li&gt;
      &lt;li&gt;In order to solve the problem of computing the importance for the whole dataset, we pre-sample a large batch of data points, compute the sampling distribution for that batch and re-sample a smaller batch with replacement.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao (Amos) Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">Learning to Retrieve Learning to Classify Others</summary></entry><entry><title type="html">Learning Bayesian Deep Learning, Uncertainty &amp;amp; Variational Techniques</title><link href="http://localhost:4000/blogs/2020-02-21-learn-bayesian-DL/" rel="alternate" type="text/html" title="Learning Bayesian Deep Learning, Uncertainty &amp; Variational Techniques" /><published>2020-02-21T00:00:00+00:00</published><updated>2020-02-21T00:00:00+00:00</updated><id>http://localhost:4000/blogs/learn-bayesian-DL</id><content type="html" xml:base="http://localhost:4000/blogs/2020-02-21-learn-bayesian-DL/">&lt;ol class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#blogs&quot;&gt;Blogs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#papers-on-theories&quot;&gt;Papers on Theories&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#papers-on-applications&quot;&gt;Papers on Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;what-am-i-working-on-now-discussions-are-welcome&quot;&gt;What am I working on now? Discussions are Welcome!&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;../2020-02-18-code-releasing&quot;&gt;Interpreting &lt;code class=&quot;MathJax_Preview&quot;&gt;p(y\|x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p(y\|x)&lt;/script&gt; and modelling example weighting&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Going to stop treating &lt;code class=&quot;MathJax_Preview&quot;&gt;p(y\|x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p(y\|x)&lt;/script&gt; as a classfication confidence metric, since it is determinstic. &lt;code class=&quot;MathJax_Preview&quot;&gt;p(y\|x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p(y\|x)&lt;/script&gt;  is not for deciding whether certain or uncertain.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;p(y\|x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p(y\|x)&lt;/script&gt; is good as a metric of whether x matches y, though not a good metric indicating whether x is blur or not.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Utilities of Uncertainties&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;blogs&quot;&gt;Blogs&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.inference.vc/everything-that-works-works-because-its-bayesian-2/&quot;&gt;Everything that Works Works Because it’s Bayesian: Why Deep Nets Generalize?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.facebook.com/yann.lecun/posts/10154058859142143&quot;&gt;Yann LeCun’s Comments&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mlg.eng.cam.ac.uk/yarin/blog_2248.html?fbclid=IwAR1lNokscvPVsGFICXDQBhVa2bweIq-mkft6EfUkj9CR8tAIYJ7mNy3Qag8&quot;&gt;YARIN GAL’s PhD Thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;papers-on-theories&quot;&gt;Papers on Theories&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1506.02142.pdf&quot;&gt;Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning-ICML 2016-YARIN GAL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf&quot;&gt;YARIN GAL’s PhD Thesis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=BJij4yg0Z&quot;&gt;A Bayesian Perspective on Generalization and Stochastic Gradient Descent-ICLR 2018 Google Brain-Samuel L. Smith and Quoc V. Le&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.08791.pdf&quot;&gt;Bayesian Deep Learning and a Probabilistic Perspective of Generalization–arXiv 2020 New York University-Andrew Gordon Wilson Pavel Izmailov&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.04933.pdf&quot;&gt;Sharp Minima Can Generalize For Deep Nets-ICML 2017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cbmm.mit.edu/sites/default/files/publications/CBMM-Memo-067.pdf&quot;&gt;Theory of Deep Learning III: Generalization Properties of SGD&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=H1oyRlYgg&quot;&gt;On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima-ICLR 2017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning&quot;&gt;The Marginal Value of Adaptive Gradient Methods in Machine Learning-NIPS 2017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume18/17-214/17-214.pdf&quot;&gt;Stochastic Gradient Descent as Approximate Bayesian Inference-JMLR 2017&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v48/mandt16.pdf&quot;&gt;A Variational Analysis of Stochastic Gradient Algorithms-ICML 2016&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1503.02406.pdf&quot;&gt;Deep Learning and the Information Bottleneck Principle&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1912.13480.pdf&quot;&gt;On the Difference Between the Information Bottleneck and the Deep Information Bottleneck&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v80/belghazi18a/belghazi18a.pdf&quot;&gt;Mutual Information Neural Estimation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;papers-on-applications&quot;&gt;Papers on Applications&lt;/h3&gt;

&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Robust_Person_Re-Identification_by_Modelling_Feature_Uncertainty_ICCV_2019_paper.pdf&quot;&gt;Robust Person Re-Identification by Modelling Feature Uncertainty&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.09658.pdf&quot;&gt;Probabilistic Face Embeddings&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.04692v1.pdf&quot;&gt;Rethinking Person Re-Identification with Confidence&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.04865.pdf&quot;&gt;Learning Confidence for Out-of-Distribution Detection in Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=ryiAv2xAZ&quot;&gt;Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao (Amos) Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">Blogs Papers on Theories Papers on Applications</summary></entry><entry><title type="html">AAAI-2020</title><link href="http://localhost:4000/paperlists/2020-02-21-AAAI/" rel="alternate" type="text/html" title="AAAI-2020" /><published>2020-02-21T00:00:00+00:00</published><updated>2020-02-21T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/AAAI</id><content type="html" xml:base="http://localhost:4000/paperlists/2020-02-21-AAAI/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;learning-to-compare-and-retrieve&quot;&gt;Learning to Compare and Retrieve&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1910.04115.pdf&quot;&gt;Active Ordinal Querying for Tuplewise Similarity Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao (Amos) Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">Code Releasing of My Recent Work-Derivative Manipulation</title><link href="http://localhost:4000/blogs/2020-02-18-code-releasing/" rel="alternate" type="text/html" title="Code Releasing of My Recent Work-Derivative Manipulation" /><published>2020-02-18T00:00:00+00:00</published><updated>2020-02-18T00:00:00+00:00</updated><id>http://localhost:4000/blogs/code-releasing</id><content type="html" xml:base="http://localhost:4000/blogs/2020-02-18-code-releasing/">&lt;ol class=&quot;message&quot;&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;../../my_docs/IMAE_Code_Illustration&quot;&gt;IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude’s Variance Matters&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;../../my_docs/DM_Code_Illustration&quot;&gt;Derivative Manipulation for General Example Weighting&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/XinshaoAmosWang/DerivativeManipulation&quot;&gt;Github Page&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Xinshao (Amos) Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude’s Variance Matters</summary></entry><entry><title type="html">Usefull Common Resources/Tricks</title><link href="http://localhost:4000/blogs/2020-02-17-useful-common-resources/" rel="alternate" type="text/html" title="Usefull Common Resources/Tricks" /><published>2020-02-17T00:00:00+00:00</published><updated>2020-02-17T00:00:00+00:00</updated><id>http://localhost:4000/blogs/useful-common-resources</id><content type="html" xml:base="http://localhost:4000/blogs/2020-02-17-useful-common-resources/">&lt;ol class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#useful-links-on-general-study&quot;&gt;Useful Links on General Study&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#github-configuration-of-local-machine-to-github-remote&quot;&gt;Github: Configuration of local machine to github remote&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gdrive&quot;&gt;GDrive&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#useful-links-on-jekyll-google-adsense-markdown&quot;&gt;Useful links on Jekyll, Google AdSense, Markdown. &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#useful-links-on-travel--insurance&quot;&gt;Useful links on Travel &amp;amp; Insurance.  &lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;replace-string-in-files-recursively&quot;&gt;Replace string in files recursively&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;Simplest way to replace (all files, directory, recursive):
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  find . -type f -not -path '*/\.*' -exec sed -i 's/Previous string/New string/g' {} + 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Note: Sometimes you might need to ignore some hidden files i.e. .git, you can use above command.
If you want to include hidden files use,
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;find . -type f  -exec sed -i 's/Previous string/New string/g' {} +
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;useful-links-on-general-study&quot;&gt;Useful Links on General Study&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Information about probabilistic models of cognition
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://cocosci.princeton.edu/tom/bayes.html&quot;&gt;Tom’s Bayesian reading list&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;message&quot;&gt;…&lt;a href=&quot;http://cocosci.princeton.edu/resources.php&quot;&gt;http://cocosci.princeton.edu/resources.php&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;useful-links-on-jekyll-google-adsense-markdown&quot;&gt;Useful links on Jekyll, Google AdSense, Markdown.&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://mycyberuniverse.com/en-gb/add-google-adsense-to-a-jekyll-website.html&quot;&gt;Add Google AdSense to a Jekyll website&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.lewisgavin.co.uk/Google-Analytics-Adsense/&quot;&gt;AdSense Jekyll + Github&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://jekyllrb.com/docs/variables/&quot;&gt;Jekyll Variables&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://michaelsoolee.com/google-analytics-jekyll/&quot;&gt;Add Google Analytics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;useful-links-on-travel--insurance&quot;&gt;Useful links on Travel &amp;amp; Insurance.&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.britishairways.com/en-gb/executive-club/collecting-avios&quot;&gt;Collecting Avios&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://uk.virginmoney.com/virgin/travel-insurance/whats-covered.jsp&quot;&gt;Virgin: What’s covered&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.aig.com.cn/individuals/travel-insurance?utm_source=baidu&amp;amp;utm_campaign=%E9%80%9A%E7%94%A8%E8%AF%8D%2D%E6%97%85%E8%A1%8C%E9%99%A9&amp;amp;utm_adgroup=%E9%80%9A%E7%94%A8%2D%E6%97%85%E6%B8%B8%E9%99%A9&amp;amp;utm_term=%E6%97%85%E8%A1%8C%E4%BF%9D%E9%99%A9&amp;amp;utm_medium=search%5Fcpc&amp;amp;utm_channel=baidu%5Fpc&amp;amp;utm_content=tyc&amp;amp;bd_vid=10708153713933454488&quot;&gt;AIG&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.allianz360.com/?media=d4cd86c5e9444316994e5a2c00fa9cd6&amp;amp;type=1&quot;&gt;京东安联&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.aerlingus.com/html/flightSearchResult.html#/fareType=ONEWAY&amp;amp;fareCategory=ECONOMY&amp;amp;promoCode=&amp;amp;numAdults=1&amp;amp;numChildren=0&amp;amp;numInfants=0&amp;amp;groupBooking=false&amp;amp;sourceAirportCode_0=DUB&amp;amp;destinationAirportCode_0=LHR&amp;amp;departureDate_0=2019-10-19&amp;amp;flightCode_0=EI168&quot;&gt;Aer Lingus: Dublin T2 =&amp;gt; London Heathrow&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gdrive&quot;&gt;GDrive&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/gdrive-org/gdrive&quot;&gt;GDrive Github&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/gdrive-org/gdrive/issues/116&quot;&gt;List Folder, Root&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://askubuntu.com/questions/867284/using-gdrive-to-download-entire-folder&quot;&gt;Download entire folder&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.linuxandubuntu.com/home/google-drive-cli-client-for-linux&quot;&gt;How to use GDrive in linux?&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;github-configuration-of-local-machine-to-github-remote&quot;&gt;Github: Configuration of local machine to github remote&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://help.github.com/en/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent#generating-a-new-ssh-key&quot;&gt;Local: Generating a new ssh key&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://help.github.com/en/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent#adding-your-ssh-key-to-the-ssh-agent&quot;&gt;Local: Adding your new SSH key to the ssh-agent&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://help.github.com/en/github/authenticating-to-github/adding-a-new-ssh-key-to-your-github-account&quot;&gt;Remote: Adding your new SSH key to your GitHub account&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/XinshaoAmosWang/Deep-Metric-Embedding/blob/master/common_git.md&quot;&gt;Common commands&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao (Amos) Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">Useful Links on General Study Github: Configuration of local machine to github remote GDrive Useful links on Jekyll, Google AdSense, Markdown. Useful links on Travel &amp;amp; Insurance.</summary></entry><entry><title type="html">arXiv-2020</title><link href="http://localhost:4000/paperlists/2020-02-16-arXiv/" rel="alternate" type="text/html" title="arXiv-2020" /><published>2020-02-16T00:00:00+00:00</published><updated>2020-02-16T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/arXiv</id><content type="html" xml:base="http://localhost:4000/paperlists/2020-02-16-arXiv/">&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#noisy-labels&quot;&gt;Noisy labels &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#active-learning&quot;&gt;Active Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#foundation-of-deep-learning&quot;&gt;Foundation of Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;foundation-of-deep-learning&quot;&gt;Foundation of Deep Learning&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.09046.pdf&quot;&gt;Neural Bayes: A Generic Parameterization Method for Unsupervised Representation Learning&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.05709.pdf&quot;&gt;A Simple Framework for Contrastive Learning of Visual Representations&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Data augmentation: composition of data augmentations plays a critical role in defining effective predictive tasks;&lt;/li&gt;
      &lt;li&gt;Auxiliary transformation:  introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations;&lt;/li&gt;
      &lt;li&gt;Larger batch size and more training steps: contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning.&lt;/li&gt;
      &lt;li&gt;Results: By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1911.09976.pdf&quot;&gt;Instance Cross Entropy for Deep Metric Learning&lt;/a&gt; and its application in SimCLR-A Simple Framework for Contrastive Learning of Visual Representations&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;I am very glad to highlight that:  our proposed ICE is simple and effective, which has also been demonstrated in recent work SimCLR, in the context of self-supervised learning: A Simple Framework for Contrastive Learning of Visual Representations&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Its loss expression NT-Xent (the normalized temperature-scaled cross entropy loss) is a fantastic application of our recently proposed Instance Cross Entropy for Deep Metric Learning,  in the context of self-supervised learnining. I am very excited about this.
        &lt;ul&gt;
          &lt;li&gt;#InstanceCrossEntropy #TemperatureScaling #RepresentationLearning&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/publication/337485049_Instance_Cross_Entropy_for_Deep_Metric_Learning/comments&quot;&gt;Research Gate&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=BJeguTEKDB&amp;amp;noteId=txrrkCL-sXhttps://openreview.net/forum?id=BJeguTEKDB&amp;amp;noteId=txrrkCL-sX&quot;&gt;Open Review&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/f4x1sh/r_instance_cross_entropy_for_deep_metric_learning/&quot;&gt;Reddit&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.03436.pdf&quot;&gt;Unsupervised Embedding Learning via Invariant and Spreading Instance Feature&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Unsupervised Embedding Learning via Invariant and SpreadingInstance Feature is even closer, also in the context of self-supervised learning, maximising the agreement over augmentations of one instance.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1911.09976.pdf&quot;&gt; Instance Cross Entropy for Deep Metric Learning&lt;/a&gt; is in the context of supervised discriminative representation learning, maximising the agreement over augmentaions of multiple images i.e. a class.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://xinshaoamoswang.github.io/Papers/AnomalyAndRegularisation/#iclr-2019-learning-deep-representations-by-mutual-information-estimation-and-maximization&quot;&gt;ICLR 2019: Learning deep representations by mutual information estimation and maximization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;noisy-labels&quot;&gt;Noisy labels&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.06482.pdf&quot;&gt;Learning Adaptive Loss for Robust Learning with Noisy Labels&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=HJgExaVtwr&quot;&gt;DivideMix: Learning with Noisy Labels as Semi-supervised Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9001093&quot;&gt;Group-Teaching: Learning Robust CNNs From Extremely Noisy Labels&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;active-learning&quot;&gt;Active Learning&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/7010-learning-active-learning-from-data.pdf&quot;&gt;Learning Active Learning from Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao (Amos) Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">Noisy labels Active Learning Foundation of Deep Learning</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">Paper notes on Anomalies&amp;amp;Regularisation</title><link href="http://localhost:4000/blogs/2020-02-15-anomalies-regularisation/" rel="alternate" type="text/html" title="Paper notes on Anomalies&amp;Regularisation" /><published>2020-02-15T00:00:00+00:00</published><updated>2020-02-15T00:00:00+00:00</updated><id>http://localhost:4000/blogs/anomalies-regularisation</id><content type="html" xml:base="http://localhost:4000/blogs/2020-02-15-anomalies-regularisation/">&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;/Papers/AnomalyAndRegularisation/&quot;&gt;Paper notes on Anomalies&amp;amp;Regularisation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao (Amos) Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">Paper notes on Anomalies&amp;amp;Regularisation</summary></entry></feed>