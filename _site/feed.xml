<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2020-02-12T20:25:02+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Xinshao Wang</title><subtitle>&quot;Welcome to my personal website&quot;. **3rd Year PhD Student**, will graduate in Sep 2020.
</subtitle><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><entry><title type="html">ICLR-2020</title><link href="http://localhost:4000/paperlists/2020-01-02-ICLR/" rel="alternate" type="text/html" title="ICLR-2020" /><published>2020-01-02T00:00:00+00:00</published><updated>2020-01-02T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/ICLR</id><content type="html" xml:base="http://localhost:4000/paperlists/2020-01-02-ICLR/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;foundation-of-deep-learning&quot;&gt;Foundation of Deep Learning&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=B1g8VkHFPH&quot;&gt;Rethinking the Hyperparameters for Fine-tuning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">ICLR-2019</title><link href="http://localhost:4000/paperlists/2019-12-30-ICLR/" rel="alternate" type="text/html" title="ICLR-2019" /><published>2019-12-30T00:00:00+00:00</published><updated>2019-12-30T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/ICLR</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-30-ICLR/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;corefundamental-deep-learning-informativeforgettable-example-uncertain-examples&quot;&gt;Core/Fundamental Deep Learning (Informative/Forgettable Example, Uncertain Examples)&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=BJlxm30cKm&quot;&gt;Informative/Forgettable Examples: An Empirical Study of Example Forgetting during Deep Neural Network Learning&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;We define a ``forgetting event’’ to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning.
  We show that catastrophic forgetting occurs within what is considered to be a single task and find that examples that are not prone to forgetting can be removed from the training set without loss of generalization.&lt;/li&gt;
      &lt;li&gt;Across several benchmark data sets, we find that:
        &lt;ul&gt;
          &lt;li&gt;(i) certain examples are forgotten with high frequency, and some not at all;&lt;/li&gt;
          &lt;li&gt;(ii) a data set’s (un)forgettable examples generalize across neural architectures;&lt;/li&gt;
          &lt;li&gt;(iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Their finding: Harder/Support/informative samples are learned later and may include noisy examples. &lt;br /&gt;
  Then a question is arouse: How to differentiate informative and noisy examples? 
  Our work provides a solution for this question. &lt;a href=&quot;https://arxiv.org/pdf/1905.11233.pdf&quot;&gt;Derivative Manipulation for General Example Weighting&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Detailed findings of theirs:
        &lt;ul&gt;
          &lt;li&gt;Support examples = forgettable examples = Informative examples that cannot be removed. Removing unforgottable examples do not hurt the generalisation performance when training a model from scratch on the remained subset.&lt;/li&gt;
          &lt;li&gt;The properties of support/informative examples: a) learnt later; b) larget misclassification margin when forgotten; c) perceptually ambiguous; d) tend to be noisy.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/6701-active-bias-training-more-accurate-neural-networks-by-emphasizing-high-variance-samples.pdf&quot;&gt;Uncertain Examples-NeurIPS2017: Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">CVPR-2019</title><link href="http://localhost:4000/paperlists/2019-12-29-CVPR/" rel="alternate" type="text/html" title="CVPR-2019" /><published>2019-12-29T00:00:00+00:00</published><updated>2019-12-29T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/CVPR</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-29-CVPR/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;deep-metric-learning&quot;&gt;Deep Metric Learning&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Sanakoyeu_Divide_and_Conquer_the_Embedding_Space_for_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Divide and Conquer the Embedding Space for Metric Learning&lt;/a&gt; 
:+1:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Each learner will learn a separate distance metric using only a subspace of the original embedding space and &lt;strong&gt;a part of the data&lt;/strong&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Natural hard negatives mining: Finally, &lt;strong&gt;the splitting and sampling connect to hard negative mining&lt;/strong&gt;, which is verified by them. (I appreciate this ablation study in Table 6 )&lt;/li&gt;
      &lt;li&gt;Divide means: 1) Splitting the training data into K Clusters; 
  2) Splitting the embedding into K Slices.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Cakir_Deep_Metric_Learning_to_Rank_CVPR_2019_paper.pdf&quot;&gt;Deep Metric Learning to Rank&lt;/a&gt; :+1:
    &lt;ul&gt;
      &lt;li&gt;Our main contribution is a novel solution to optimizing Average Precision under the Euclidean metric, based on the probabilistic interpretation of AP as the area under precision-recall curve, as well as distance quantization.&lt;/li&gt;
      &lt;li&gt;We also propose a category-based minibatch sampling strategy and a large-batch training heuristic.&lt;/li&gt;
      &lt;li&gt;On three &lt;strong&gt;few-shot image retrieval datasets&lt;/strong&gt;, FastAP consistently outperforms competing methods, which often involve complex optimization heuristics or costly model ensembles.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Multi-Similarity Loss With General Pair Weighting for Deep Metric Learning&lt;/a&gt; :+1:
    &lt;ul&gt;
      &lt;li&gt;Objective of the proposed multi-similarity loss, which aims to collect informative pairs, and weight these pairs through their own and relative similarities.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1903.03238.pdf&quot;&gt;Ranked List Loss for Deep Metric Learning&lt;/a&gt; :+1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Suh_Stochastic_Class-Based_Hard_Example_Mining_for_Deep_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Stochastic Class-Based Hard Example Mining for Deep Metric Learning&lt;/a&gt; :+1:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Scale linearly to the number of classes.&lt;/li&gt;
      &lt;li&gt;The methods proposed by Movshovitz-Attias et al. [14] and Wen et al. [34] are related to ours in a sense that class representatives are jointly trained with the feature extractor. 
However, their goal is to formulate new losses using the class representatives whereas we use them for hard negative mining.&lt;/li&gt;
      &lt;li&gt;Given an anchor instance, our algorithm first selects a few hard negative classes based on the class-to-sample distances and then performs a refined search in an instance-level only from the selected classes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Theoretically Sound Upper Bound on the Triplet Loss for Improving the Efficiency of Deep Distance Metric Learning :+1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unsupervised&lt;/strong&gt; Embedding Learning via Invariant and Spreading Instance Feature :+1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Yuan_Signal-To-Noise_Ratio_A_Robust_Distance_Metric_for_Deep_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Signal-To-Noise Ratio: A Robust Distance Metric for Deep Metric Learning&lt;/a&gt; :+1:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;We propose a robust SNR distance metric based on Signal-to-Noise Ratio (SNR) for measuring the similarity of image pairs for deep metric learning. Compared with Euclidean distance metric, our SNR distance metric can further jointly reduce the intra-class distances and enlarge the inter-class distances for learned features.&lt;/li&gt;
      &lt;li&gt;SNR in signal processing is used to measure the level of a desired signal to the level of noise, and a larger SNR value means a higher signal quality.
  For similarity measurement in deep metric learning, a pair of learned features x and y can be given as y = x + n, where n can be treated as a noise. Then, the SNR is the ratio of the feature variance and the noise variance.&lt;/li&gt;
      &lt;li&gt;To show the generality of our SNR-based metric, we also extend our approach to hashing retrieval learning.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Branchaud-Charron_Spectral_Metric_for_Dataset_Complexity_Assessment_CVPR_2019_paper.pdf&quot;&gt;Spectral Metric for Dataset Complexity Assessment&lt;/a&gt; :+1:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Related work: &lt;a href=&quot;https://openreview.net/forum?id=ryup8-WCW&quot;&gt;Measuring the Intrinsic Dimension of Objective Landscapes ICLR 2018&lt;/a&gt;, 
  &lt;a href=&quot;https://arxiv.org/abs/1808.03591&quot;&gt;How Complex is your classification problem? A survey on measuring classification complexity Survey on complexity measures&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Deep Asymmetric Metric Learning via Rich Relationship Mining :+1:
    &lt;ul&gt;
      &lt;li&gt;DAMLRRM relaxes the constraint on positive pairs to extend the generalization capability. We build positive pairs training pool by constructing a minimum connected tree for each category instead of considering all positive pairs within a mini-batch. As a result, there will exist a direct or indirect path between any positive pair, which ensures the relevance being bridged to each other. The inspiration comes from ranking on manifold [58] that spreads the relevance to their nearby neighbors one by one.&lt;/li&gt;
      &lt;li&gt;Idea is novel. The results on SOP are not good, only 69.7 with GoogLeNet&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Hybrid-Attention_Based_Decoupled_Metric_Learning_for_Zero-Shot_Image_Retrieval_CVPR_2019_paper.pdf&quot;&gt;Hybrid-Attention Based Decoupled Metric Learning for Zero-Shot Image Retrieval&lt;/a&gt; :-1:
    &lt;ul&gt;
      &lt;li&gt;Very complex: object attention, spatial attention, random walk graph, etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.09626.pdf&quot;&gt;Deep Metric Learning Beyond Binary Supervision&lt;/a&gt; :-1:
    &lt;ul&gt;
      &lt;li&gt;Binary supervision indicating whether a pair of images are of the same class or not.&lt;/li&gt;
      &lt;li&gt;Using continuous labels&lt;/li&gt;
      &lt;li&gt;Learn the degree of similarity rather than just the order.&lt;/li&gt;
      &lt;li&gt;A triplet mining strategy adapted to metric learning with continuous labels.&lt;/li&gt;
      &lt;li&gt;Image retrieval tasks with continuous labels in terms of human poses, room layouts and image captions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hardness-aware deep metric learning 
:-1: : data augmentation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensemble Deep Manifold Similarity Learning using Hard Proxies :-1: random walk algorithm, ensemble models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Re-Ranking via Metric Fusion for Object Retrieval and Person Re-Identification :-1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Deep Embedding Learning With Discriminative Sampling Policy :-1:&lt;/li&gt;
  &lt;li&gt;Point Cloud Oversegmentation With Graph-Structured Deep Metric Learning :-1:&lt;/li&gt;
  &lt;li&gt;Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval :-1:&lt;/li&gt;
  &lt;li&gt;A Compact Embedding for Facial Expression Similarity :-1:&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Karlinsky_RepMet_Representative-Based_Metric_Learning_for_Classification_and_Few-Shot_Object_Detection_CVPR_2019_paper.pdf&quot;&gt;RepMet: Representative-Based Metric Learning for Classification and Few-Shot Object Detection&lt;/a&gt; :-1:&lt;/li&gt;
  &lt;li&gt;Eliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking :-1:&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;robustness&quot;&gt;Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Barron_A_General_and_Adaptive_Robust_Loss_Function_CVPR_2019_paper.pdf&quot;&gt;A General and Adaptive Robust Loss Function&lt;/a&gt; :+1:&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1812.05214.pdf&quot;&gt;Learning to Learn from Noisy Labeled Data&lt;/a&gt; :+1:
This work achieves promising results with meta-learning. Our result on Clothing 1M is comparable with theirs. However, their modelling via meta-learning seems extremely complex in practice.
&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/bws5iv/r_cvpr_2019_noisetolerant_training_work_learning/&quot;&gt;https://www.reddit.com/r/MachineLearning/comments/bws5iv/r_cvpr_2019_noisetolerant_training_work_learning/&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Too many hyper-parameters shown in their Algorithm 1 and implementation section 4.2.&lt;/li&gt;
      &lt;li&gt;The strategies of iterative training together with iterative data filtering/cleaning, reusing last-round best model as mentor, etc., make it difficult to handle in practice.&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">ICCV-2019</title><link href="http://localhost:4000/paperlists/2019-12-29-ICCV/" rel="alternate" type="text/html" title="ICCV-2019" /><published>2019-12-29T00:00:00+00:00</published><updated>2019-12-29T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/ICCV</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-29-ICCV/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;noisy-labels-&quot;&gt;Noisy Labels, …&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Deep_Self-Learning_From_Noisy_Labels_ICCV_2019_paper.pdf&quot;&gt;Deep Self-Learning From Noisy Labels&lt;/a&gt;:  Self means `without extra supervision’
    &lt;ul&gt;
      &lt;li&gt;The proposed SMP trains in an iterative manner which
contains two phases: the first phase is to train a network
with &lt;strong&gt;the original noisy label and corrected label&lt;/strong&gt; generated
in the second phase.&lt;/li&gt;
      &lt;li&gt;By extracting multiple prototypes for a category, we demonstrate that more prototypes would get a better representation of a class and obtain better label-correction results.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Co-Mining_Deep_Face_Recognition_With_Noisy_Labels_ICCV_2019_paper.pdf&quot;&gt;Co-Mining: Deep Face Recognition With Noisy Labels&lt;/a&gt;: We propose a novel &lt;strong&gt;co-mining&lt;/strong&gt; framework, which employs two peer networks to &lt;strong&gt;detect the noisy faces,
exchanges the high-confidence clean faces and reweights the clean faces&lt;/strong&gt; in a mini-batch fashion.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_NLNL_Negative_Learning_for_Noisy_Labels_ICCV_2019_paper.pdf&quot;&gt;NLNL: Negative Learning for Noisy Labels&lt;/a&gt;: Input image belongs to this label–Positive Learning; Negative Learning (NL)–CNNs are trained using a complementary label as in “input image does not belong to this complementary label.”&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Symmetric_Cross_Entropy_for_Robust_Learning_With_Noisy_Labels_ICCV_2019_paper.pdf&quot;&gt;Symmetric Cross Entropy for Robust Learning With Noisy Labels&lt;/a&gt;: Already compared in our method.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_O2U-Net_A_Simple_Noisy_Label_Detection_Approach_for_Deep_Neural_ICCV_2019_paper.pdf&quot;&gt;O2U-Net: A Simple Noisy Label Detection Approach for Deep Neural Networks&lt;/a&gt;–&lt;strong&gt;Overall, this method is complex&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;We propose a novel noisy label detection approach, named O2U-net, without human annotation and verification.
  It only requires adjusting the hyper-parameters of the deep network to &lt;strong&gt;make its status transfer from overfitting to underfitting (O2U) cyclically&lt;/strong&gt;. 
  &lt;strong&gt;By calculating and ranking the normalized average loss of every sample, the mislabeled samples can be identified.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;The losses of each sample are recorded during iterations.&lt;/strong&gt; The higher the normalized average loss of a sample, the higher the probability of being noisy labels.
  =&amp;gt; &lt;strong&gt;Is it scalable to large datasets?&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;O2U-net is naturally compatible with active learning and other human annotation approaches. This introduces extra flexibility for learning with noisy labels.&lt;/li&gt;
      &lt;li&gt;The whole training process:
        &lt;ul&gt;
          &lt;li&gt;Pre-training:  Train the network directly on the original dataset including noisy labels. At this step, a common constant learning rate is applied. &lt;strong&gt;A large batch size&lt;/strong&gt; is applied to reduce the impact of label noise &lt;a href=&quot;https://arxiv.org/abs/1705.10694&quot;&gt;Deep Learning is Robust to Massive Label Noise&lt;/a&gt;.  We use &lt;strong&gt;a validation set to monitor the performance of training.&lt;/strong&gt; The network is trained until the accuracy in the validation set stays stable. (&lt;strong&gt;Validation Data is Needed!&lt;/strong&gt;)&lt;/li&gt;
          &lt;li&gt;Cyclical Training: &lt;strong&gt;A small batch size&lt;/strong&gt;–A smaller batch size is chosen to make the network more easily transfer from overfitting to underfitting.
  After the whole cyclical training, the average of the
  normalized losses of every sample is computed. All
  the average losses are then ranked in descending order.
  The top k% of samples are removed from the original
  dataset as noisy labels, where k depends on the prior
  knowledge on the dataset. Such prior knowledge can
  be obtained by manually verifying a small group of
  randomly selected samples.&lt;/li&gt;
          &lt;li&gt;Training on Clean Data: Lastly, we &lt;strong&gt;re-initialize&lt;/strong&gt; the
  parameters of the network, and &lt;strong&gt;re-train&lt;/strong&gt; it on the cleansing dataset ordinarily until achieving stable accuracy
  and loss in the validation set.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;robustness&quot;&gt;Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Peterson_Human_Uncertainty_Makes_Classification_More_Robust_ICCV_2019_paper.pdf&quot;&gt;Human uncertainty makes classification more robust–From Labels to Label Distributions&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;We suggest &lt;strong&gt;an alternative objective: not
just trying to capture the most likely label, but trying to capture the full distribution over labels.&lt;/strong&gt; Although there has been much work scaling the number of images in datasets [18], and investigating label noise
[40, 12, 48], little effort has been put into identifying the
benefits from increasing the richness of (informative) label
distributions for image classification tasks.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Soft Labels&lt;/strong&gt;: One of the core contributions of our work is around using the soft labels provided through human confusion as a replacement for one-hot label encodings.&lt;/li&gt;
      &lt;li&gt;Our approach proposes &lt;strong&gt;utilizing these human disagreements&lt;/strong&gt; to improve the accuracy and robustness of a model, complementing existing work aimed at leveraging “errors” in human labeling&lt;/li&gt;
      &lt;li&gt;As accuracy gains have begun to asymptote at
near-perfect levels [11], there has been &lt;strong&gt;increasing focus on
out-of-training-set performance—in particular, the ability
to generalize to related stimuli [39], and robustness to adversarial examples [29]&lt;/strong&gt;. On these tasks, by contrast, CNNs
tend to perform rather poorly, whereas humans continue to
perform well.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Yamaguchi_Subspace_Structure-Aware_Spectral_Clustering_for_Robust_Subspace_Clustering_ICCV_2019_paper.pdf&quot;&gt;Subspace Structure-aware Spectral Clustering for Robust Subspace Clustering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adversarial-robustness&quot;&gt;Adversarial Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Gowal_Scalable_Verified_Training_for_Provably_Robust_Image_Classification_ICCV_2019_paper.pdf&quot;&gt;Scalable Verified Training for Provably Robust Image Classification&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Train deep neural networks that are provably robust to norm-bounded adversarial perturbations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Improving_Adversarial_Robustness_via_Guided_Complement_Entropy_ICCV_2019_paper.pdf&quot;&gt;Improving Adversarial Robustness via Guided Complement Entropy&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Bilateral_Adversarial_Training_Towards_Fast_Training_of_More_Robust_Models_ICCV_2019_paper.pdf&quot;&gt;Bilateral Adversarial Training: Towards Fast Training of More Robust Models
Against Adversarial Attacks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-metric-learning-&quot;&gt;Deep Metric Learning, …&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">ICML-2019</title><link href="http://localhost:4000/paperlists/2019-12-29-ICML/" rel="alternate" type="text/html" title="ICML-2019" /><published>2019-12-29T00:00:00+00:00</published><updated>2019-12-29T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/ICML</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-29-ICML/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;label-noise&quot;&gt;Label Noise&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v97/arazo19a/arazo19a.pdf&quot;&gt;Unsupervised Label Noise Modeling and Loss Correction&lt;/a&gt; :+1:
    &lt;ul&gt;
      &lt;li&gt;A suitable two-component mixture model as an unsupervised generative model
of sample loss values during training to allow
online estimation of the probability that a sample is mislabelled.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.05040.pdf&quot;&gt;Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;We find that the test accuracy can be
quantitatively characterized in terms of the noise
ratio in datasets. The test accuracy
is a quadratic function of the noise ratio in the
case of symmetric noise, which explains the experimental findings previously published. (I am not convinced on this!)&lt;/li&gt;
      &lt;li&gt;DNNs tend to learn simple patterns
first, then gradually memorize all samples, which justifies
the widely used small-loss criteria: treating samples with
small training loss as clean ones (Han et al., 2018; Jiang
et al., 2018).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v97/song19b/song19b.pdf&quot;&gt;SELFIE: Refurbishing Unclean Samples for Robust Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;robustness&quot;&gt;Robustness&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf&quot;&gt;Certified Adversarial Robustness via Randomized Smoothing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">IJCAI-2019</title><link href="http://localhost:4000/paperlists/2019-12-29-IJCAI/" rel="alternate" type="text/html" title="IJCAI-2019" /><published>2019-12-29T00:00:00+00:00</published><updated>2019-12-29T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/IJCAI</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-29-IJCAI/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;robustness&quot;&gt;Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ijcai.org/Proceedings/2019/0654.pdf&quot;&gt;Robustra: Training Provable Robust Neural Networks
over Reference Adversarial Space&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">NeurIPS-2019</title><link href="http://localhost:4000/paperlists/2019-12-29-NeurIPS/" rel="alternate" type="text/html" title="NeurIPS-2019" /><published>2019-12-29T00:00:00+00:00</published><updated>2019-12-29T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/NeurIPS</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-29-NeurIPS/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;example-weighting-by-meta-learning-for-robustness&quot;&gt;Example Weighting by Meta Learning for Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/8467-meta-weight-net-learning-an-explicit-mapping-for-sample-weighting.pdf&quot;&gt;Meta-Weight-Net: Learning an Explicit Mapping
For Sample Weighting&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;A following work of &lt;a href=&quot;http://127.0.0.1:4000/paperlists/2018-12-30-ICML/#example-weighting-by-meta-learning-gradient-directions&quot;&gt;https://xinshaoamoswang.github.io/paperlists/2018-12-30-ICML/#example-weighting-by-meta-learning-gradient-directions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;We propose to automatically learn an explicit loss-weight function, parameterized by an MLP from
data in a meta-learning manner. Due to the universal approximation capability of this weight net, it can finely fit a wide range of weighting functions including those used in conventional research.&lt;/li&gt;
      &lt;li&gt;The sample weights of those
samples better complying with the meta-data knowledge will be improved, while those violating such meta-knowledge will be suppressed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">Foundations of Deep Learning, Machine Learning</title><link href="http://localhost:4000/blogs/2019-12-12-papers-summary-reweighting/" rel="alternate" type="text/html" title="Foundations of Deep Learning, Machine Learning" /><published>2019-12-12T00:00:00+00:00</published><updated>2019-12-12T00:00:00+00:00</updated><id>http://localhost:4000/blogs/papers-summary-reweighting</id><content type="html" xml:base="http://localhost:4000/blogs/2019-12-12-papers-summary-reweighting/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;iccv-2019-robustness&quot;&gt;ICCV 2019: Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Gowal_Scalable_Verified_Training_for_Provably_Robust_Image_Classification_ICCV_2019_paper.pdf&quot;&gt;Scalable Verified Training for Provably Robust Image Classification&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Improving_Adversarial_Robustness_via_Guided_Complement_Entropy_ICCV_2019_paper.pdf&quot;&gt;Improving Adversarial Robustness via Guided Complement Entropy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Bilateral_Adversarial_Training_Towards_Fast_Training_of_More_Robust_Models_ICCV_2019_paper.pdf&quot;&gt;Bilateral Adversarial Training: Towards Fast Training of More Robust Models
Against Adversarial Attacks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Peterson_Human_Uncertainty_Makes_Classification_More_Robust_ICCV_2019_paper.pdf&quot;&gt;Human uncertainty makes classification more robust&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Yamaguchi_Subspace_Structure-Aware_Spectral_Clustering_for_Robust_Subspace_Clustering_ICCV_2019_paper.pdf&quot;&gt;Subspace Structure-aware Spectral Clustering for Robust Subspace Clustering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-neurips-2019-learning-data-manipulation-for-augmentation-and-weighting&quot;&gt;:+1: &lt;a href=&quot;https://papers.nips.cc/paper/9706-learning-data-manipulation-for-augmentation-and-weighting.pdf&quot;&gt;NeurIPS 2019-Learning Data Manipulation for Augmentation and Weighting&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;Our
approach builds upon a recent connection of supervised learning and reinforcement
learning (RL), and adapts an off-the-shelf reward learning algorithm from RL for
joint data manipulation learning and model training. Different parameterization
of the “data reward” function instantiates different manipulation schemes.&lt;/li&gt;
  &lt;li&gt;We
showcase data augmentation that learns a text transformation network, and data
weighting that dynamically adapts the data sample importance. Experiments show
the resulting algorithms significantly improve the image and text classification
performance in low data regime and class-imbalance problems.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-iclr-2019-critical-learning-periods-in-deep-networks&quot;&gt;:+1: &lt;a href=&quot;https://openreview.net/forum?id=BkeStsCcKQ&quot;&gt;ICLR 2019-Critical Learning Periods in Deep Networks&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;Counterintuitively, information rises rapidly in the early phases of
training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of “Information Plasticity”&lt;/li&gt;
  &lt;li&gt;Our analysis suggests that the
first few epochs are critical for the creation of strong connections that are optimal relative
to the input data distribution. Once such strong connections are created, they do not appear
to change during additional training.&lt;/li&gt;
  &lt;li&gt;The initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining
the outcome of the training process.&lt;/li&gt;
  &lt;li&gt;The early transient is critical in determining the
final solution of the optimization associated with training an artificial neural network. In particular,
the effects of sensory deficits during a critical period cannot be overcome, no matter how much
additional training is performed.&lt;/li&gt;
  &lt;li&gt;Our experiments show that, rather than helpful, pre-training can be detrimental, even if the
tasks are similar (e.g., same labels, slightly blurred images).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-neurips-2019-time-matters-in-regularizing-deep-networks-weight-decay-and-data-augmentation-affect-early-learning-dynamics-matter-little-near-convergence&quot;&gt;:+1: &lt;a href=&quot;https://papers.nips.cc/paper/9252-time-matters-in-regularizing-deep-networks-weight-decay-and-data-augmentation-affect-early-learning-dynamics-matter-little-near-convergence.pdf&quot;&gt;NeurIPS 2019-Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;Regularization is typically understood as improving generalization by altering
the landscape of local extrema to which the model eventually converges. Deep
neural networks (DNNs), however, challenge this view: We show that removing
regularization after an initial transient period has little effect on generalization,
even if the final loss landscape is the same as if there had been no regularization.&lt;/li&gt;
  &lt;li&gt;In some cases, generalization even improves after interrupting regularization.&lt;/li&gt;
  &lt;li&gt;Conversely, if regularization is applied only after the initial transient, it has no effect
on the final solution, whose generalization gap is as bad as if regularization never
happened.&lt;/li&gt;
  &lt;li&gt;What matters for training deep networks is not just
whether or how, but when to regularize.&lt;/li&gt;
  &lt;li&gt;The phenomena we observe are manifest
in different datasets (CIFAR-10, CIFAR-100, SVHN, ImageNet), different architectures (ResNet-18, All-CNN), different regularization methods (weight decay, data
augmentation, mixup), different learning rate schedules (exponential, piece-wise
constant). They collectively suggest that there is a “critical period” for regularizing
deep networks that is decisive of the final performance. More analysis should,
therefore, focus on the transient rather than asymptotic behavior of learning.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Imposing regularization all along, however, causes over-smoothing&lt;/strong&gt;, whereas the ground-truth disparity field is typically discontinuous. So, &lt;strong&gt;regularization is introduced initially and then removed to capture fine details.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;neurips-2019-inherent-weight-normalization-in-stochastic-neural-networks&quot;&gt;&lt;a href=&quot;https://papers.nips.cc/paper/8591-inherent-weight-normalization-in-stochastic-neural-networks&quot;&gt;NeurIPS 2019-Inherent Weight Normalization in Stochastic Neural Networks&lt;/a&gt;&lt;/h2&gt;
&lt;p class=&quot;message&quot;&gt;&lt;img src=&quot;/imgs/Inherent_weight_normalisation.png&quot; alt=&quot;Full-width image&quot; class=&quot;lead&quot; data-width=&quot;200&quot; data-height=&quot;100&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;neurips-2019-weight-agnostic-neural-networks&quot;&gt;&lt;a href=&quot;https://papers.nips.cc/paper/8777-weight-agnostic-neural-networks.pdf&quot;&gt;NeurIPS 2019-Weight Agnostic Neural Networks&lt;/a&gt;&lt;/h2&gt;
&lt;p class=&quot;message&quot;&gt;Not all neural network architectures are created equal, some perform much better
than others for certain tasks. But how important are the weight parameters of a
neural network compared to its architecture? In this work, we question to what
extent neural network architectures alone, without learning any weight parameters,
can encode solutions for a given task. We propose a search method for neural
network architectures that can already perform a task without any explicit weight
training. To evaluate these networks, we populate the connections with a single
shared weight parameter sampled from a uniform random distribution, and measure
the expected performance. We demonstrate that our method can find minimal neural
network architectures that can perform several reinforcement learning tasks without
weight training. On a supervised learning domain, we find network architectures
that achieve much higher than chance accuracy on MNIST using random weights.
Interactive version of this paper at &lt;a href=&quot;https://weightagnostic.github.io/&quot;&gt;https://weightagnostic.github.io/&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary></entry><entry><title type="html">Robust Learning under Arbitrary Semantic Anomalies</title><link href="http://localhost:4000/projects/2019-10-10-arbitrary-semantic-anomalies/" rel="alternate" type="text/html" title="Robust Learning under Arbitrary Semantic Anomalies" /><published>2019-10-10T00:00:00+01:00</published><updated>2019-10-10T00:00:00+01:00</updated><id>http://localhost:4000/projects/arbitrary-semantic-anomalies</id><content type="html" xml:base="http://localhost:4000/projects/2019-10-10-arbitrary-semantic-anomalies/">&lt;p class=&quot;message&quot;&gt;Summary of research progress on Robust Learning under Arbitrary Semantic Anomalies. &lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/XinshaoAmosWang/Emphasis-Regularisation-by-Gradient-Rescaling&quot;&gt;An Extremely Simple and Principled Solution for Avoiding Overfitting and Achieving Better Generalisation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">Summary of research progress on Robust Learning under Arbitrary Semantic Anomalies.</summary></entry><entry><title type="html">Person Re-identification</title><link href="http://localhost:4000/projects/2019-10-10-person-reidentification/" rel="alternate" type="text/html" title="Person Re-identification" /><published>2019-10-10T00:00:00+01:00</published><updated>2019-10-10T00:00:00+01:00</updated><id>http://localhost:4000/projects/person-reidentification</id><content type="html" xml:base="http://localhost:4000/projects/2019-10-10-person-reidentification/">&lt;p class=&quot;message&quot;&gt;Summary of research progress on person re-identification.&lt;/p&gt;

&lt;h2 id=&quot;paper-notes-on-video-person-re-identification&quot;&gt;&lt;a href=&quot;/Papers/VideoReID/&quot;&gt;Paper notes on video person re-identification&lt;/a&gt;&lt;/h2&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">Summary of research progress on person re-identification.</summary></entry></feed>