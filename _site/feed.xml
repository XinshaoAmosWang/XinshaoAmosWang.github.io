<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2019-10-09T18:44:21+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Xinshao Wang</title><subtitle>&quot;Welcome to my personal website&quot;. **3rd Year PhD Student**, will graduate in Sep 2020.
</subtitle><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><entry><title type="html">An Extremely Simple and Principled Solution for Robust Learning under Arbitrary Anomalies</title><link href="http://localhost:4000/blogs/2019-10-01-Research-GR/" rel="alternate" type="text/html" title="An Extremely Simple and Principled Solution for Robust Learning under Arbitrary Anomalies" /><published>2019-10-01T00:00:00+01:00</published><updated>2019-10-01T00:00:00+01:00</updated><id>http://localhost:4000/blogs/Research-GR</id><content type="html" xml:base="http://localhost:4000/blogs/2019-10-01-Research-GR/">&lt;p&gt;General applicability: Label noise (semantic noise), outliers, heavy perceptual data noise, etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;It is reasonable to assume that there is semantic noise in large-scale training datasets&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Class labels may be missing.&lt;/li&gt;
  &lt;li&gt;The labelling process may be subjective.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Label noise is one of the most explicit cases where some observations and their labels are not matched in the training data. In this case, it is quite crucial to make your models learn meaningful patterns instead of errors.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Main contribution:&lt;/strong&gt; Intuitively and principally, we claim that two basic factors, what examples
get higher weights (emphasis focus) and how large variance over examples’ weights (emphasis
spread), should be babysit simultaneously when it comes to sample differentiation and reweighting.
Unfortunately, these two intuitive and indispensable factors are not studied together in the literature.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What training examples should be focused and how much more should they be emphasised when training DNNs under label noise?&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;When noise rate is higher, we can improve a model’s robustness by focusing on relatively less difficult examples.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/publication/333418661_Emphasis_Regularisation_by_Gradient_Rescaling_for_Training_Deep_Neural_Networks_with_Noisy_Labels/comments&quot;&gt;More comments and comparison with related work&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://drive.google.com/file/d/1fU3N_u-_puOwEbupK6aOENerP2S45tZX/view?usp=sharing&quot;&gt;Paper reading about outlier detection and robust inference&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;effective-qualitative-and-quantitative-results&quot;&gt;Effective (Qualitative and Quantitative Results)&lt;/h2&gt;

&lt;p&gt;Please see &lt;a href=&quot;https://arxiv.org/pdf/1905.11233.pdf&quot;&gt;our paper&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Outperform existing work on synthetic label noise;&lt;/li&gt;
  &lt;li&gt;Outperform existing work on unknown real-world noise.&lt;/li&gt;
&lt;/ul&gt;

&lt;p float=&quot;left&quot;&gt;
  &lt;img src=&quot;/assets/img/blog/figs/Figure1.png&quot; width=&quot;800&quot; /&gt;
  &lt;img src=&quot;/assets/img/blog/figs/Table1.png&quot; width=&quot;800&quot; /&gt;
  &lt;img src=&quot;/assets/img/blog/figs/Figure2.png&quot; width=&quot;800&quot; /&gt;
  &lt;img src=&quot;/assets/img/blog/figs/Table4.png&quot; width=&quot;800&quot; /&gt;
  &lt;img src=&quot;/assets/img/blog/figs/Table5.png&quot; width=&quot;800&quot; /&gt;
  &lt;img src=&quot;/assets/img/blog/figs/Table6.png&quot; width=&quot;800&quot; /&gt;
  &lt;img src=&quot;/assets/img/blog/figs/Table7.png&quot; width=&quot;800&quot; /&gt;
  &lt;img src=&quot;/assets/img/blog/figs/Table9.png&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;extremely-simple&quot;&gt;Extremely Simple&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Without advanced training strategies&lt;/strong&gt;: e.g.,&lt;/p&gt;

&lt;p&gt;a. Iterative retraining on gradual data correction&lt;/p&gt;

&lt;p&gt;b. Training based on carefully-designed curriculums&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Without using extra networks&lt;/strong&gt;: e.g.,&lt;/p&gt;

&lt;p&gt;a. Decoupling” when to update” from” how to update”&lt;/p&gt;

&lt;p&gt;b. Co-teaching: Robust training of deep neural networks with extremely noisy labels&lt;/p&gt;

&lt;p&gt;c. Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Without using extra validation sets for model optimisation&lt;/strong&gt;: e.g.,&lt;/p&gt;

&lt;p&gt;a.  Learning to reweight examples for
robust deep learning&lt;/p&gt;

&lt;p&gt;b. Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels&lt;/p&gt;

&lt;p&gt;c. Toward robustness against label noise in training deep discriminative neural networks&lt;/p&gt;

&lt;p&gt;d. Learning
from noisy large-scale datasets with minimal supervision.&lt;/p&gt;

&lt;p&gt;e. Learning from
noisy labels with distillation.&lt;/p&gt;

&lt;p&gt;f. Cleannet: Transfer learning for
scalable image classifier training with label noise&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Without data pruning&lt;/strong&gt;: e.g.,&lt;/p&gt;

&lt;p&gt;a. Generalized cross entropy loss for training deep neural networks
with noisy labels. &lt;br /&gt;
  …&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Without relabelling&lt;/strong&gt;: e.g.,&lt;/p&gt;

&lt;p&gt;a. A semi-supervised two-stage approach
to learning from noisy labels&lt;/p&gt;

&lt;p&gt;b. Joint optimization framework for learning with noisy labels&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;Please kindly cite us if you find our work useful and inspiring.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@article&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;wang2019emphasis,
  &lt;span class=&quot;nv&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;={&lt;/span&gt;Emphasis Regularisation by Gradient Rescaling &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;Training Deep Neural Networks Robustly&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
  &lt;span class=&quot;nv&quot;&gt;author&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;={&lt;/span&gt;Wang, Xinshao and Hua, Yang and Kodirov, Elyor and Robertson, Neil&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
  &lt;span class=&quot;nv&quot;&gt;journal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;={&lt;/span&gt;arXiv preprint arXiv:1905.11233&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
  &lt;span class=&quot;nv&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;={&lt;/span&gt;2019&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Eran Malach and Shai Shalev-Shwartz. Decoupling” when to update” from” how to update”. In
NIPS, 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
NIPS, 2018&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. In ICML, 2018.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Arash Vahdat. Toward robustness against label noise in training deep discriminative neural networks.
In NIPS, 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie. Learning
from noisy large-scale datasets with minimal supervision. In CVPR, 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from
noisy labels with distillation. In ICCV, 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for
scalable image classifier training with label noise. In CVPR, 2018.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Zhilu Zhang and Mert R Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In NIPS, 2018.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Yifan Ding, Liqiang Wang, Deliang Fan, and Boqing Gong. A semi-supervised two-stage approach
to learning from noisy labels. In WACV, 2018.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hwanjun Song, Minseok Kim, and Jae-Gil Lee. Selfie: Refurbishing unclean samples for robust
deep learning. In ICML, 2019.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework for learning with noisy labels. In CVPR, 2018.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">General applicability: Label noise (semantic noise), outliers, heavy perceptual data noise, etc.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/GR/Figure1.png" /></entry><entry><title type="html">Paper Summary on Distance Metric, Representation Learning</title><link href="http://localhost:4000/blogs/2019-10-01-papers-summary-metric/" rel="alternate" type="text/html" title="Paper Summary on Distance Metric, Representation Learning" /><published>2019-10-01T00:00:00+01:00</published><updated>2019-10-01T00:00:00+01:00</updated><id>http://localhost:4000/blogs/papers-summary-metric</id><content type="html" xml:base="http://localhost:4000/blogs/2019-10-01-papers-summary-metric/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;few-shot-learning&quot;&gt;&lt;a href=&quot;/my_docs/few-shot/&quot;&gt;Few-shot Learning&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;ICLR 2018-Meta-Learning for Semi-Supervised Few-Shot Classification&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Unsupervised Meta Learning for Few-Show Image Classification&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Learning to Self-Train for Semi-Supervised Few-Shot Classification&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Adaptive Cross-Modal Few-shot Learning&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Cross Attention Network for Few-shot Classification&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Incremental Few-Shot Learning with Attention Attractor Networks&lt;/li&gt;
  &lt;li&gt;ICML 2019-LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;large-output-spaces&quot;&gt;&lt;a href=&quot;/my_docs/large-output-spaces/&quot;&gt;Large Output Spaces&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;NeurIPS 2019-Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces&lt;/li&gt;
  &lt;li&gt;AISTATS 2019-Stochastic Negative Mining for Learning with Large Output Spaces&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;poincaré-hyperbolic-curvilinear&quot;&gt;&lt;a href=&quot;/my_docs/Poincare-Hyperbolic-Curvilinear/&quot;&gt;Poincaré, Hyperbolic, Curvilinear&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;NeurIPS 2019-Multi-relational Poincaré Graph Embeddings&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Numerically Accurate Hyperbolic Embeddings Using Tiling-Based Models&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Curvilinear Distance Metric Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;wasserstein&quot;&gt;&lt;a href=&quot;/my_docs/wasserstein/&quot;&gt;Wasserstein&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;NeurIPS 2019-Generalized Sliced Wasserstein Distances&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Tree-Sliced Variants of Wasserstein Distances&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Sliced Gromov-Wasserstein&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Wasserstein Dependency Measure for Representation Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semi-supervised-or-unsupervised-learning&quot;&gt;&lt;a href=&quot;/my_docs/Semi-Un-Supervised-Learning/&quot;&gt;Semi-supervised or Unsupervised Learning&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;CVPR 2019-Label Propagation for Deep Semi-supervised Learning&lt;/li&gt;
  &lt;li&gt;NeurIPS 2017-Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results&lt;/li&gt;
  &lt;li&gt;ICLR 2019-Unsupervised Learning via Meta-Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;neurips-2019-stochastic-shared-embeddings-data-driven-regularization-of-embedding-layers&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.10630.pdf&quot;&gt;NeurIPS 2019-Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers&lt;/a&gt;&lt;/h2&gt;
&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: 
In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters.&lt;strong&gt;Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope to reduce statistical complexity.&lt;/strong&gt; Alternatively, we propose stochastically shared embeddings (SSE), a data-driven approach to regularizing embedding layers, which stochastically transitions between embeddings during stochastic gradient descent (SGD). Because SSE integrates seamlessly with existing SGD algorithms, it can be used with only minor modifications when training large scale neural networks. We develop two versions of SSE: SSE-Graph using knowledge graphs of embeddings; SSE-SE using no prior information. We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages. &lt;strong&gt;We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed SSE can further reduce overfitting, which often leads to more favorable generalization results.&lt;/strong&gt; &lt;br /&gt;
We conducted &lt;strong&gt;experiments for a total of 6 tasks from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages.&lt;/strong&gt;&lt;/p&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">Paper Summary on Noise, Anomalies, Adversaries, Robust Learning, Generalization</title><link href="http://localhost:4000/blogs/2019-10-01-papers-summary-noise/" rel="alternate" type="text/html" title="Paper Summary on Noise, Anomalies, Adversaries, Robust Learning, Generalization" /><published>2019-10-01T00:00:00+01:00</published><updated>2019-10-01T00:00:00+01:00</updated><id>http://localhost:4000/blogs/papers-summary-noise</id><content type="html" xml:base="http://localhost:4000/blogs/2019-10-01-papers-summary-noise/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;1-icml-2019-better-generalization-with-less-data-using-robust-gradient-descent&quot;&gt;:+1: &lt;a href=&quot;http://proceedings.mlr.press/v97/holland19a/holland19a.pdf&quot;&gt;ICML 2019-Better generalization with less data using robust gradient descent&lt;/a&gt;&lt;/h2&gt;

&lt;h2 id=&quot;gan-adversary-examples-adversary-machine-learning&quot;&gt;&lt;a href=&quot;/my_docs/adversary/&quot;&gt;GAN, Adversary Examples, Adversary Machine Learning&lt;/a&gt;&lt;/h2&gt;

&lt;h2 id=&quot;label-noise&quot;&gt;&lt;a href=&quot;/my_docs/Label-Noise/&quot;&gt;Label Noise&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;NeurIPS 2019-L_DMI: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Are Anchor Points Really Indispensable in Label-Noise Learning?&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Combinatorial Inference against Label Noise&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;stochastic-gradient-noise&quot;&gt;&lt;a href=&quot;/my_docs/Stochastic-Gradient-Noise/&quot;&gt;Stochastic-Gradient-Noise&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;ICML 2019-A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;denoiser-noise-removal&quot;&gt;&lt;a href=&quot;/my_docs/Denoiser/&quot;&gt;Denoiser, Noise Removal&lt;/a&gt;&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;NeurIPS 2019-Extending Stein’s unbiased risk estimator to train deep denoisers with correlated pairs of noisy images&lt;/li&gt;
  &lt;li&gt;NeurIPS 2019-Variational Denoising Network: Toward Blind Noise Modeling and Removal&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1--neurips-2019-noise-tolerant-fair-classification&quot;&gt;:+1:  &lt;a href=&quot;https://arxiv.org/abs/1901.10837&quot;&gt;NeurIPS 2019-Noise-tolerant fair classification&lt;/a&gt;&lt;/h2&gt;
&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Existing work on the problem operates &lt;strong&gt;under the assumption that the sensitive feature available in one’s training sample is perfectly reliable.&lt;/strong&gt; This assumption may be violated in many real-world cases: for example, respondents to a survey may choose to conceal or obfuscate their group identity out of fear of potential discrimination. This poses the question of whether one can still learn fair classifiers given noisy sensitive features.&lt;/p&gt;

&lt;h2 id=&quot;neurips-2019-neural-networks-grown-and-self-organized-by-noise&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.01039&quot;&gt;NeurIPS 2019-Neural networks grown and self-organized by noise&lt;/a&gt;&lt;/h2&gt;
&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: &lt;strong&gt;Living neural networks&lt;/strong&gt; emerge through a process of growth and self-organization that begins with a single cell and results in a brain, an organized and functional computational device. Artificial neural networks, however, rely on human-designed, hand-programmed architectures for their remarkable performance. &lt;strong&gt;Can we develop artificial computational devices that can grow and self-organize without human intervention?&lt;/strong&gt; In this paper, we propose a biologically inspired developmental algorithm that can &lt;strong&gt;‘grow’ a functional, layered neural network from a single initial cell.&lt;/strong&gt; The algorithm organizes inter-layer connections to construct a convolutional pooling layer, a key constituent of convolutional neural networks (CNN’s). Our approach is inspired by the mechanisms employed by the early visual system to wire the retina to the lateral geniculate nucleus (LGN), days before animals open their eyes. The key ingredients for robust self-organization are an emergent spontaneous spatiotemporal activity wave in the first layer and a local learning rule in the second layer that ‘learns’ the underlying activity pattern in the first layer. The algorithm is adaptable to a wide-range of input-layer geometries, robust to malfunctioning units in the first layer, and so can be used to &lt;strong&gt;successfully grow and self-organize pooling architectures of different pool-sizes and shapes.&lt;/strong&gt; The algorithm provides a primitive procedure for constructing layered neural networks through growth and self-organization. Broadly, our work shows that biologically inspired developmental algorithms can be applied to autonomously grow functional ‘brains’ in-silico.&lt;/p&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">Start 3rd year (the final year of my PhD)</title><link href="http://localhost:4000/projects/2019-10-01-PhD-3rd-start/" rel="alternate" type="text/html" title="Start 3rd year (the final year of my PhD)" /><published>2019-10-01T00:00:00+01:00</published><updated>2019-10-01T00:00:00+01:00</updated><id>http://localhost:4000/projects/PhD-3rd-start</id><content type="html" xml:base="http://localhost:4000/projects/2019-10-01-PhD-3rd-start/">&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This post is outdated and only included for legacy reasons.
See the &lt;a href=&quot;/docs/&quot;&gt;Documentation&lt;/a&gt; for up-to-date instructions.&lt;/p&gt;</content><author><name>XinshaoAmosWang</name></author><summary type="html">NOTE: This post is outdated and only included for legacy reasons. See the Documentation for up-to-date instructions.</summary></entry><entry><title type="html">Example Content III</title><link href="http://localhost:4000/blogs/2019-06-01-example-content-iii/" rel="alternate" type="text/html" title="Example Content III" /><published>2019-06-01T00:00:00+01:00</published><updated>2019-06-01T00:00:00+01:00</updated><id>http://localhost:4000/blogs/example-content-iii</id><content type="html" xml:base="http://localhost:4000/blogs/2019-06-01-example-content-iii/">&lt;p&gt;Hydejack offers a few additional features to markup your markdown.
Don’t worry, these are merely CSS classes added with kramdown’s &lt;code class=&quot;highlighter-rouge&quot;&gt;{:...}&lt;/code&gt; syntax,
so that your content remains compatible with other Jekyll themes.&lt;/p&gt;

&lt;h2 id=&quot;large-tables&quot;&gt;Large Tables&lt;/h2&gt;

&lt;table class=&quot;scroll-table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Default aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Left aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Center aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Right aligned&lt;/th&gt;
      &lt;th&gt;Default aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Left aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Center aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Right aligned&lt;/th&gt;
      &lt;th&gt;Default aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Left aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Center aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Right aligned&lt;/th&gt;
      &lt;th&gt;Default aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Left aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Center aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Right aligned&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;First body part&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Second cell&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Third cell&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fourth cell&lt;/td&gt;
      &lt;td&gt;First body part&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Second cell&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Third cell&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fourth cell&lt;/td&gt;
      &lt;td&gt;First body part&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Second cell&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Third cell&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fourth cell&lt;/td&gt;
      &lt;td&gt;First body part&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Second cell&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Third cell&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fourth cell&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Second line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;foo&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;strong&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;baz&lt;/td&gt;
      &lt;td&gt;Second line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;foo&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;strong&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;baz&lt;/td&gt;
      &lt;td&gt;Second line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;foo&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;strong&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;baz&lt;/td&gt;
      &lt;td&gt;Second line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;foo&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;strong&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;baz&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Third line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;quux&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;baz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;bar&lt;/td&gt;
      &lt;td&gt;Third line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;quux&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;baz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;bar&lt;/td&gt;
      &lt;td&gt;Third line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;quux&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;baz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;bar&lt;/td&gt;
      &lt;td&gt;Third line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;quux&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;baz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;bar&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Second body&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
      &lt;td&gt;Second body&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
      &lt;td&gt;Second body&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
      &lt;td&gt;Second body&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2 line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
      &lt;td&gt;2 line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
      &lt;td&gt;2 line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
      &lt;td&gt;2 line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Footer row&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
      &lt;td&gt;Footer row&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
      &lt;td&gt;Footer row&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
      &lt;td&gt;Footer row&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;code-blocks&quot;&gt;Code blocks&lt;/h2&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Example can be run directly in your JavaScript console&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Create a function that takes two arguments and returns the sum of those&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// arguments&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;adder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;return a + b&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Call the function&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;adder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// &amp;gt; 8&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;math&quot;&gt;Math&lt;/h2&gt;
&lt;p&gt;Lorem ipsum &lt;code class=&quot;MathJax_Preview&quot;&gt;f(x) = x^2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;f(x) = x^2&lt;/script&gt;.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\begin{aligned}
  \phi(x,y) &amp;amp;= \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right) \\[2em]
            &amp;amp;= \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j)            \\[2em]
            &amp;amp;= (x_1, \ldots, x_n)
               \left(\begin{array}{ccc}
                 \phi(e_1, e_1)  &amp;amp; \cdots &amp;amp; \phi(e_1, e_n) \\
                 \vdots          &amp;amp; \ddots &amp;amp; \vdots         \\
                 \phi(e_n, e_1)  &amp;amp; \cdots &amp;amp; \phi(e_n, e_n)
               \end{array}\right)
               \left(\begin{array}{c}
                 y_1    \\
                 \vdots \\
                 y_n
               \end{array}\right)
\end{aligned}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
  \phi(x,y) &amp;= \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right) \\[2em]
            &amp;= \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j)            \\[2em]
            &amp;= (x_1, \ldots, x_n)
               \left(\begin{array}{ccc}
                 \phi(e_1, e_1)  &amp; \cdots &amp; \phi(e_1, e_n) \\
                 \vdots          &amp; \ddots &amp; \vdots         \\
                 \phi(e_n, e_1)  &amp; \cdots &amp; \phi(e_n, e_n)
               \end{array}\right)
               \left(\begin{array}{c}
                 y_1    \\
                 \vdots \\
                 y_n
               \end{array}\right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;message-boxes&quot;&gt;Message boxes&lt;/h2&gt;
&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: You can add a message box.&lt;/p&gt;

&lt;h2 id=&quot;large-text&quot;&gt;Large text&lt;/h2&gt;
&lt;p class=&quot;lead&quot;&gt;You can add large text.&lt;/p&gt;

&lt;h2 id=&quot;large-images&quot;&gt;Large images&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://placehold.it/800x100&quot; alt=&quot;Full-width image&quot; class=&quot;lead&quot; data-width=&quot;800&quot; data-height=&quot;100&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;captions-to-images&quot;&gt;Captions to images&lt;/h2&gt;
&lt;p class=&quot;figure&quot;&gt;&lt;img src=&quot;https://placehold.it/800x100&quot; alt=&quot;Full-width image&quot; class=&quot;lead&quot; data-width=&quot;800&quot; data-height=&quot;100&quot; /&gt;
A caption to an image.&lt;/p&gt;

&lt;h2 id=&quot;large-quotes&quot;&gt;Large quotes&lt;/h2&gt;
&lt;blockquote class=&quot;lead&quot;&gt;
  &lt;p&gt;You can make a quote “pop out”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;faded-text&quot;&gt;Faded text&lt;/h2&gt;
&lt;p class=&quot;faded&quot;&gt;I’m faded, faded, faded.&lt;/p&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">Hydejack offers a few additional features to markup your markdown. Don’t worry, these are merely CSS classes added with kramdown’s {:...} syntax, so that your content remains compatible with other Jekyll themes.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/example-content-iii.jpg" /></entry><entry><title type="html">Undergraduate graduation</title><link href="http://localhost:4000/projects/2017-07-01-undergraduate-graduation/" rel="alternate" type="text/html" title="Undergraduate graduation" /><published>2017-07-01T00:00:00+01:00</published><updated>2017-07-01T00:00:00+01:00</updated><id>http://localhost:4000/projects/undergraduate-graduation</id><content type="html" xml:base="http://localhost:4000/projects/2017-07-01-undergraduate-graduation/">&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This post is outdated and only included for legacy reasons.
See the &lt;a href=&quot;/docs/&quot; class=&quot;heading flip-title&quot;&gt;Documentation&lt;/a&gt; for up-to-date instructions.&lt;/p&gt;</content><author><name>XinshaoAmosWang</name></author><summary type="html">NOTE: This post is outdated and only included for legacy reasons. See the Documentation for up-to-date instructions.</summary></entry></feed>