<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2020-09-09T00:13:28+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Postdoc@OxfordU</title><subtitle>&quot;Stay Hungry. Stay Foolish. -- Steve Jobs 2005&quot;. A ML/DL/AI Researcher
</subtitle><author><name>XW</name></author><entry><title type="html">Robust DL/ML</title><link href="http://localhost:4000/readingnotes/2020-09-08-RobustMLDL/" rel="alternate" type="text/html" title="Robust DL/ML" /><published>2020-09-08T00:00:00+01:00</published><updated>2020-09-08T00:00:00+01:00</updated><id>http://localhost:4000/readingnotes/RobustMLDL</id><content type="html" xml:base="http://localhost:4000/readingnotes/2020-09-08-RobustMLDL/">&lt;p&gt;In general, robust deep learning covers: missing labels (semisupervised learning); noisy labels (noise detection and correction); regularisation techniques; sample imbalance (long-tailed class distribution); adversarial learning; and so on.&lt;/p&gt;

&lt;ol class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#icml-20-papers&quot;&gt;ICML-20 papers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-design-of-loss-functions-ie-optimisation-objectives-or-output-regularistion&quot;&gt;The design of loss functions (i.e., optimisation objectives or output regularistion)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;icml-20-papers&quot;&gt;ICML-20 papers&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://proceedings.icml.cc/paper/2020/file/87682805257e619d49b8e0dfdc14affa-Paper.pdf&quot;&gt;Error-Bounded Correction of Noisy Labels, Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, Chao Chen&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;To be robust against label noise, many successful methods rely on the noisy classifiers (i.e., models trained on the noisy training data) to determine whether a label is trustworthy. However, it remains unknown why this heuristic works well in practice.&lt;/li&gt;
      &lt;li&gt;In this paper, &lt;strong&gt;we provide the first theoretical explanation&lt;/strong&gt; for these methods.&lt;/li&gt;
      &lt;li&gt;We prove that the prediction of a noisy classifier can indeed be a good indicator of whether the label of a training data is clean.&lt;/li&gt;
      &lt;li&gt;Based on the theoretical result, we propose a novel algorithm that corrects the labels based on the noisy classifier prediction. The corrected labels are &lt;strong&gt;consistent with the true Bayesian optimal classifier with high probability.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;We prove that when the noisy classifier has &lt;strong&gt;low confidence on the label of a datum, such label is likely corrupted.&lt;/strong&gt; In fact, we can quantify the threshold of confidence, below which the label is likely to be corrupted, and above which is it likely to be not. We also empirically show that the bound in our theorem is tight.&lt;/li&gt;
      &lt;li&gt;We provide a theorem quantifying how a noisy classifier’s prediction correlates to the purity of a datum’s label. This provides theoretical explanation for data-recalibrating methods for noisy labels.&lt;/li&gt;
      &lt;li&gt;Inspired by the theorem, we propose &lt;strong&gt;a new label correction algorithm with guaranteed success rate.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;A Bayes optimal classifier is the minimizer of the risk over all possible hypotheses.&lt;/li&gt;
      &lt;li&gt;We also have a burn-in stage in which we train the networkusing the original noisy labels for \(m\) epochs. During theburn-in stage, we use the original cross-entropy loss;&lt;/li&gt;
      &lt;li&gt;After the burn-in stage, we want to avoid overfitting of theneural network. To achieve this goal, we introduce aretroactive loss term. The  idea  is  to  enforce  the consistency between \(f\) and the prediction of the model at a previous epoch.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In all experiments, &lt;strong&gt;we use early stopping on validation set to tune hyperparameters and report theperformance on test set.&lt;/strong&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://xinshaoamoswang.github.io/blogs/2020-06-07-Progressive-self-label-correction/&quot;&gt;Simple and Effective ProSelfLC: Progressive Self Label Correction&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://proceedings.icml.cc/paper/2020/file/f2d887e01a80e813d9080038decbbabb-Paper.pdf&quot;&gt;Learning with Bounded Instance- and Label-dependent Label Noise, Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, Dacheng Tao&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Binary classification =&amp;gt; Not highly useful.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;. Specifically, we introduce the concept of &lt;strong&gt;distilled examples&lt;/strong&gt;, i.e. examples whose labels are identical with the labels assigned for them by the Bayes optimal classifier, and prove that &lt;strong&gt;under certain conditions classifiers learnt on distilled examples will converge to the Bayes optimal classifier&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Inspired by the idea of learning with distilled examples, we then propose a learning algorithm with &lt;strong&gt;theoretical guarantees for its robustness to BILN&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://proceedings.icml.cc/book/2020/hash/77f959f119f4fb2321e9ce801e2f5163&quot;&gt;Normalized Loss Functions for Deep Learning with Noisy Labels Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, James Bailey&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;This work is motivated by &lt;a href=&quot;https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/&quot;&gt;DM and IMAE&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;We provide new theoretical insights into robust loss func-tions demonstrating that a simple normalization can makeany loss function robust to noisy labels.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;We identify that existing robust loss functions suffer from an underfitting problem.  To address this, we propose ageneric framework Active Passive Loss(APL) to build new loss functions with &lt;strong&gt;theoretically guaranteed robustness and sufficient learning properties.&lt;/strong&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Robustness and Convergence?&lt;/strong&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://proceedings.icml.cc/static/paper_files/icml/2020/3285-Paper.pdf&quot;&gt;Searching to Exploit Memorization Effect in Learning with Noisy Labels QUANMING YAO, Hansi Yang, Bo Han, Gang Niu, James Kwok&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Sample selection approaches: select \(R(t)\) small-loss samples based on network’s predictions&lt;/li&gt;
      &lt;li&gt;Formulation as an AutoML Problem  (complex algorithm personally);&lt;/li&gt;
      &lt;li&gt;Bi-level optimisation&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;No sample selection is needed: &lt;a href=&quot;https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/&quot;&gt;DM and IMAE&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://proceedings.icml.cc/static/paper_files/icml/2020/4950-Paper.pdf&quot;&gt;Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates, Yang Liu, Hongyi Guo&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Overall, this method is complex due to &lt;strong&gt;peer samples&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;The motivation/highlight is not novel&lt;/strong&gt;: without Knowing Noise Rates.  Our main goal is to provide an al-ternative that does not require the specification of the noiserates, nor an additional estimation step for the noise.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Peer loss is invariant to label noise when optimizing with it. This effect helps us get rid of theestimation of noise rates.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;i) is robust to asymmetriclabel noise with &lt;strong&gt;formal theoretical guarantees&lt;/strong&gt;  and  ii)  requires  no  prior  knowledge  or  estimationof the noise rates (&lt;strong&gt;no need for specifying noise rates&lt;/strong&gt;).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;We also provide preliminary results on &lt;strong&gt;how peer loss generalizes to multi-class clas-sification problems.&lt;/strong&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Relevant work 1: &lt;a href=&quot;https://papers.nips.cc/paper/8853-l_dmi-a-novel-information-theoretic-loss-function-for-training-deep-nets-robust-to-label-noise.pdf&quot;&gt;neurips-19: \(L_{DMI}\): A Novel Information-theoretic Loss Functionfor Training Deep Nets Robust to Label Noise&lt;/a&gt; To the best ofour knowledge, \(L_{DMI}\) is the first loss function that is provably robust to instance-independent label noise, regardless of noise pattern, and it can be applied to any existing classification neural networks straightforwardly without any auxiliary information. In addition to theoretical justification, we also empirically show that using \(L_{DMI}\) outperforms all other counterparts in the classification task on both image dataset and natural language dataset include Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a variety of synthesized noise patterns and noise amounts,as well as a real-world dataset Clothing1M.
  The core of \(L_{DMI}\) is a generalized version of mutual information, termed Determinant based Mutual Information (DMI), which is not only information-monotone but also relatively invariant.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Relevant work 2: &lt;a href=&quot;https://arxiv.org/pdf/1802.08887.pdf&quot;&gt;Water from Two Rocks: Maximizing the Mutual Information&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;No loss function is needed: &lt;a href=&quot;https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/&quot;&gt;DM and IMAE&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://proceedings.icml.cc/book/2020/hash/2e2079d63348233d91cad1fa9b1361e9&quot;&gt;Federated Learning with Only Positive Labels Felix Xinnan Yu, Ankit Singh Rawat, Aditya Menon, Sanjiv Kumar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-design-of-loss-functions-ie-optimisation-objectives-or-output-regularistion&quot;&gt;The design of loss functions (i.e., optimisation objectives or output regularistion)&lt;/h3&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;:+1: &lt;a href=&quot;https://arxiv.org/pdf/1905.11528.pdf&quot;&gt;Improved Training Speed, Accuracy, and Data Utilization Through Loss Function Optimization&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Speed, Accuracy, Data Efficiency, etc;&lt;/li&gt;
      &lt;li&gt;BAIKAL loss;&lt;/li&gt;
      &lt;li&gt;Genetic Loss Function Optimization (GLO) builds loss functions hierarchically from a set of operators and leaf nodes;&lt;/li&gt;
      &lt;li&gt;A general framework for loss function metalearning, covering both novel loss function discovery and optimization, is developed and evaluated experimentally.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;No loss function is needed: &lt;a href=&quot;https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/&quot;&gt;DM and IMAE&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1506.01186.pdf&quot;&gt;Cyclical Learning Rates for Training Neural Networks&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1702.05659.pdf&quot;&gt;On loss functions for deep neural networks in classification =&amp;gt; with theory of robustness and convergence&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;We try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects;&lt;/li&gt;
      &lt;li&gt;We present new insights into theoretical properties of a couple of these losses;&lt;/li&gt;
      &lt;li&gt;We provide experimental evaluation of resulting models’ properties, including the effect on speed of learning, final performance, input data and label noise robustness as well as convergence.&lt;/li&gt;
      &lt;li&gt;So why is using these two loss functions (\(L_1\), \(L_2\) losses) unpopular? Is there anything fundamentally wrong with this formulation from the mathematical perspective? While the following observation is not definitive, it shows an insight into what might be the issue causing slow convergence of such methods.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Lack of convexity&lt;/strong&gt; comes from the same argument since &lt;strong&gt;second derivative wrt. to any weight in the final layer of the model changes sign (as it is equivalent to first derivative being non-monotonic)&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;strong&gt;Proposition 2&lt;/strong&gt;. \(L_1\), \(L_2\) losses applied to probabilities estimates coming
  from sigmoid (or softmax) have &lt;strong&gt;non-monotonic partial derivatives wrt. to the output of the final layer (and the loss is not convex nor concave wrt. to last layer weights)&lt;/strong&gt;. Furthermore, &lt;strong&gt;they vanish in both infinities, which slows down learning of heavily misclassified examples&lt;/strong&gt;.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;No loss function is needed: &lt;a href=&quot;https://xinshaoamoswang.github.io/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/&quot;&gt;DM and IMAE&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>XW</name></author><summary type="html">In general, robust deep learning covers: missing labels (semisupervised learning); noisy labels (noise detection and correction); regularisation techniques; sample imbalance (long-tailed class distribution); adversarial learning; and so on.</summary></entry><entry><title type="html">In deep metric learning, The improvements over time have been marginal?</title><link href="http://localhost:4000/blogs/2020-07-03-deep-metric-learning-improvement/" rel="alternate" type="text/html" title="In deep metric learning, The improvements over time have been marginal?" /><published>2020-07-03T00:00:00+01:00</published><updated>2020-07-03T00:00:00+01:00</updated><id>http://localhost:4000/blogs/deep-metric-learning-improvement</id><content type="html" xml:base="http://localhost:4000/blogs/2020-07-03-deep-metric-learning-improvement/">Recently, in paper [A Metric Learning Reality Check](https://arxiv.org/pdf/2003.08505.pdf), it is reported that the improvements over time have been marginal at best. Is it true? I present my personal viewpoints as follows:
* First of all, acedemic research progress is naturally slow, continuous and tortuous. Beyond, it is full of flaws on its progress. For example, 
    * In person re-identification, several years ago, some researchers vertically split one image into several parts for alignment, which is against the design of CNNs and non-meaningful. Because deep CNNs are designed to be invariant against translation, so that hand-crafted alignment is unnecessary. 

    * The Adam optimiser is found to be very sensitive to the setting of delta recently.

    * [Does Mean Absolute Error Treat Examples Equally?](https://arxiv.org/pdf/1903.12141.pdf)

    * [How to understand a loss function in a right way? Does a loss function have to be differentiable?](https://arxiv.org/pdf/1905.11233.pdf)

    * Is [DisturbLabel](https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_DisturbLabel_Regularizing_CNN_CVPR_2016_paper.pdf) a meaningful regulariser? If so, it makes me think that we should **deliberately generate label noise** at the training dataset! Is not it ridiculous? You will cultivate your own opinion after reading [ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks](https://arxiv.org/pdf/2005.03788.pdf).

    * [Confidence penalty or rewarding, which one to go ahead?](https://xinshaoamoswang.github.io/blogs/2020-06-07-Progressive-self-label-correction/#storyline)

    * [To trust a learner (i.e. a deep model) or human annotations (i.e. textbooks for supervision), which one to choose?](https://arxiv.org/pdf/2005.03788.pdf)
    
    * ...
    
* There are some vital breakthroughs over the time although they seem to be trivial now. 
    * [Multibatch Method](https://www.cse.huji.ac.il/~shashua/papers/multibatch-nips16.pdf), after which people rarely use rigid input formats. 
        * Before this milestone, we heard a lot about siamese networks, triplet networks, etc. 
        * After Multibatch Method, we construct doublets, triplets, or high-order tuples directly in the embedding space.

    * The importance of sample mining/weighting becomes clearer for our community. Of course, there exist many variants of sample mining/weighting for different scenarios.

    * Our community become much more open-minded: all methods which learn discriminative representations can be categorised into deep metric learning, e.g., softmax + categorical cross entropy.

    * Deep metric learning tends to follow a similar training setting with few-shot training, i.e., in one training batch, C classes and K examples per class are randomly sampled. 
    Naturally, we can make C and K random to increase the training stochasticity.  

    * ...</content><author><name>XW</name></author><summary type="html">Recently, in paper A Metric Learning Reality Check, it is reported that the improvements over time have been marginal at best. Is it true? I present my personal viewpoints as follows: First of all, acedemic research progress is naturally slow, continuous and tortuous. Beyond, it is full of flaws on its progress. For example, In person re-identification, several years ago, some researchers vertically split one image into several parts for alignment, which is against the design of CNNs and non-meaningful. Because deep CNNs are designed to be invariant against translation, so that hand-crafted alignment is unnecessary.</summary></entry><entry><title type="html">Robust Deep Learning via Derivative Manipulation and IMAE</title><link href="http://localhost:4000/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/" rel="alternate" type="text/html" title="Robust Deep Learning via Derivative Manipulation and IMAE" /><published>2020-06-14T00:00:00+01:00</published><updated>2020-06-14T00:00:00+01:00</updated><id>http://localhost:4000/blogs/Robust-Deep-LearningviaDerivativeManipulationIMAE</id><content type="html" xml:base="http://localhost:4000/blogs/2020-06-14-Robust-Deep-LearningviaDerivativeManipulationIMAE/">For source codes, the usage is conditioned on academic use only and kindness to cite our work: Derivative Manipulation and IMAE.&lt;br /&gt;
As a young researcher, your interest and kind citation (star) will definitely mean a lot for me and my collaborators.&lt;br /&gt;
For any specific discussion or potential future collaboration, please feel free to contact me. 

### When talking about robustness/regularisation, our community tend to connnect it merely to better test performance. I advocate caring training performance as well because: 
* If noisy training examples are fitted well, a model has learned something wrong;
* If clean ones are not fitted well,  a model is not good enough. 
* There is a potential arguement that the test dataset can be infinitely large theorectically, thus being significant. 
  * Personal comment: Though being true theorectically, in realistic deployment, we obtain more testing samples as time goes, accordingly we generally choose to retrain or fine-tune to make the system adaptive. Therefore, this arguement does not make much sense. 

### We really need to rethink robust losses and optimisation in deep learning!
* In [Normalized Loss Functions for Deep Learning with Noisy Labels](https://arxiv.org/abs/2006.13554),   it is stated in the abstract that &quot;**we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs.**&quot;
  * This statement is **Quite** **Contradictory**: A ROBUST LOSS IS NOT SUFFICIENT (i.e., ROBUST AND ACCURATE)?  =&gt; **Then what is value to say whether a loss is robust or not?**

* For me, **a trained robust model should be accurate on both training and testing datasets.**

* **I remark that we are the first to thoroughly analyse robust losses, e.g., MAE's underfitting, and how it weights data points.**


0. [IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude’s Variance Matters](../../my_docs/IMAE_Code_Illustration)
    * Following work: [Derivative Manipulation for General Example Weighting](https://arxiv.org/pdf/1905.11233.pdf)

0. [Derivative Manipulation for General Example Weighting](../../my_docs/DM_Code_Illustration)
    * Preliminary: [IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters](https://arxiv.org/pdf/1903.12141.pdf#arXiv%20Paper.IMAE%20for%20Noise-Robust%20Learning)

0. Github Pages
    * [DerivativeManipulation](https://github.com/XinshaoAmosWang/DerivativeManipulation)
    * [IMAE](https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE)
0. Citation
```
@article{wang2019derivative,
  title={Derivative Manipulation for General Example Weighting},
  author={Wang, Xinshao and Kodirov, Elyor and Hua, Yang and Robertson, Neil M},
  journal={arXiv preprint arXiv:1905.11233},
  year={2019}
}
```
```
@article{wang2019imae,
  title={ {IMAE} for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters},
  author={Wang, Xinshao and Hua, Yang and Kodirov, Elyor and Robertson, Neil M},
  journal={arXiv preprint arXiv:1903.12141},
  year={2019}
}
```
{:.message}</content><author><name>XW</name></author><summary type="html">For source codes, the usage is conditioned on academic use only and kindness to cite our work: Derivative Manipulation and IMAE. As a young researcher, your interest and kind citation (star) will definitely mean a lot for me and my collaborators. For any specific discussion or potential future collaboration, please feel free to contact me.</summary></entry><entry><title type="html">Progressive Self Label Correction (ProSelfLC) for Training Robust Deep Neural Networks</title><link href="http://localhost:4000/blogs/2020-06-07-Progressive-self-label-correction/" rel="alternate" type="text/html" title="Progressive Self Label Correction (ProSelfLC) for Training Robust Deep Neural Networks" /><published>2020-06-07T00:00:00+01:00</published><updated>2020-06-07T00:00:00+01:00</updated><id>http://localhost:4000/blogs/Progressive-self-label-correction</id><content type="html" xml:base="http://localhost:4000/blogs/2020-06-07-Progressive-self-label-correction/">For any specific discussion or potential future collaboration, please feel free to contact me. As a young
researcher, your interest and star (citation) will mean a lot for me and my collaborators. &lt;br /&gt;
Paper link: [https://arxiv.org/abs/2005.03788](https://arxiv.org/abs/2005.03788)
```
Cite our work if you find it useful
@article{wang2020proselflc,
  title={ProSelfLC: Progressive Self Label Correction 
  for Training Robust Deep Neural Networks},
  author={Wang, Xinshao and Hua, Yang and Kodirov, Elyor and Robertson, Neil M},
  journal={arXiv preprint arXiv:2005.03788},
  year={2020}
}
```

&lt;!-- :+1: means being highly related to my personal research interest. --&gt;
List of Content
0. [Feedbacks](#feedbacks)
0. [Storyline](#storyline)
0. [Open ML Research Questions](#open-ml-research-questions)
0. [Noticeable Findings](#noticeable-findings)
0. [Literature Review](#literature-review)
0. [In Self LC, a core question is not well answered](#in-self-lc-a-core-question-is-not-well-answered)
0. [Underlying Principle of ProSelfLC](#underlying-principle-of-proselflc)
0. [Mathematical Details of ProSelfLC](#mathematical-details-of-proselflc)
0. [Design Reasons of ProSelfLC](#design-reasons-of-proselflc)
0. [Related Interesting Work](#related-interesting-work)
{:.message}

## Feedbacks
* [Reviews](/imgs/figsProSelfLC/CMT_Review.pdf)
* [Personal replies](/imgs/figsProSelfLC/ProSelfLC_personal_response.pdf): This is an unofficial letter from me to let peers know better about our work.
{:.message}


## Storyline
* Human annotations contain bias, subjectiveness, and errors. 
    * Therefore, some prior work **penalises low-entropy statuses =&gt; so that wrong fitting is alleviated in some degree.** Representative proposals are label smoothing and confidence penalty.

* Our new finding on **Entropy Minimisation**: 
    * We can solve it still by minimum entropy regularisation principle;
    * Diverse minimum-entropy statuses exist (e.g., when a learner perfectly fits random labels, the entropy also reaches a minimum):
        * The minimum-entropy status defined by untrusted human-annotated labels is incorrect, thus leading to poor generalisation. \\
        **CCE =&gt; Non-meaningful minimum-entropy status =&gt; poor generalisation**.
        * We propose to redefine a more meaningful minimum-entropy status by exploiting the knowledge of a learner itself, which shows promising results.  \\
        **Label correction =&gt; Meaningful low-entropy status =&gt; good generalisation**.

* We highlight **ProSelfLC's Underlying Principle is ''Contradictory'' with: Maximum-Entropy Learning, Confidence Penalty and Label Smoothing**, which are popular recently. 
Then we wish our community think critically about two principles: 
    * **Rewarding a correct low-entropy status** (ProSelfLC)   
    * **Penalising a non-meaningful low-entropy status** (CCE+LS, or CCE+CP)
    * In our experiments: **ProSelfLC &gt; (CCE+LS, or CCE+CP) &gt; CCE**
    * Being contradictory in entropy, both help but their angles differ: 
        * CCE fits non-meaningful patterns =&gt; LS and CP penalise such fitting;
        * CCE fits non-meaningful patterns =&gt; ProSelfLC first corrects them =&gt; then fits. 
        
* Why does CCE fit non-meaningful patterns? 
    * [2019-Derivative manipulation for general example weighting](https://arxiv.org/pdf/1905.11233.pdf)
    * [2019-IMAE for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude’s variance matters.](https://arxiv.org/pdf/1903.12141.pdf)
{:.message}

## Open ML Research Questions
* Should we trust and exploit a learner’s knowledge as training goes, or always trust human
annotations?
    * As a learner, to trust yourself or supervison/textbooks?
    * The answer should depend on what a learner has learned.

* Should we optimise a learner towards a correct low-entropy status, or penalise a low-entropy
status?
    * As a supervisor/evaluator, to reward or penalise a confident learner?

* Open discussion: we show it’s fine for a learner to be confident towards a correct low-entropy status. 
Then more future research attention should be paid to the definition of correct knowledge, as in general we accept, human annotations used for learning supervision may be biased, subjective, and wrong.
* As a supervisor, before training multiple learners, to think about how to train one great learner first?
    * 1st context: recently, many techniques about **training multiple learners** (co-training, mutual learning, knowledge distillation, adversarial training, etc) have been proposed.
    * 2nd context: in our work, we work on **how to train single learner better.**
    * 1st personal comment: training multiple learners is much more expensive and complex;
    * 2nd personal comment: when training multiple learners collaboratively, if one learner does not perform well, it tends to hurt the other learners. 
{:.message}

## Noticeable Findings
* Rewarding low entropy (towards a meaningful status) leads to better generalisation than penalising low entropy.
{:.message}

Comprehensive learning dynamics for thorough understanding of learning behaviours.
![ProSelfLC comprehensive_dynamics](/imgs/figsProSelfLC/comprehensive_dynamics.png){:.lead data-width=&quot;800&quot; data-height=&quot;100&quot;}
{:.figure}
* Result analysis:
    * **Revising the semantic class and perceptual similarity structure.** Generally, the semantic class of an example is defined according to its perceptual similarities with training classes, and is chosen to be the most similar class. In Figure 3b and 3c, we show a learner’s behaviours on without fitting wrong labels and correcting them in different approaches. We remark that ProSelfLC performs the best.
    &lt;br /&gt;
    &lt;br /&gt;
    * **To reward or penalise low entropy?** LS and CP are proposed to penalise low entropy. On the one hand, we observe that LS and CP work, being consistent with prior evidence. As shown in Figure 3d and 3e, the entropies of both clean and noisy subset are the largest in LS and CP, and correspondingly their generalisation performance is the best except for ProSelfLC in Figure 3f. On the other hand, our ProSelfLC has the lowest low entropy while performs the best, which demonstrates **it does not hurt for a learner to be confident. However, a learning model needs to be careful about what to be confident in.** Let us look at Figure 3b and 3c, ProSelfLC has the least wrong fitting while the highest semantic class correction rate, which denotes it is confident in learning meaningful patterns.
{:.message}

## Literature Review

Target modification includes OR (LS and CP), and LC (Self LC and Non-self LC). &lt;br /&gt;
Self LC is the most appealing because it requires no extra learners to
revise learning targets, &lt;br /&gt; 
being free! &lt;br /&gt;
![Method Analysis](/imgs/figsProSelfLC/methods_grouping.png){:.lead data-width=&quot;800&quot; data-height=&quot;100&quot;}
{:.figure}

Summary of CCE, LS, CP and LC from the angle of target modification, entropy and KL divergence.
![Table Method Analysis](/imgs/figsProSelfLC/table_method_analysis.png){:.lead data-width=&quot;800&quot; data-height=&quot;100&quot;}
{:.figure}


## In Self LC, a core question is not well answered:
$$\textit{How much do we trust a learner to leverage its knowledge?}$$
{:.message}


## Underlying Principle of ProSelfLC
* When a learner starts to learn, it trusts the supervision from human annotations. 
&lt;br /&gt;
&lt;br /&gt;
This idea is inspired by the paradigm that deep models learn simple meaningful patterns before fitting noise, even when severe label noise exists in human annotations [1];
&lt;br /&gt;
&lt;br /&gt;
* As a learner attains confident knowledge as time goes, we leverage its confident knowledge to correct labels. 
&lt;br /&gt;
&lt;br /&gt;
This is surrounded by minimum entropy regularisation, which has been widely evaluated in unsupervised and semi-supervised scenarios [10, 2].
{:.message}


## Mathematical Details of ProSelfLC

Beyond semantic class: the similarity structure defined by a label distribution.
![The definiton of learning targets/labels](/imgs/figsProSelfLC/definition_learning_target.png){:.lead data-width=&quot;800&quot; data-height=&quot;100&quot;}
{:.figure}


Human annotations and predicted label distributions, which should we trust more?
![Mathematical expression of ProSelfLC](/imgs/figsProSelfLC/mathematical_expression.png){:.lead data-width=&quot;800&quot; data-height=&quot;100&quot;}
{:.figure}


## Design Reasons of ProSelfLC
*  Regarding $$g(t)$$, 
in the earlier learning phase, i.e., $$t &lt; \Gamma/2$$, $$g(t) &lt; 0.5 \Rightarrow \epsilon_{\mathrm{ProSelfLC}} &lt; 0.5, \forall \mathbf{p}$$, so that the human annotations dominate and ProSelfLC only modifies the similarity structure. This is because when a learner does not see the training data for enough times, we assume it is not trained well, which is the most elementary concept in deep learning. Most importantly, more randomness exists at the earlier phase, as a result, the learner may output a wrong confident prediction. In our design, $$\epsilon_{\mathrm{ProSelfLC}} &lt; 0.5, \forall \mathbf{p}$$ can assuage the bad impact of such unexpected cases.   
When it comes to the later learning phase, i.e., $$t &gt; \Gamma/2$$, we have $$g(t) &gt; 0.5$$, which means overall we give enough credits to a learner as it has been trained for more than the half of total iterations. 

*  Regarding $$l(\mathbf{p})$$, we discuss its effect in the later learning phase when it becomes more meaningful. 
If $$\mathbf{p}$$ is not confident, $$l(\mathbf{p})$$ will be large, then $$\epsilon_{\mathrm{ProSelfLC}}$$ will be small, which means we choose to trust a one-hot annotation more when its prediction is of high entropy, so that we can further reduce the entropy of output distributions}. In this case, ProSelfLC only modifies the similarity structure.
Beyond, when $$\mathbf{p}$$ is highly confident, there are two fine cases: If $$\mathbf{p}$$ is consistent with $$\mathbf{q}$$ in the semantic class, ProSelfLC only modifies the similarity structure too; If they are inconsistent, ProSelfLC further corrects the semantic class of a human annotation. 



Ablation study on the design of ProSelfLC, where $$\epsilon_{\mathrm{ProSelfLC}}$$ consistently performs the best when multiple metrics are reported. 
![Ablation study on the design of ProSelfLC](/imgs/figsProSelfLC/ablation_study.png){:.lead data-width=&quot;800&quot; data-height=&quot;100&quot;}
{:.figure}



Case analysis on the design of ProSelfLC.
![Case analysis on the design of ProSelfLC](/imgs/figsProSelfLC/case_analysis.png){:.lead data-width=&quot;800&quot; data-height=&quot;100&quot;}
{:.figure}

* **Correct the similarity structure for every data point in all cases.**
Given any data point $$\mathbf{x}$$, by a convex combination of $$\mathbf{p}$$ and $$\mathbf{q}$$, 
we add the information about its relative probabilities of being different training classes using the knowledge of a learner itself. 


* **Revise the semantic class of an example only when the learning time is long and its prediction is confidently inconsistent.**
As highlighted in Table 2, only when two conditions are met, we have $$\epsilon_{\mathrm{ProSelfLC}}  &gt; 0.5$$ and 
$$\argmax\nolimits_j \mathbf{p}(j|\mathbf{x}) \neq \argmax\nolimits_j \mathbf{q}(j|\mathbf{x})$$, then the semantic class in $\mathbf{\tilde{q}_{\mathrm{ProSelfLC}}}$ is changed to be determined by $$\mathbf{p}$$. 
For example, we can deduce $$\mathbf{p} = [0.95, 0.01, 0.04], \mathbf{q} = [0, 0, 1], \epsilon_{\mathrm{ProSelfLC}}=0.8 \Rightarrow \mathbf{\tilde{q}_{\mathrm{ProSelfLC}}}=(1- \epsilon_{\mathrm{ProSelfLC}})  \mathbf{q}+\epsilon_{\mathrm{ProSelfLC}} \mathbf{p}=[0.76, 0.008, 0.232]$$.  
Theoretically, ProSelfLC also becomes robust against long time being exposed to the training data, so that early stopping is not required.
{:.message}



## Related Interesting Work
* Contradictory Underlying Principle: Maximum-Entropy Learning, Confidence Penalty, Label Smoothing
    * [Confidence Penalty is proposed in Regularizing Neural Networks by Penalizing Confident Output Distributions](https://arxiv.org/pdf/1701.06548.pdf)
    * [Label Smoothing is proposed in Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567.pdf)
    * [Maximum Entropy Fine-Grained Classification](https://papers.nips.cc/paper/7344-maximum-entropy-fine-grained-classification.pdf)
* Deep models learn simple meaningful patterns before fitting noise, even when severe label noise exists in human annotations. 
    * [2019-Derivative manipulation for general example weighting](https://arxiv.org/pdf/1905.11233.pdf)
    ```
    @article{wang2019derivative,
        title={Derivative Manipulation for
        General Example Weighting},
        author={Wang, Xinshao and Kodirov, Elyor and Hua, Yang and Robertson, Neil M},
        journal={arXiv preprint arXiv:1905.11233},
        year={2019}
    }
    ```
    * [2019-IMAE for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude’s variance matters.](https://arxiv.org/pdf/1903.12141.pdf)
    ```
    @article{wang2019imae,
        title={ {IMAE} for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally 
        and Gradient Magnitude's Variance Matters},
        author={Wang, Xinshao and Hua, Yang and Kodirov, Elyor and Robertson, Neil M},
        journal={arXiv preprint arXiv:1903.12141},
        year={2019}
    }
    ```
    * Arpit, D., Jastrz ̨ebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M.S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., Lacoste-Julien, S.: A closer look at memorization in deep networks. In: ICML. (2017)
{:.message}</content><author><name>XW</name></author><summary type="html">For any specific discussion or potential future collaboration, please feel free to contact me. As a young researcher, your interest and star (citation) will mean a lot for me and my collaborators. Paper link: https://arxiv.org/abs/2005.03788 Cite our work if you find it useful @article{wang2020proselflc, title={ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks}, author={Wang, Xinshao and Hua, Yang and Kodirov, Elyor and Robertson, Neil M}, journal={arXiv preprint arXiv:2005.03788}, year={2020} }</summary></entry><entry><title type="html">Paper Summary on Distance Metric, Representation Learning</title><link href="http://localhost:4000/blogs/2020-04-23-deep-metric-learning/" rel="alternate" type="text/html" title="Paper Summary on Distance Metric, Representation Learning" /><published>2020-04-23T00:00:00+01:00</published><updated>2020-04-23T00:00:00+01:00</updated><id>http://localhost:4000/blogs/deep-metric-learning</id><content type="html" xml:base="http://localhost:4000/blogs/2020-04-23-deep-metric-learning/">:+1: means being highly related to my personal research interest. 
0. [arXiv 2020-On the Fairness of Deep Metric Learning](#arxiv-2020)
0. [ICCV 2019, CVPR 2020 Deep Metric Learning](#iccv-2019-cvpr-2020-deep-metric-learning)
0. [CVPR 2019 Deep Metric Learning](#cvpr-2019-deep-metric-learning)
0. [Few-shot Learning](#few-shot-learning)
0. [Large Output Spaces](#large-output-spaces)
0. [Poincaré, Hyperbolic, Curvilinear](#poincaré-hyperbolic-curvilinear)
0. [Wasserstein](#wasserstein)
0. [Semi-supervised or Unsupervised Learning](#semi-supervised-or-unsupervised-learning)
0. [NeurIPS 2019-Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers](#neurips-2019-stochastic-shared-embeddings-data-driven-regularization-of-embedding-layers)
{:.message}


## arXiv 2020
* [Revisiting Training Strategies and Generalization Performance in Deep Metric Learning-Karsten Roth et al](https://arxiv.org/pdf/2002.08473.pdf)
    * Deep Metric Learning (DML) is arguably one of
the most influential lines of research for learning visual similarities with many proposed approaches every year. Although the field benefits
from the rapid progress, the divergence in training
protocols, architectures, and parameter choices
make an unbiased comparison difficult. To provide a consistent reference point, we revisit the
most widely used DML objective functions and
conduct a study of the crucial parameter choices
as well as the commonly neglected mini-batch
sampling process. Based on our analysis, we uncover a correlation between the embedding space
compression and the generalization performance
of DML models. Exploiting these insights, we
propose a simple, yet effective, training regularization to reliably boost the performance of rankingbased DML models on various standard benchmark datasets. 
    * We propose a simple technique to regularize the embedding space compression
which we find to boost generalization performance of
ranking-based DML approaches.

* [Unbiased Evaluation of Deep Metric Learning Algorithms--Istvan Feh ´ erv ´ ari etal 2019](https://arxiv.org/pdf/1911.12528.pdf)
    * we perform an unbiased comparison of
the most popular DML baseline methods under same conditions and more importantly, not obfuscating any hyper
parameter tuning or adjustment needed to favor a particular method. We find, that under equal conditions several
older methods perform significantly better than previously
believed.
    * In this work, **it stated &quot;On the SOP dataset, we never managed to make this algorithm converge.&quot; using [Ranked List Loss](https://arxiv.org/abs/1903.03238).** 
        * This is not the fact: I thank their interest in our work, which is a great motivation for me and my collaborators. I appreciate their report on the difficulty of applying our method.
        * Please see [Ranked List Loss](https://arxiv.org/abs/1903.03238) for its improved results, and [Github page](https://github.com/XinshaoAmosWang/Ranked-List-Loss-for-DML) for reproducible results.  

* [A Metric Learning Reality Check--Kevin Musgrave, Serge Belongie, Ser-Nam Lim](https://arxiv.org/pdf/2003.08505.pdf)
{:.message}

## ICCV 2019, CVPR 2020 Deep Metric Learning
* [Mic: Mining interclass characteristics for improved metric learning-Karsten Roth∗ , Biagio Brattoli⋆ , Bjorn Ommer](http://openaccess.thecvf.com/content_ICCV_2019/papers/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.pdf)
    * The common approach to metric learning is to **enforce a representation that is invariant under all factors but the ones of interest.** (Very Common Practice)
    * In contrast, we propose to **explicitly learn the latent characteristics that are shared by and go across object classes**. We can then directly explain away structured visual variability, rather than assuming it to be unknown random noise. (Being Contrastive is Interesting! =&gt; Regularisation Technique?)
    * We propose a novel surrogate task to learn visual characteristics shared across classes with a separate encoder. This encoder is trained jointly with the encoder for class information by reducing their mutual information.
    * ResNet-50 + PyTorch
    * Complex methods for me:  **The number of clusters is set before training to a fixed, problem-specific value**: 30 for CUB200-2011 [37], 200 for CARS196 [19], 50 for Stanford Online Products [28], 150 for In-Shop Clothes [43] and 50 for PKU VehicleID [21]. We update the cluster labels every other epoch.
    * For all experiments, we use the original images without bounding boxes.


* [Cross-Batch Memory for Embedding Learning-Xun Wang∗ , Haozhi Zhang∗ , Weilin Huang†, Matthew R. Scott](https://arxiv.org/pdf/1912.06798.pdf)
    * We propose a cross-batch memory (XBM) mechanism that memorizes the embeddings of past iterations, allowing the model to collect sufficient hard negative pairs across multiple minibatches - even over the whole dataset.
    * GoogLeNet V1, V2 and ResNet-50
    
* [SoftTriple Loss: Deep Metric Learning Without Triplet Sampling, Qian, Qi and Shang, Lei and Sun, Baigui and Hu, Juhua and Li, Hao and Jin, Rong](http://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.pdf)
    
    * **Multiple Centers or Adaptive Number of Centers =&gt; Softmax Loss** 
    
    * Analogous to  ProxyNCA or ProxyTriplet 
    
    * **Considering that images in CUB-2011 and Cars196 are similar to those in ImageNet, we freeze BN on these two data sets and keep BN training on the rest one.** Embeddings of examples and centers have the unit length in the experiments. 
    
    * Backbone: GoogLeNet V2 (Inception with BN)
    
    * During training, only random horizontal mirroring and random crop are used as the data augmentation. A single center crop is taken for test.
    
    * CUB-2011: We note that different works report the results with different dimension of embeddings while the size of embeddings has a significant impact on the performance. **For fair comparison, we report the results for the dimension of 64, which is adopted by many existing methods and the results with 512 feature embeddings, which reports the state-of-the-art results on most of data sets.**
    
    * Prior Work: ProxyNCA

* [Circle Loss: A Unified Perspective of Pair Similarity Optimization](https://arxiv.org/pdf/2002.10857.pdf)
    *  Motivation: aiming to maximize the within-class similarity $$s_p$$ and minimize the between-class similarity $$s_n$$. We find a majority of loss functions, including the triplet loss and the softmax plus cross-entropy loss, embed $$s_n$$ and $$s_p$$ into similarity pairs and seek to reduce $$(s_n − s_p)$$. Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. 
    * Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized.
    * we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. 
    *  Circle loss offers a more flexible optimization approach towards a more definite convergence target,
compared with the loss functions optimizing $$(s_n − s_p)$$.
    * (1)  a unified loss function; (2) flexible optimization; (3) definite convergence status. 
    * Evaluation:
        * Tasks: 
            * Face recognition 
            * Person re-identification (Market-1501,MSMT17)
            * Fine-grained image retrieval (CUB-100-2011, CARS-196, SOP-11318)

        * Net architecture-1: ResNet50 (globla) + MGN (local features) for person reid (. Our implementation concatenates all the part features into a single feature vector for simplici);

        * Net architecture-2: GoogLeNet (BN-Inception) for CUB, CARS, SOP, 512-D embeddings;
    
    * The performance is not better than Ranked List Loss on SOP. 
    
* [Sampling Wisely: Deep Image Embedding by Top-k Precision Optimization](http://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_Sampling_Wisely_Deep_Image_Embedding_by_Top-K_Precision_Optimization_ICCV_2019_paper.pdf)
    * This work is partially inspired by our work: Ranked List Loss, CVPR 2019
    * In contrast, in this paper, we propose a novel deep image embedding algorithm with end-to-end optimization to top-k precision, the evaluation metric that is **closely related to user experience.**
    * Specially, our loss function is constructed with **Wisely Sampled “misplaced” images along the top-k nearest neighbor decision boundary,** so that the gradient descent update directly
promotes the concerned metric, top-k precision.
    * **Our theoretical analysis** on the upper bounding and consistency properties of the proposed loss supports that minimizing our proposed loss is equivalent to maximizing top-k precision
    * Evaluation:
        * Datasets: CUB-200-2011, CARS-196, SOP 
        * PyTorch + Adam
        * Net architecture: Densenet 201, GoogLeNet V2 (Inception with BN)
        * Finetuning
        * Embedding size: 64, 512? 
        * Input size: warp (256x256) =&gt; crop (227x227)
        * Testing: only center crop
    * The performance is not better than Ranked List Loss
{:.message}

## CVPR 2019 Deep Metric Learning 
* [Divide and Conquer the Embedding Space for Metric Learning](http://openaccess.thecvf.com/content_CVPR_2019/papers/Sanakoyeu_Divide_and_Conquer_the_Embedding_Space_for_Metric_Learning_CVPR_2019_paper.pdf) 
:+1:  
    * ResNet-50
    * Each learner will learn a separate distance metric using only a subspace of the original embedding space and **a part of the data**. 

    * Natural hard negatives mining: Finally, **the splitting and sampling connect to hard negative mining**, which is verified by them. (I appreciate this ablation study in Table 6 )
    * Divide means: 
    (1) Splitting the training data into K Clusters; 
    (2) Splitting the embedding into K Slices.  


* [Deep Metric Learning to Rank=FastAP](http://openaccess.thecvf.com/content_CVPR_2019/papers/Cakir_Deep_Metric_Learning_to_Rank_CVPR_2019_paper.pdf) :+1:
    * ResNet-18 &amp; ResNet-50
    * Our main contribution is a novel solution to optimizing Average Precision under the Euclidean metric, based on the probabilistic interpretation of AP as the area under precision-recall curve, as well as distance quantization.
    * We also propose a category-based minibatch sampling strategy and a large-batch training heuristic.
    * On three **few-shot image retrieval datasets**, FastAP consistently outperforms competing methods, which often involve complex optimization heuristics or costly model ensembles.


* [Multi-Similarity Loss With General Pair Weighting for Deep Metric Learning](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf) :+1:
    * Objective of the proposed multi-similarity loss, which aims to collect informative pairs, and weight these pairs through their own and relative similarities.
    * GoogLeNet V2 (Inception BN)


* [Ranked List Loss for Deep Metric Learning](https://arxiv.org/pdf/1903.03238.pdf) :+1:
    * GoogLeNet V2 (Inception BN)

* [Stochastic Class-Based Hard Example Mining for Deep Metric Learning](http://openaccess.thecvf.com/content_CVPR_2019/papers/Suh_Stochastic_Class-Based_Hard_Example_Mining_for_Deep_Metric_Learning_CVPR_2019_paper.pdf) :+1:
    * Inception V1
    * Scale linearly to the number of classes. 
    * The methods proposed by Movshovitz-Attias et al. [14] and Wen et al. [34] are related to ours in a sense that class representatives are jointly trained with the feature extractor. 
However, their goal is to formulate new losses using the class representatives whereas we use them for hard negative mining.
    * Given an anchor instance, our algorithm first selects a few hard negative classes based on the class-to-sample distances and then performs a refined search in an instance-level only from the selected classes.

* A Theoretically Sound Upper Bound on the Triplet Loss for Improving the Efficiency of Deep Distance Metric Learning :+1:

* **Unsupervised** Embedding Learning via Invariant and Spreading Instance Feature :+1:


* [Signal-To-Noise Ratio: A Robust Distance Metric for Deep Metric Learning](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yuan_Signal-To-Noise_Ratio_A_Robust_Distance_Metric_for_Deep_Metric_Learning_CVPR_2019_paper.pdf) :+1:

    * We propose a robust SNR distance metric based on Signal-to-Noise Ratio (SNR) for measuring the similarity of image pairs for deep metric learning. Compared with Euclidean distance metric, our SNR distance metric can further jointly reduce the intra-class distances and enlarge the inter-class distances for learned features.
    * SNR in signal processing is used to measure the level of a desired signal to the level of noise, and a larger SNR value means a higher signal quality.
    For similarity measurement in deep metric learning, a pair of learned features x and y can be given as y = x + n, where n can be treated as a noise. Then, the SNR is the ratio of the feature variance and the noise variance.
    * To show the generality of our SNR-based metric, we also extend our approach to hashing retrieval learning.


* [Spectral Metric for Dataset Complexity Assessment](http://openaccess.thecvf.com/content_CVPR_2019/papers/Branchaud-Charron_Spectral_Metric_for_Dataset_Complexity_Assessment_CVPR_2019_paper.pdf) :+1:

    * Related work: [Measuring the Intrinsic Dimension of Objective Landscapes ICLR 2018](https://openreview.net/forum?id=ryup8-WCW), 
    [How Complex is your classification problem? A survey on measuring classification complexity Survey on complexity measures](https://arxiv.org/abs/1808.03591)


* Deep Asymmetric Metric Learning via Rich Relationship Mining :+1:
    * DAMLRRM relaxes the constraint on positive pairs to extend the generalization capability. We build positive pairs training pool by constructing a minimum connected tree for each category instead of considering all positive pairs within a mini-batch. As a result, there will exist a direct or indirect path between any positive pair, which ensures the relevance being bridged to each other. The inspiration comes from ranking on manifold [58] that spreads the relevance to their nearby neighbors one by one.
    * Idea is novel. The results on SOP are not good, only 69.7 with GoogLeNet



* [Hybrid-Attention Based Decoupled Metric Learning for Zero-Shot Image Retrieval](http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Hybrid-Attention_Based_Decoupled_Metric_Learning_for_Zero-Shot_Image_Retrieval_CVPR_2019_paper.pdf) :-1:
    * Very complex: object attention, spatial attention, random walk graph, etc.

* [Deep Metric Learning Beyond Binary Supervision](https://arxiv.org/pdf/1904.09626.pdf) :-1:
    * Binary supervision indicating whether a pair of images are of the same class or not.
    * Using continuous labels
    * Learn the degree of similarity rather than just the order.
    * A triplet mining strategy adapted to metric learning with continuous labels.
    * Image retrieval tasks with continuous labels in terms of human poses, room layouts and image captions.

* Hardness-aware deep metric learning 
:-1: : data augmentation

* Ensemble Deep Manifold Similarity Learning using Hard Proxies :-1: random walk algorithm, ensemble models.

* Re-Ranking via Metric Fusion for Object Retrieval and Person Re-Identification :-1:

* Deep Embedding Learning With Discriminative Sampling Policy :-1:
* Point Cloud Oversegmentation With Graph-Structured Deep Metric Learning :-1: 
* Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval :-1:
* A Compact Embedding for Facial Expression Similarity :-1:
* [RepMet: Representative-Based Metric Learning for Classification and Few-Shot Object Detection](http://openaccess.thecvf.com/content_CVPR_2019/papers/Karlinsky_RepMet_Representative-Based_Metric_Learning_for_Classification_and_Few-Shot_Object_Detection_CVPR_2019_paper.pdf) :-1:
* Eliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking :-1:
{:.message}

## [Few-shot Learning](/my_docs/few-shot/) 
* ICLR 2018-Meta-Learning for Semi-Supervised Few-Shot Classification
* NeurIPS 2019-Unsupervised Meta Learning for Few-Show Image Classification
* NeurIPS 2019-Learning to Self-Train for Semi-Supervised Few-Shot Classification
* NeurIPS 2019-Adaptive Cross-Modal Few-shot Learning
* NeurIPS 2019-Cross Attention Network for Few-shot Classification
* NeurIPS 2019-Incremental Few-Shot Learning with Attention Attractor Networks
* ICML 2019-LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning
{:.message}

## [Large Output Spaces](/my_docs/large-output-spaces/)
* NeurIPS 2019-Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces
* AISTATS 2019-Stochastic Negative Mining for Learning with Large Output Spaces
{:.message}

## [Poincaré, Hyperbolic, Curvilinear](/my_docs/Poincare-Hyperbolic-Curvilinear/)
* NeurIPS 2019-Multi-relational Poincaré Graph Embeddings
* NeurIPS 2019-Numerically Accurate Hyperbolic Embeddings Using Tiling-Based Models
* NeurIPS 2019-Curvilinear Distance Metric Learning
{:.message}



## [Wasserstein](/my_docs/wasserstein/)
* NeurIPS 2019-Generalized Sliced Wasserstein Distances 
* NeurIPS 2019-Tree-Sliced Variants of Wasserstein Distances
* NeurIPS 2019-Sliced Gromov-Wasserstein
* NeurIPS 2019-Wasserstein Dependency Measure for Representation Learning
{:.message}


## [Semi-supervised or Unsupervised Learning](/my_docs/Semi-Un-Supervised-Learning/)
* CVPR 2019-Label Propagation for Deep Semi-supervised Learning
* NeurIPS 2017-Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results
* ICLR 2019-Unsupervised Learning via Meta-Learning
{:.message}




## [NeurIPS 2019-Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers](https://arxiv.org/pdf/1905.10630.pdf)
**NOTE**: 
In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters.**Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope to reduce statistical complexity.** Alternatively, we propose stochastically shared embeddings (SSE), a data-driven approach to regularizing embedding layers, which stochastically transitions between embeddings during stochastic gradient descent (SGD). Because SSE integrates seamlessly with existing SGD algorithms, it can be used with only minor modifications when training large scale neural networks. We develop two versions of SSE: SSE-Graph using knowledge graphs of embeddings; SSE-SE using no prior information. We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages. **We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed SSE can further reduce overfitting, which often leads to more favorable generalization results.** &lt;br /&gt;
We conducted **experiments for a total of 6 tasks from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages.** 
{:.message}</content><author><name>XW</name></author><summary type="html">:+1: means being highly related to my personal research interest. arXiv 2020-On the Fairness of Deep Metric Learning ICCV 2019, CVPR 2020 Deep Metric Learning CVPR 2019 Deep Metric Learning Few-shot Learning Large Output Spaces Poincaré, Hyperbolic, Curvilinear Wasserstein Semi-supervised or Unsupervised Learning NeurIPS 2019-Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Paper Summary on Label Manipulation, Output Regularisation (Optimisation tricks)</title><link href="http://localhost:4000/blogs/2020-04-09-label-manipulation/" rel="alternate" type="text/html" title="Paper Summary on Label Manipulation, Output Regularisation (Optimisation tricks)" /><published>2020-04-09T00:00:00+01:00</published><updated>2020-04-09T00:00:00+01:00</updated><id>http://localhost:4000/blogs/label-manipulation</id><content type="html" xml:base="http://localhost:4000/blogs/2020-04-09-label-manipulation/">:+1: means being highly related to my personal research interest. 
0. [Label Smoothing](#label-smoothing)
0. [Confidence Penalty](#)
0. [Label Correction](#)
0. [Example Weighting](#)
0. [Know the unknown](#know-the-unknown)
0. [Semi-supervised learning](#)

[Related Notes](../2020-02-14-Core-machine-learning-topics/#knowledge-distillation)
{:.message}

## Experiments
* [ICLR 2020 Discussion](https://openreview.net/forum?id=SJxyOhVtvB)
* 
{:.message}


## Know the unknown and open-set noise
* 
{:.message}

## Label Smoothing
* [Does label smoothing mitigate label noise?- Michal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon and Sanjiv Kumar](https://arxiv.org/pdf/2003.02819.pdf)
    * **The definition of LS:** Label smoothing is commonly used in training deep learning models, wherein one-hot training labels are mixed with uniform label vectors.

    * While **label smoothing apparently amplifies this problem — being equivalent to injecting symmetric noise to the labels** — we show how it relates to a general family of loss-correction techniques from the label noise literature. Building on this connection, we show that label smoothing is competitive with loss-correction under label noise. 
        * **Do you agree with this?** 

    * Further, we show that when distilling models from noisy data, label smoothing of the teacher is beneficial; this is in contrast to recent findings for noise-free problems, and sheds further light on settings where label smoothing is beneficial.
    
    * Interestingly, there are two competing intuitions. On the one hand, smoothing might mitigate the problem, as it **prevents overconfidence on any one example**. On the other hand, smoothing might accentuate the problem, as it is **equivalent to injecting uniform noise into all labels** [ DisturbLabel Xie et al., 2016].

    * **At first glance, this connection indicates that smoothing has an opposite effect to one such loss-correction technique.** However, we **empirically show that smoothing is competitive with such techniques in denoising**, and that it improves performance of distillation.
        * we present a novel connection of label smoothing to loss correction techniques from the label noise literature;
        * We empirically demonstrate that label smoothing significantly improves performance under label noise, which we explain by relating smoothing to l2 regularisation. 
        * we show that when distilling from noisy labels, smoothing the teacher improves the student. While Müller et al. [2019] established that label smoothing can harm distillation, we show an opposite picture in noisy settings.

* [Does label smoothing mitigate label noise?- Label smoothing meets loss correction](https://arxiv.org/pdf/2003.02819.pdf)
    * 

* [DisturbLabel: Regularizing CNN on the Loss Layer-CVPR 2016-Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, Qi Tian](https://arxiv.org/pdf/1605.00055.pdf)
    * Randomly replaces a part of labels as incorrect values in each iteration.

    * In each training iteration, DisturbLabel randomly selects a small subset of samples (from those in the current mini-batch) and randomly sets their ground-truth labels to be incorrect, which results in a noisy loss function and, consequently, noisy gradient back-propagation.

    * DisturbLabel works on each mini-batch independently.

* [Rethinking the inception architecture for computer vision-CVPR 2016 Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z](https://arxiv.org/pdf/1512.00567.pdf)
    * Model Regularization via Label Smoothing
    * LS is firstly proposed in this paper. 

* [Distilling the knowledge in a neural network-NeurIPS 2015 Workshop-Hinton, G., Vinyals, O., Dean, J](https://arxiv.org/pdf/1503.02531.pdf)
    
    * **Soft targets definition**: An obvious way to transfer the generalization ability of the cumbersome model to a small model is
    to **use the class probabilities produced by the cumbersome model as “soft targets” for training the small model.**

    * **More information and less variance**: When the soft targets have high entropy, they provide **much more information** per training case than hard targets and **much less variance in the gradient between training cases**, so the small model can often be trained on much    less data than the original cumbersome model and using a much higher learning rate.

    * **Why?**: 
        * **Feasibility**: **Caruana and his collaborators [1]** have shown that it is possible to
        compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique.
        * For tasks like MNIST in which the cumbersome model almost always produces the correct answer with very high confidence, much of the information about the learned function resides in the ratios of very small probabilities in the soft targets. For example, one version of a 2 may be given a probability of 10−6 of being a 3 and 10−9 of being a 7 whereas for another version it may be the other way around. 
        * **Relative Probabilities?** This is valuable information that defines a rich similarity structure over the data (i. e. it says which 2’s look like 3’s and which look like 7’s) but it has very little influence on the cross-entropy cost function during the transfer stage because **the probabilities are so close to zero.** 

        * **Matching Logits?**: **Caruana and his collaborators** circumvent this problem by **using the logits (the inputs to the final softmax) rather than the probabilities produced by the softmax as the targets** for learning the small model and they minimize the squared difference between the logits produced by the cumbersome model and the logits produced by the small model.
    
    * **Distillation Definition**: 
        * Our more general solution, called “distillation”, is to **raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets**. We then use the same high temperature when training the small model to match these soft targets. We show later that matching the logits of the cumbersome model is actually a special case of distillation.
        * we call “distillation” to transfer the knowledge from the cumbersome model to a small model that is more suitable for deployment.
    
    * **Why Temperature?** =&gt; **Matching Logits is a special case of distillaiton?**
        * Using a higher value for T produces a softer probability distribution over classes.
        

    * **Knowledge Definition**: 
        * Relative probabilities: For cumbersome models that learn to discriminate between a large number of classes, the normal training objective is to maximize the average log probability of the correct answer, but a side-effect of the learning is that **the trained model assigns probabilities to all of the incorrect answers and even when these probabilities are very small**, **some of them are much larger than others**. **The relative probabilities of incorrect answers** tell us a lot about how the cumbersome model tends to generalize.
        
    * **Training Data**: The transfer set that is used to train the small model could consist entirely of unlabeled data [1] or we could use the original training set.  
        * We have found that using the original training set works well, especially if we add a small term to the objective function that encourages the small model to predict the true targets as well as matching the soft targets provided by the cumbersome model.

    * **Case Analysis**
        * In the simplest form of distillation: knowledge is transferred to the distilled model by training it on a transfer set and using a soft target distribution for each case in the transfer set that is produced by using the cumbersome model with a high temperature in its softmax. The same high temperature is used when training the distilled model, but after it has been trained it uses a temperature of 1.
        * **Two objectives: matching correct labels and soft targets generated by a cumbersome model**: When the correct labels are known for all or some of the transfer set, this method can be significantly improved by also training the distilled model to produce the correct labels. One way to do this is to use the correct labels to modify the soft targets, but we found that a better way is to simply use a weighted average of two different objective functions.
        * Matching logits is a special case of distillation? **Matching softer probabilities produced with high temperature versus matching logits!**
{:.message}

## Entropy Minimization (Minimum Entropy Principle)
* [Semi-supervised Learning by Entropy Minimization-NeurIPS 2015-Yves Grandvalet, Yoshua Bengio](https://papers.nips.cc/paper/2740-semi-supervised-learning-by-entropy-minimization.pdf)
    * We consider the **semi-supervised learning problem**, where a decision rule is to be learned from labeled and unlabeled data. 
    * A series of experiments illustrates that the proposed solution **benefits from unlabeled data**.
    * The method challenges **mixture models** when the data are sampled from the **distribution class spanned by the generative model**. The performances are definitely in favor of minimum entropy regularization when generative models are misspecified, and the weighting of unlabeled data provides robustness to the violation of the “cluster assumption”. 

    * Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces.
* 
{:.message}</content><author><name>XW</name></author><summary type="html">:+1: means being highly related to my personal research interest. Label Smoothing Confidence Penalty Label Correction Example Weighting Know the unknown Semi-supervised learning</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Example Weighting, Importance Sampling?</title><link href="http://localhost:4000/blogs/2020-03-08-example-weighting/" rel="alternate" type="text/html" title="Example Weighting, Importance Sampling?" /><published>2020-03-08T00:00:00+00:00</published><updated>2020-03-08T00:00:00+00:00</updated><id>http://localhost:4000/blogs/example-weighting</id><content type="html" xml:base="http://localhost:4000/blogs/2020-03-08-example-weighting/">0. [Importance sampling](#importance-samplinggradient)
{:.message}


### Importance sampling--Gradient?
* [Not All Samples Are Created Equal: Deep Learning with Importance Sampling-Angelos Katharopoulos et al](http://proceedings.mlr.press/v80/katharopoulos18a.html)

* [Stochastic Optimization with Importance Sampling for Regularized Loss Minimization-Peilin Zhao, Tong Zhang](http://proceedings.mlr.press/v37/zhaoa15.pdf)

* [Stochastic Optimization with Importance Sampling-Peilin Zhao et al](https://arxiv.org/pdf/1401.2753.pdf)

* [Importance Sampling for Minibatches-Dominik Csiba et al](http://www.jmlr.org/papers/volume19/16-241/16-241.pdf)

* [Safe Adaptive Importance Sampling-Sebastian U. Stich et al](https://papers.nips.cc/paper/7025-safe-adaptive-importance-sampling.pdf)
{:.message}</content><author><name>XW</name></author><summary type="html">Importance sampling</summary></entry><entry><title type="html">I Love Learning and Applying Mathematics, Statistics!</title><link href="http://localhost:4000/blogs/2020-03-04-I-love-math/" rel="alternate" type="text/html" title="I Love Learning and Applying Mathematics, Statistics!" /><published>2020-03-04T00:00:00+00:00</published><updated>2020-03-04T00:00:00+00:00</updated><id>http://localhost:4000/blogs/I-love-math</id><content type="html" xml:base="http://localhost:4000/blogs/2020-03-04-I-love-math/">0. [SGD &amp; Newton's Method &amp; Second-order Derivative Optimisation](#sgd--newtons-method--second-order-derivative-optimisation)
0. [Linear algebra](#linear-algebra)
0. [Probabilistic view of the world](#probabilistic-view-of-the-world)
0. [Optimisation](#optimisation)
{:.message}

### SGD &amp; Newton's Method &amp; Second-order Derivative Optimisation
* [Newton's Method](https://en.wikipedia.org/wiki/Newton%27s_method)
* [Newton's Method: Second-order Derivative Optimisation](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization)


### Linear algebra
* [Properties of the Covariance Matrix](http://www.robots.ox.ac.uk/~davidc/pubs/tt2015_dac1.pdf)
* [Covariance and Correlation](https://www.mathsisfun.com/data/correlation.html)
    * Interpreting: **Variable = Axis**
        * Variable = axis/component/one hyper-line
        * Observations of this variable = points of this axis/hyper-line
        * The observations of a variable = one vector of points in this line.
        * $$E(XY)=$$ the dot product of two variables' observation vector (multiple points for each variable) 
        * If $$X,Y$$ are orthogonal, then for any point in $$X$$ and any point in $$Y$$, their dot product are zero, therefore, we have $$E(XY) = 0$$.  
        * Diagonalisation (Orthogonal, Making them independent) =&gt;  Decorrelation
            * In this context, uncorrelation = independence. 
    * Uncorrelation (Orthogonalisaion) using Eigen vectors 
        * **Projection of a normal distribtion** $$ X \sim \mathcal{N}(0,\,{\sigma_x}^2)$$ to a standard normal distribution $$ \mathcal{N}(0,\,1)$$: 
            * **the projected distribution** is $$\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)$$
            * $$\theta$$ determines their linear dependency: correlation coefficient=$$\cos(\theta)$$. 
        * For each eigen vector: **we project all the original variables to this eigen vector** (an axis in the transformed orthogonal space) =&gt; summarise/accumulate those projected variables in this axis =&gt; New variable 

            * Two accumulated projected new variables of two axises (eigen vectors) are uncorrelated (being independent now).

            * Eigen value = sum of projected standard deviation.

    * The number of variables, the number of observations, the number of axises/components
        * The number of variables = the number of axis. 
        * When the number of variables (feature dim) &gt; the number of observations? 

    * In a high-dimensional space with orthogonal axises: 
        * Each axis is one independent event/variable. (Without losing generality, feel free to treat it as an unit normal distribution $$~\sim \mathcal{N}(0,\,1)$$) 
        * **The sum of two independent normal random variables is also normal.** However, if the two normal random variables are not independent, then their sum is not necessarily normal.
        * Then, the whole space becomes the combination (summarisation) of multiple independent normal random variables. 
        
        * In this context, **uncorrelation = independence.**
        Independent variables = indepdent axis/components = orthogonal components.

    * Covariance: 
        * remove mean for each axis/variable 
        * projection (accumulation/expectation of dot product of observations from different axis/variable)
        * In other words, dot product of two points (dim &gt;= 2) from two axises (out of multiple axises) =&gt; expectation/accumulation
    * **Dot product of two variable/axis**
    * Correlation = $$\frac{Covariance(X_i, X_j)} { \sigma_i \times \sigma_j } \in [-1, 1]$$ 
    * For easier and intuitive understanding, looking at $$\frac{(X_i - E(X_i))} { \sigma_i } \sim \mathcal{N}(0,\,1) = \mathbf{e}_i$$ and $$\frac{(X_j - E(X_j))} { \sigma_j } \sim \mathcal{N}(0,\,1) = \lambda\mathbf{e}_i + \sqrt{(1-\lambda^2)} \mathbf{e}_j$$
        * $$\mathbf{e}_i \sim \mathcal{N}(0,\,1)$$, $$\mathbf{e}_j \sim \mathcal{N}(0,\,1)$$
        * $$\mathbf{e}_i \text{ and } \mathbf{e}_j $$ are two variables in two orthogonal axises. 
        * Correlation = $$\lambda \in [-1, 1]$$  
        * Covariance = $$\lambda \sigma_i \sigma_j$$ 
        * $$
\begin{aligned}
  E\{\frac{(X_i - E(X_i))} { \sigma_i }  \frac{(X_j - E(X_j))} { \sigma_j } \} &amp;= E\{ \mathbf{e}_i \cdot \lambda\mathbf{e}_i + \sqrt{(1-\lambda^2)} \mathbf{e}_j \} \\
  &amp;= \lambda E\{ \mathbf{e}_i \cdot \mathbf{e}_i  \} + \sqrt{(1-\lambda^2)} E\{ \mathbf{e}_i \cdot \mathbf{e}_j \} \\
  &amp;= \lambda .
\end{aligned}$$
        * $$
        \begin{aligned}
  E\{ {(X_i - E(X_i))}  \cdot {(X_j - E(X_j))}  \} =  \lambda  \sigma_i  \sigma_j.
    \end{aligned}
    $$
* [Correlation and dependence -1](https://en.wikipedia.org/wiki/Correlation_and_dependence#Correlation_and_independence)
    * Correlation and dependencd are totally different concepts/terms for discribing two random variables. 
    * **In the special case** when $${\displaystyle X}$$ and $${\displaystyle Y}$$ are jointly normal, uncorrelatedness is equivalent to independence.
* [Correlation and dependence-2-Bivariate Normal Distribution](https://www.probabilitycourse.com/chapter5/5_3_2_bivariate_normal_dist.php)
    * The sum of two independent normal random variables is also normal. However, **if the two normal random variables are not independent, then their sum is not necessarily normal.**

    * If $$X$$ and $$Y$$ are **independent**, $$P(Y\|X)=P(Y) =&gt; E(XY)=E(X)E(Y) =&gt; Cov(X,Y)=E((X-E(X))(Y-E(Y)))=E(XY)-E(X)E(Y) = 0 =&gt; $$**Uncorrelated**.
    
    * If $$X$$ and $$Y$$ are uncorrelated, $$Cov(X,Y)=0 =&gt; E(XY) = E(X)E(Y) =&gt; ?$$
        * [Zero Correlation Implies Independence](http://home.iitk.ac.in/~zeeshan/pdf/The%20Bivariate%20Normal%20Distribution.pdf)
* [Why are we interested in
correlation/dependency?](https://www.actuaries.org.uk/system/files/documents/pdf/correlation.pdf)
* [Conditioning and Independence](https://www.probabilitycourse.com/chapter5/5_2_3_conditioning_independence.php)

* [Positive semi-definite]()
* [Eigen vectors and Diagonalisation]()
* [Eigen values and Determinant]()

* [Mahalanobis distance]()
    * Transformation/projection to the orthogonal space, where one axis is an independent normal distribution
    * One each axis: compute the distance $$\frac{(x-u)^T(x-u)}{\sigma^2}$$
    * Summarise all the distances of all axises. 
{:.message}

### Probabilistic view of the world
* Basic concepts/terms
    * Covariance and correlation 
    * Bivariate Normal Distribution
    * The sum of two **independent** normal distributions
    * Distance = $$num \times standard~deviation$$
        =&gt; Mahalanobis distance
    * Diagonalisation (Orthogonal, Making them independent) =&gt;  Decorrelation
        * In this context, uncorrelation = independence. 
    
* Uncorrelation (Orthogonalisaion) using Eigen vectors 
    * **Projection of a normal distribtion** $$ X \sim \mathcal{N}(0,\,{\sigma_x}^2)$$ to a standard normal distribution $$ \mathcal{N}(0,\,1)$$: 
        * **the projected distribution** is $$\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)$$
        * $$\theta$$ determines their linear dependency: correlation coefficient=$$\cos(\theta)$$. 
        
    * For each eigen vector: **we project all the original variables to this eigen vector** (an axis in the transformed orthogonal space) =&gt; summarise/accumulate those projected variables in this axis =&gt; New variable 

        * Two accumulated projected new variables of two axises (eigen vectors) are uncorrelated (being independent now).

        * Eigen value = sum of projected standard deviation.

    * Eigen vectors are orthogonal, serving as independent axises where independent variables lie in. 
        * Diagonal covariance matrix: each entry is the square of eigen value.
        * Eigen value = sum of projected standard deviation.
        * **Projection of a normal distribtion**
            * **the projected distribution** is $$\mathcal{N}(0,\,\{\cos(\theta) \times \sigma_x \times 1\}^2)$$

* [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance)
    * [Distance is not always what it seems](https://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance.html)
    * [Detecting outliers in SAS: Part 3: Multivariate location and scatter &amp; MCD: Robust estimation by subsampling](https://blogs.sas.com/content/iml/2012/02/02/detecting-outliers-in-sas-part-3-multivariate-location-and-scatter.html)
    * [Sum of normally distributed random variables-Independent random variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables)
    * [Outlier, Anomaly, and Adversaries Detection using Mahalanobis distance](https://github.com.cnpmjs.org/XinshaoAmosWang/DerivativeManipulation/blob/master/OutlierDetection_RobustInference.pptx.pdf)
    * The Mahalanobis distance is a measure of the **distance between a point $$\mathrm{P}$$ and a distribution $$\mathbf{D}$$.**
    It is a multi-dimensional generalization of the idea of measuring how many standard deviations away P is from the mean of D. This distance is zero if P is at the mean of D, and grows as P moves away from the mean along **each principal component axis.** 
        * If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. 
        * The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set. 
    * Mahalanobis distance is proportional, for a normal distribution, to the square root of the negative log likelihood (after adding a constant so the minimum is at zero).
    * This intuitive approach can be made quantitative by defining **the normalized distance between the test point and the set to be $${\displaystyle {x-\mu } \over \sigma } $$**. By plugging this into the normal distribution we can derive the probability of the test point belonging to the set.
    * Were the distribution to be decidedly non-spherical, for instance ellipsoidal, then we would expect the probability of the test point belonging to the set to depend not only on the distance from the center of mass, but also on the direction. In those directions where the ellipsoid has a short axis the test point must be closer, while in those where the axis is long the test point can be further away from the center.
    * The Mahalanobis distance is the distance of the test point from the center of mass **divided by the width of the ellipsoid in the direction of the test point.** (distance normalisation)
* [Maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)

* [Properties of the Covariance Matrix](http://www.robots.ox.ac.uk/~davidc/pubs/tt2015_dac1.pdf)

* [Differential entropy](https://en.wikipedia.org/wiki/Differential_entropy)

* [What does Determinant of Covariance Matrix give](https://math.stackexchange.com/questions/889425/what-does-determinant-of-covariance-matrix-give)

* [Why do we use the determinant of the covariance matrix when using the multivariate normal?](https://stats.stackexchange.com/questions/89952/why-do-we-use-the-determinant-of-the-covariance-matrix-when-using-the-multivaria)
{:.message}

### Optimisation
* [Concave function](https://en.wikipedia.org/wiki/Concave_function)
{:.message}</content><author><name>XW</name></author><summary type="html">SGD &amp;amp; Newton’s Method &amp;amp; Second-order Derivative Optimisation Linear algebra Probabilistic view of the world Optimisation</summary></entry><entry><title type="html">Papers Related to My PhD Thesis-Example Weighting in Classification and Distance Metric Learning</title><link href="http://localhost:4000/readingnotes/2020-02-24-ThesisRelatedPapers/" rel="alternate" type="text/html" title="Papers Related to My PhD Thesis-Example Weighting in Classification and Distance Metric Learning" /><published>2020-02-24T00:00:00+00:00</published><updated>2020-02-24T00:00:00+00:00</updated><id>http://localhost:4000/readingnotes/ThesisRelatedPapers</id><content type="html" xml:base="http://localhost:4000/readingnotes/2020-02-24-ThesisRelatedPapers/">0. [Learning to Retrieve](#distance-metric-learning-learning-to-retrieve)
0. [Learning to Classify ](#learning-to-classify)
0. [Others](#others)
{:.message}

## Distance Metric Learning: Learning to Retrieve 
* [Active Ordinal Querying for Tuplewise Similarity Learning](https://arxiv.org/pdf/1910.04115.pdf) 
* [Representation Learning with
Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748.pdf)
{:.message}



## Learning to Classify 
* 
{:.message}



## Others 
* [A general and Adaptive Robust Loss Function-CVPR 2019 Best Paper Finalist]()
* [Unsupervised Embedding Learning via Invariant and Spreading Instance Feature-CVPR 2019]()
* [Regularising Deep Neural Networks by Noise: Its Interpretation and Optimisation-NeurIPS 2017]()
* [Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks-NeurIPS 2017]()
* [Selfie: Refurbishing Unclean Samples for Robust Deep Learning-ICML 2019]()
* [Unsupervised Label Noise Modeling and Loss Correction-ICML 2019]()
* [Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise-NeurIPS 2018]()

## Example Weighting
* [Weighted Machine Learning-Mahdi Hashemi∗, Hassan A. Karimi](https://www.researchgate.net/publication/328731166_Weighted_Machine_Learning)
* [Not All Samples Are Created Equal: Deep Learning with Importance Sampling-Angelos Katharopoulos, Franc¸ois Fleuret, ICML 2018](http://proceedings.mlr.press/v80/katharopoulos18a.html)
    * computing the importance score for the whole
dataset is still prohibitive and would render the method
unsuitable for online learning.
    * In order to solve the problem of computing the importance for the whole dataset, we pre-sample a large batch of data points, compute the sampling distribution for that batch and re-sample a smaller batch with replacement. 
{:.message}</content><author><name>XW</name></author><summary type="html">Learning to Retrieve Learning to Classify Others</summary></entry><entry><title type="html">Learning Bayesian Deep Learning, Uncertainty &amp;amp; Variational Techniques</title><link href="http://localhost:4000/blogs/2020-02-21-learn-bayesian-DL/" rel="alternate" type="text/html" title="Learning Bayesian Deep Learning, Uncertainty &amp; Variational Techniques" /><published>2020-02-21T00:00:00+00:00</published><updated>2020-02-21T00:00:00+00:00</updated><id>http://localhost:4000/blogs/learn-bayesian-DL</id><content type="html" xml:base="http://localhost:4000/blogs/2020-02-21-learn-bayesian-DL/">0. [Blogs](#blogs)
0. [Papers on Theories](#papers-on-theories)
0. [Papers on Applications](#papers-on-applications)
{:.message}


### What am I working on now? Discussions are Welcome!
* [Interpreting $$ p(y\|x) $$ and modelling example weighting](../2020-02-18-code-releasing)

* Going to stop treating $$ p(y\|x) $$ as a classfication confidence metric, since it is determinstic. $$ p(y\|x) $$  is not for deciding whether certain or uncertain.


* $$ p(y\|x) $$ is good as a metric of whether x matches y, though not a good metric indicating whether x is blur or not.  

* Utilities of Uncertainties
{:.message}


### Blogs
* [Everything that Works Works Because it's Bayesian: Why Deep Nets Generalize?](https://www.inference.vc/everything-that-works-works-because-its-bayesian-2/)
* [Yann LeCun's Comments](https://www.facebook.com/yann.lecun/posts/10154058859142143)
* [YARIN GAL's PhD Thesis](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html?fbclid=IwAR1lNokscvPVsGFICXDQBhVa2bweIq-mkft6EfUkj9CR8tAIYJ7mNy3Qag8)
{:.message}


### Papers on Theories
* [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning-ICML 2016-YARIN GAL](https://arxiv.org/pdf/1506.02142.pdf)
* [YARIN GAL's PhD Thesis](http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf)
* [A Bayesian Perspective on Generalization and Stochastic Gradient Descent-ICLR 2018 Google Brain-Samuel L. Smith and Quoc V. Le](https://openreview.net/forum?id=BJij4yg0Z)
* [Bayesian Deep Learning and a Probabilistic Perspective of Generalization--arXiv 2020 New York University-Andrew Gordon Wilson Pavel Izmailov](https://arxiv.org/pdf/2002.08791.pdf)
* [Sharp Minima Can Generalize For Deep Nets-ICML 2017](https://arxiv.org/pdf/1703.04933.pdf)
* [Theory of Deep Learning III: Generalization Properties of SGD](https://cbmm.mit.edu/sites/default/files/publications/CBMM-Memo-067.pdf)
* [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima-ICLR 2017](https://openreview.net/forum?id=H1oyRlYgg)
* [The Marginal Value of Adaptive Gradient Methods in Machine Learning-NIPS 2017](https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning)
* [Stochastic Gradient Descent as Approximate Bayesian Inference-JMLR 2017](http://www.jmlr.org/papers/volume18/17-214/17-214.pdf)
* [A Variational Analysis of Stochastic Gradient Algorithms-ICML 2016](http://proceedings.mlr.press/v48/mandt16.pdf)

* [Deep Learning and the Information Bottleneck Principle](https://arxiv.org/pdf/1503.02406.pdf)
* [On the Difference Between the Information Bottleneck and the Deep Information Bottleneck](https://arxiv.org/pdf/1912.13480.pdf)
* [Mutual Information Neural Estimation](http://proceedings.mlr.press/v80/belghazi18a/belghazi18a.pdf)
{:.message}

### Papers on Applications

* [Robust Person Re-Identification by Modelling Feature Uncertainty](http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Robust_Person_Re-Identification_by_Modelling_Feature_Uncertainty_ICCV_2019_paper.pdf)
* [Probabilistic Face Embeddings](https://arxiv.org/pdf/1904.09658.pdf)
* [Rethinking Person Re-Identification with Confidence](https://arxiv.org/pdf/1906.04692v1.pdf)
* [Learning Confidence for Out-of-Distribution Detection in Neural Networks](https://arxiv.org/pdf/1802.04865.pdf)
* [Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples](https://openreview.net/forum?id=ryiAv2xAZ)
{:.message}</content><author><name>XW</name></author><summary type="html">Blogs Papers on Theories Papers on Applications</summary></entry></feed>