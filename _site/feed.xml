<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2020-02-16T22:50:32+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Xinshao Wang</title><subtitle>&quot;Welcome to my personal website&quot;. **3rd Year PhD Student**, will graduate in Sep 2020.
</subtitle><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><entry><title type="html">arXiv-2020</title><link href="http://localhost:4000/paperlists/2020-02-16-arXiv/" rel="alternate" type="text/html" title="arXiv-2020" /><published>2020-02-16T00:00:00+00:00</published><updated>2020-02-16T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/arXiv</id><content type="html" xml:base="http://localhost:4000/paperlists/2020-02-16-arXiv/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;foundation-of-deep-learning&quot;&gt;Foundation of Deep Learning&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.05709.pdf&quot;&gt;A Simple Framework for Contrastive Learning of Visual Representations&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Data augmentation: composition of data augmentations plays a critical role in defining effective predictive tasks;&lt;/li&gt;
      &lt;li&gt;Auxiliary transformation:  introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations;&lt;/li&gt;
      &lt;li&gt;Larger batch size and more training steps: contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning.&lt;/li&gt;
      &lt;li&gt;Results: By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;message&quot;&gt;Note that the proposed loss expression &lt;strong&gt;NT-Xent (the normalized
temperature-scaled cross entropy loss)&lt;/strong&gt; has been proposed in &lt;a href=&quot;https://openreview.net/pdf?id=BJeguTEKDB&quot;&gt;Instance Cross Entropy for Deep Metric Learning–ICLR2020 Submission Version&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/1911.09976.pdf&quot;&gt;arXiv Version&lt;/a&gt;: cross entropy computation and dot product scaling.&lt;/p&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">Paper notes on Anomalies&amp;amp;Regularisation</title><link href="http://localhost:4000/blogs/2020-02-15-anomalies-regularisation/" rel="alternate" type="text/html" title="Paper notes on Anomalies&amp;Regularisation" /><published>2020-02-15T00:00:00+00:00</published><updated>2020-02-15T00:00:00+00:00</updated><id>http://localhost:4000/blogs/anomalies-regularisation</id><content type="html" xml:base="http://localhost:4000/blogs/2020-02-15-anomalies-regularisation/">&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;/Papers/AnomalyAndRegularisation/&quot;&gt;Paper notes on Anomalies&amp;amp;Regularisation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">Paper notes on Anomalies&amp;amp;Regularisation</summary></entry><entry><title type="html">Notes on Core ML Topics</title><link href="http://localhost:4000/blogs/2020-01-02-Core-machine-learning-topics/" rel="alternate" type="text/html" title="Notes on Core ML Topics" /><published>2020-01-02T00:00:00+00:00</published><updated>2020-01-02T00:00:00+00:00</updated><id>http://localhost:4000/blogs/Core-machine-learning-topics</id><content type="html" xml:base="http://localhost:4000/blogs/2020-01-02-Core-machine-learning-topics/">&lt;ol class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#kullback-leibler-divergence&quot;&gt;Kullback-Leibler Divergence&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-the-main-difference-between-gan-and-autoencoder&quot;&gt;What is the main difference between GAN and autoencoder&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#whats-the-difference-between-a-variational-autoencoder-vae-and-an-autoencoder&quot;&gt;What’s the difference between a Variational Autoencoder (VAE) and an Autoencoder?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;kullback-leibler-divergence&quot;&gt;Kullback-Leibler Divergence&lt;/h4&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained&quot;&gt;How to approximate our data (choose a parameterized distribution =&amp;gt; optimise its parameters): KL Divergence helps us to measure just how much information we lose when we choose an approximation compared with our observations.&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;The most important metric in information theory is called &lt;strong&gt;Entropy&lt;/strong&gt;, typically denoted as $\mathbf{H}$. The definition of Entropy for a probability distribution is: $\mathbf{H}=-\sum_{i=1}^{n} p(\mathbf{x}_i) \log p(\mathbf{x}_i) $.&lt;/li&gt;
      &lt;li&gt;If we use $\log_2$ for our calculation we can interpret entropy as “the minimum number of bits it would take us to encode our information”.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8&quot;&gt;Intuitive Guide to Understanding KL Divergence&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;What is a distributin?&lt;/li&gt;
      &lt;li&gt;What is an event?&lt;/li&gt;
      &lt;li&gt;Problem we’re trying to solve: choose a parameterized distribution =&amp;gt; optimise its parameters): KL Divergence helps us to measure just how much information we lose when we choose an approximation compared with our observations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;what-is-the-main-difference-between-gan-and-autoencoder&quot;&gt;What is the main difference between GAN and autoencoder?&lt;/h4&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://datascience.stackexchange.com/a/55094&quot;&gt;An autoencoder learns to represent some input information very efficiently, and subsequently how to reconstruct the input from it’s compressed form.&lt;/a&gt;
  ~ :) ~&lt;a href=&quot;https://qr.ae/TzM5Mv&quot;&gt;An autoencoder compresses its input down to a vector - with much fewer dimensions than its input data, and then transforms it back into a tensor with the same shape as its input over several neural net layers. They’re trained to reproduce their input, so it’s kind of like learning a compression algorithm for that specific dataset.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://datascience.stackexchange.com/a/55094&quot;&gt;A GAN uses an adversarial feedback loop to learn how to generate some information that “seems real” (i.e. looks the same/sounds the same/is otherwise indistinguishable from some real data)&lt;/a&gt; ~ :) ~ &lt;a href=&quot;https://qr.ae/TzM5Mv&quot;&gt;Instead of being given a bit of data as input, it’s given a small vector of random numbers. The generator network tries to transform this little vector into a realistic sample from the training data. The discriminator network then takes this generated sample(and some real samples from the dataset) and learns to guess whether the samples are real or fake.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://qr.ae/TzM5Mv&quot;&gt;Another difference: while they both fall under the umbrella of unsupervised learning, they are different approaches to the problem. A GAN is a generative model - it’s supposed to learn to generate realistic &lt;em&gt;new&lt;/em&gt; samples of a dataset. Variational autoencoders are generative models, but normal “vanilla” autoencoders just reconstruct their inputs and can’t generate realistic new samples.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://qr.ae/TzMSyS&quot;&gt;Autoencoders learn a given distribution comparing its input to its output, this is good for learning hidden representations of data, but is pretty bad for generating new data. Mainly because we learn an averaged representation of the data thus the output becomes pretty blurry.
  Generative Adversarial Networks take an entirely different approach. They use another network (so-called Discriminator) to measure the distance between the generated and the real data.
  The main advantage of GANs over Autoencoders in generating data is that they can be conditioned by different inputs. For example, you can learn the mapping between two domains: satellite images to google maps [1] . Or you can teach the generator to reproduce several classes of data: generating the MNIST dataset[2] .
  &lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.keras.io/building-autoencoders-in-keras.html&quot;&gt; Building Autoencoders in Keras&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/gans-vs-autoencoders-comparison-of-deep-generative-models-985cf15936ea&quot;&gt;Coding: GANs vs. Autoencoders: Comparison of Deep Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;whats-the-difference-between-a-variational-autoencoder-vae-and-an-autoencoder&quot;&gt;What’s the difference between a Variational Autoencoder (VAE) and an Autoencoder?&lt;/h4&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf&quot;&gt;Intuitively Understanding Variational Autoencoders – Towards Data Science by Irhum Shafkat.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span class=&quot;quora-content-embed&quot; data-name=&quot;Whats-the-difference-between-a-Variational-Autoencoder-VAE-and-an-Autoencoder/answer/Vishal-Sharma-154&quot;&gt;Read &lt;a class=&quot;quora-content-link&quot; data-width=&quot;560&quot; data-height=&quot;260&quot; href=&quot;https://www.quora.com/Whats-the-difference-between-a-Variational-Autoencoder-VAE-and-an-Autoencoder/answer/Vishal-Sharma-154&quot; data-type=&quot;answer&quot; data-id=&quot;66853410&quot; data-key=&quot;a5099035f08fbac1ed45a4bb7a1c5d2c&quot; load-full-answer=&quot;False&quot; data-embed=&quot;trhonms&quot;&gt;&lt;a href=&quot;https://www.quora.com/Vishal-Sharma-154&quot;&gt;Vishal Sharma&lt;/a&gt;'s &lt;a href=&quot;/Whats-the-difference-between-a-Variational-Autoencoder-VAE-and-an-Autoencoder?top_ans=66853410&quot;&gt;answer&lt;/a&gt; to &lt;a href=&quot;/Whats-the-difference-between-a-Variational-Autoencoder-VAE-and-an-Autoencoder&quot; ref=&quot;canonical&quot;&gt;&lt;span class=&quot;rendered_qtext&quot;&gt;What's the difference between a Variational Autoencoder (VAE) and an Autoencoder?&lt;/span&gt;&lt;/a&gt;&lt;/a&gt; on &lt;a href=&quot;https://www.quora.com&quot;&gt;Quora&lt;/a&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;https://www.quora.com/widgets/content&quot;&gt;&lt;/script&gt;&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.keras.io/building-autoencoders-in-keras.html&quot;&gt; Building Autoencoders in Keras&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://distill.pub/2017/aia/&quot;&gt;Using Artificial Intelligence to Augment Human Intelligence&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://efrosgans.eecs.berkeley.edu/CVPR18_slides/VAE_GANS_by_Rosca.pdf&quot;&gt;VAEs and GANs
Mihaela Rosca&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://syncedreview.com/2019/06/06/going-beyond-gan-new-deepmind-vae-model-generates-high-fidelity-human-faces/&quot;&gt;Going Beyond GAN? New DeepMind VAE Model Generates High Fidelity Human Faces&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">Kullback-Leibler Divergence What is the main difference between GAN and autoencoder What’s the difference between a Variational Autoencoder (VAE) and an Autoencoder?</summary></entry><entry><title type="html">ICLR-2020</title><link href="http://localhost:4000/paperlists/2020-01-02-ICLR/" rel="alternate" type="text/html" title="ICLR-2020" /><published>2020-01-02T00:00:00+00:00</published><updated>2020-01-02T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/ICLR</id><content type="html" xml:base="http://localhost:4000/paperlists/2020-01-02-ICLR/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;foundation-of-deep-learning&quot;&gt;Foundation of Deep Learning&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=B1g8VkHFPH&quot;&gt;Rethinking the Hyperparameters for Fine-tuning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=BJgnXpVYwS&quot;&gt;WHY GRADIENT CLIPPING ACCELERATES TRAINING:
A THEORETICAL JUSTIFICATION FOR ADAPTIVITY&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">ICLR-2019</title><link href="http://localhost:4000/paperlists/2019-12-30-ICLR/" rel="alternate" type="text/html" title="ICLR-2019" /><published>2019-12-30T00:00:00+00:00</published><updated>2019-12-30T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/ICLR</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-30-ICLR/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;corefundamental-deep-learning-informativeforgettable-example-uncertain-examples&quot;&gt;Core/Fundamental Deep Learning (Informative/Forgettable Example, Uncertain Examples)&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=BJlxm30cKm&quot;&gt;Informative/Forgettable Examples: An Empirical Study of Example Forgetting during Deep Neural Network Learning&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;We define a ``forgetting event’’ to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning.
  We show that catastrophic forgetting occurs within what is considered to be a single task and find that examples that are not prone to forgetting can be removed from the training set without loss of generalization.&lt;/li&gt;
      &lt;li&gt;Across several benchmark data sets, we find that:
        &lt;ul&gt;
          &lt;li&gt;(i) certain examples are forgotten with high frequency, and some not at all;&lt;/li&gt;
          &lt;li&gt;(ii) a data set’s (un)forgettable examples generalize across neural architectures;&lt;/li&gt;
          &lt;li&gt;(iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Their finding: Harder/Support/informative samples are learned later and may include noisy examples. &lt;br /&gt;
  Then a question is arouse: How to differentiate informative and noisy examples? 
  Our work provides a solution for this question. &lt;a href=&quot;https://arxiv.org/pdf/1905.11233.pdf&quot;&gt;Derivative Manipulation for General Example Weighting&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Detailed findings of theirs:
        &lt;ul&gt;
          &lt;li&gt;Support examples = forgettable examples = Informative examples that cannot be removed. Removing unforgottable examples do not hurt the generalisation performance when training a model from scratch on the remained subset.&lt;/li&gt;
          &lt;li&gt;The properties of support/informative examples: a) learnt later; b) larget misclassification margin when forgotten; c) perceptually ambiguous; d) tend to be noisy.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/6701-active-bias-training-more-accurate-neural-networks-by-emphasizing-high-variance-samples.pdf&quot;&gt;Uncertain Examples-NeurIPS2017: Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;How to define uncertain examples: predicitons in the history.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">CVPR-2019</title><link href="http://localhost:4000/paperlists/2019-12-29-CVPR/" rel="alternate" type="text/html" title="CVPR-2019" /><published>2019-12-29T00:00:00+00:00</published><updated>2019-12-29T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/CVPR</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-29-CVPR/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;deep-metric-learning&quot;&gt;Deep Metric Learning&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Sanakoyeu_Divide_and_Conquer_the_Embedding_Space_for_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Divide and Conquer the Embedding Space for Metric Learning&lt;/a&gt; 
:+1:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Each learner will learn a separate distance metric using only a subspace of the original embedding space and &lt;strong&gt;a part of the data&lt;/strong&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Natural hard negatives mining: Finally, &lt;strong&gt;the splitting and sampling connect to hard negative mining&lt;/strong&gt;, which is verified by them. (I appreciate this ablation study in Table 6 )&lt;/li&gt;
      &lt;li&gt;Divide means: 1) Splitting the training data into K Clusters; 
  2) Splitting the embedding into K Slices.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Cakir_Deep_Metric_Learning_to_Rank_CVPR_2019_paper.pdf&quot;&gt;Deep Metric Learning to Rank&lt;/a&gt; :+1:
    &lt;ul&gt;
      &lt;li&gt;Our main contribution is a novel solution to optimizing Average Precision under the Euclidean metric, based on the probabilistic interpretation of AP as the area under precision-recall curve, as well as distance quantization.&lt;/li&gt;
      &lt;li&gt;We also propose a category-based minibatch sampling strategy and a large-batch training heuristic.&lt;/li&gt;
      &lt;li&gt;On three &lt;strong&gt;few-shot image retrieval datasets&lt;/strong&gt;, FastAP consistently outperforms competing methods, which often involve complex optimization heuristics or costly model ensembles.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Multi-Similarity Loss With General Pair Weighting for Deep Metric Learning&lt;/a&gt; :+1:
    &lt;ul&gt;
      &lt;li&gt;Objective of the proposed multi-similarity loss, which aims to collect informative pairs, and weight these pairs through their own and relative similarities.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1903.03238.pdf&quot;&gt;Ranked List Loss for Deep Metric Learning&lt;/a&gt; :+1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Suh_Stochastic_Class-Based_Hard_Example_Mining_for_Deep_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Stochastic Class-Based Hard Example Mining for Deep Metric Learning&lt;/a&gt; :+1:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Scale linearly to the number of classes.&lt;/li&gt;
      &lt;li&gt;The methods proposed by Movshovitz-Attias et al. [14] and Wen et al. [34] are related to ours in a sense that class representatives are jointly trained with the feature extractor. 
However, their goal is to formulate new losses using the class representatives whereas we use them for hard negative mining.&lt;/li&gt;
      &lt;li&gt;Given an anchor instance, our algorithm first selects a few hard negative classes based on the class-to-sample distances and then performs a refined search in an instance-level only from the selected classes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Theoretically Sound Upper Bound on the Triplet Loss for Improving the Efficiency of Deep Distance Metric Learning :+1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unsupervised&lt;/strong&gt; Embedding Learning via Invariant and Spreading Instance Feature :+1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Yuan_Signal-To-Noise_Ratio_A_Robust_Distance_Metric_for_Deep_Metric_Learning_CVPR_2019_paper.pdf&quot;&gt;Signal-To-Noise Ratio: A Robust Distance Metric for Deep Metric Learning&lt;/a&gt; :+1:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;We propose a robust SNR distance metric based on Signal-to-Noise Ratio (SNR) for measuring the similarity of image pairs for deep metric learning. Compared with Euclidean distance metric, our SNR distance metric can further jointly reduce the intra-class distances and enlarge the inter-class distances for learned features.&lt;/li&gt;
      &lt;li&gt;SNR in signal processing is used to measure the level of a desired signal to the level of noise, and a larger SNR value means a higher signal quality.
  For similarity measurement in deep metric learning, a pair of learned features x and y can be given as y = x + n, where n can be treated as a noise. Then, the SNR is the ratio of the feature variance and the noise variance.&lt;/li&gt;
      &lt;li&gt;To show the generality of our SNR-based metric, we also extend our approach to hashing retrieval learning.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Branchaud-Charron_Spectral_Metric_for_Dataset_Complexity_Assessment_CVPR_2019_paper.pdf&quot;&gt;Spectral Metric for Dataset Complexity Assessment&lt;/a&gt; :+1:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Related work: &lt;a href=&quot;https://openreview.net/forum?id=ryup8-WCW&quot;&gt;Measuring the Intrinsic Dimension of Objective Landscapes ICLR 2018&lt;/a&gt;, 
  &lt;a href=&quot;https://arxiv.org/abs/1808.03591&quot;&gt;How Complex is your classification problem? A survey on measuring classification complexity Survey on complexity measures&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Deep Asymmetric Metric Learning via Rich Relationship Mining :+1:
    &lt;ul&gt;
      &lt;li&gt;DAMLRRM relaxes the constraint on positive pairs to extend the generalization capability. We build positive pairs training pool by constructing a minimum connected tree for each category instead of considering all positive pairs within a mini-batch. As a result, there will exist a direct or indirect path between any positive pair, which ensures the relevance being bridged to each other. The inspiration comes from ranking on manifold [58] that spreads the relevance to their nearby neighbors one by one.&lt;/li&gt;
      &lt;li&gt;Idea is novel. The results on SOP are not good, only 69.7 with GoogLeNet&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Hybrid-Attention_Based_Decoupled_Metric_Learning_for_Zero-Shot_Image_Retrieval_CVPR_2019_paper.pdf&quot;&gt;Hybrid-Attention Based Decoupled Metric Learning for Zero-Shot Image Retrieval&lt;/a&gt; :-1:
    &lt;ul&gt;
      &lt;li&gt;Very complex: object attention, spatial attention, random walk graph, etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.09626.pdf&quot;&gt;Deep Metric Learning Beyond Binary Supervision&lt;/a&gt; :-1:
    &lt;ul&gt;
      &lt;li&gt;Binary supervision indicating whether a pair of images are of the same class or not.&lt;/li&gt;
      &lt;li&gt;Using continuous labels&lt;/li&gt;
      &lt;li&gt;Learn the degree of similarity rather than just the order.&lt;/li&gt;
      &lt;li&gt;A triplet mining strategy adapted to metric learning with continuous labels.&lt;/li&gt;
      &lt;li&gt;Image retrieval tasks with continuous labels in terms of human poses, room layouts and image captions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hardness-aware deep metric learning 
:-1: : data augmentation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensemble Deep Manifold Similarity Learning using Hard Proxies :-1: random walk algorithm, ensemble models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Re-Ranking via Metric Fusion for Object Retrieval and Person Re-Identification :-1:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Deep Embedding Learning With Discriminative Sampling Policy :-1:&lt;/li&gt;
  &lt;li&gt;Point Cloud Oversegmentation With Graph-Structured Deep Metric Learning :-1:&lt;/li&gt;
  &lt;li&gt;Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval :-1:&lt;/li&gt;
  &lt;li&gt;A Compact Embedding for Facial Expression Similarity :-1:&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Karlinsky_RepMet_Representative-Based_Metric_Learning_for_Classification_and_Few-Shot_Object_Detection_CVPR_2019_paper.pdf&quot;&gt;RepMet: Representative-Based Metric Learning for Classification and Few-Shot Object Detection&lt;/a&gt; :-1:&lt;/li&gt;
  &lt;li&gt;Eliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking :-1:&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;robustness&quot;&gt;Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Barron_A_General_and_Adaptive_Robust_Loss_Function_CVPR_2019_paper.pdf&quot;&gt;A General and Adaptive Robust Loss Function&lt;/a&gt; :+1:&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1812.05214.pdf&quot;&gt;Learning to Learn from Noisy Labeled Data&lt;/a&gt; :+1:
This work achieves promising results with meta-learning. Our result on Clothing 1M is comparable with theirs. However, their modelling via meta-learning seems extremely complex in practice.
&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/bws5iv/r_cvpr_2019_noisetolerant_training_work_learning/&quot;&gt;https://www.reddit.com/r/MachineLearning/comments/bws5iv/r_cvpr_2019_noisetolerant_training_work_learning/&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Too many hyper-parameters shown in their Algorithm 1 and implementation section 4.2.&lt;/li&gt;
      &lt;li&gt;The strategies of iterative training together with iterative data filtering/cleaning, reusing last-round best model as mentor, etc., make it difficult to handle in practice.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/LiJunnan1992/MLNT/issues/1&quot;&gt;https://github.com/LiJunnan1992/MLNT/issues/1&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Yi_Probabilistic_End-To-End_Noise_Correction_for_Learning_With_Noisy_Labels_CVPR_2019_paper.pdf&quot;&gt;Probabilistic End-to-end Noise Correction for Learning with Noisy Labels&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Questions on “Probabilistic End-to-end Noise Correction for Learning with Noisy Labels, CVPR 2019”. Discussion and sharing are appreciated.&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Question 1: There is a softmax transformation between two label vectors and a gradient flow path between them. However, according to my understanding, this path is not necessary. &lt;strong&gt;The target is to learn true labels y^d, which can be initialised by observed labels directly.&lt;/strong&gt; Therefore, the true label distributions should be the end of the graph, it does not make sense to back-propagate to another label vector version.&lt;/li&gt;
          &lt;li&gt;Question 2: &lt;strong&gt;If the answer of Question1 is yes&lt;/strong&gt;, then learning the true labels for minimising the loss should be exactly the same as ‘Joint Optimisation Framework for Learning with Noisy Labels’, i.e., Alternative Optimisation.  The fact is that if we set the true labels as the network’s predictions, the loss becomes zero naturally. &lt;strong&gt;Therefore, gradient back-propagation is unnecessary for estimating the true labels.&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;Question 3: The compatibility loss penalises distant true labels versus observed labels. I have no idea why it works when noise rate is high in the experiments?  Is it meaningful to penalise distant true labels when noise rate is very high?&lt;/li&gt;
          &lt;li&gt;Question 4: The model is trained by 3 stages:
            &lt;ul&gt;
              &lt;li&gt;1) Backbone learning without noise handling (only cross entropy loss);&lt;/li&gt;
              &lt;li&gt;2) pencil learning with 3 losses jointly (one classification loss + two regularisation terms);&lt;/li&gt;
              &lt;li&gt;3) fine-tuning with only classification loss (regularisation terms are removed).&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Is anybody interested in seeing the result of each stage training? By which we can know exactly how much improvement comes from each step.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">ICCV-2019</title><link href="http://localhost:4000/paperlists/2019-12-29-ICCV/" rel="alternate" type="text/html" title="ICCV-2019" /><published>2019-12-29T00:00:00+00:00</published><updated>2019-12-29T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/ICCV</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-29-ICCV/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;noisy-labels-&quot;&gt;Noisy Labels, …&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Deep_Self-Learning_From_Noisy_Labels_ICCV_2019_paper.pdf&quot;&gt;Deep Self-Learning From Noisy Labels&lt;/a&gt;:  Self means `without extra supervision’
    &lt;ul&gt;
      &lt;li&gt;The proposed SMP trains in an iterative manner which
contains two phases: the first phase is to train a network
with &lt;strong&gt;the original noisy label and corrected label&lt;/strong&gt; generated
in the second phase.&lt;/li&gt;
      &lt;li&gt;By extracting multiple prototypes for a category, we demonstrate that more prototypes would get a better representation of a class and obtain better label-correction results.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Co-Mining_Deep_Face_Recognition_With_Noisy_Labels_ICCV_2019_paper.pdf&quot;&gt;Co-Mining: Deep Face Recognition With Noisy Labels&lt;/a&gt;: We propose a novel &lt;strong&gt;co-mining&lt;/strong&gt; framework, which employs two peer networks to &lt;strong&gt;detect the noisy faces,
exchanges the high-confidence clean faces and reweights the clean faces&lt;/strong&gt; in a mini-batch fashion.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_NLNL_Negative_Learning_for_Noisy_Labels_ICCV_2019_paper.pdf&quot;&gt;NLNL: Negative Learning for Noisy Labels&lt;/a&gt;: Input image belongs to this label–Positive Learning; Negative Learning (NL)–CNNs are trained using a complementary label as in “input image does not belong to this complementary label.”&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Symmetric_Cross_Entropy_for_Robust_Learning_With_Noisy_Labels_ICCV_2019_paper.pdf&quot;&gt;Symmetric Cross Entropy for Robust Learning With Noisy Labels&lt;/a&gt;: Already compared in our method.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_O2U-Net_A_Simple_Noisy_Label_Detection_Approach_for_Deep_Neural_ICCV_2019_paper.pdf&quot;&gt;O2U-Net: A Simple Noisy Label Detection Approach for Deep Neural Networks&lt;/a&gt;–&lt;strong&gt;Overall, this method is complex&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;We propose a novel noisy label detection approach, named O2U-net, without human annotation and verification.
  It only requires adjusting the hyper-parameters of the deep network to &lt;strong&gt;make its status transfer from overfitting to underfitting (O2U) cyclically&lt;/strong&gt;. 
  &lt;strong&gt;By calculating and ranking the normalized average loss of every sample, the mislabeled samples can be identified.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;The losses of each sample are recorded during iterations.&lt;/strong&gt; The higher the normalized average loss of a sample, the higher the probability of being noisy labels.
  =&amp;gt; &lt;strong&gt;Is it scalable to large datasets?&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;O2U-net is naturally compatible with active learning and other human annotation approaches. This introduces extra flexibility for learning with noisy labels.&lt;/li&gt;
      &lt;li&gt;The whole training process:
        &lt;ul&gt;
          &lt;li&gt;Pre-training:  Train the network directly on the original dataset including noisy labels. At this step, a common constant learning rate is applied. &lt;strong&gt;A large batch size&lt;/strong&gt; is applied to reduce the impact of label noise &lt;a href=&quot;https://arxiv.org/abs/1705.10694&quot;&gt;Deep Learning is Robust to Massive Label Noise&lt;/a&gt;.  We use &lt;strong&gt;a validation set to monitor the performance of training.&lt;/strong&gt; The network is trained until the accuracy in the validation set stays stable. (&lt;strong&gt;Validation Data is Needed!&lt;/strong&gt;)&lt;/li&gt;
          &lt;li&gt;Cyclical Training: &lt;strong&gt;A small batch size&lt;/strong&gt;–A smaller batch size is chosen to make the network more easily transfer from overfitting to underfitting.
  After the whole cyclical training, the average of the
  normalized losses of every sample is computed. All
  the average losses are then ranked in descending order.
  The top k% of samples are removed from the original
  dataset as noisy labels, where k depends on the prior
  knowledge on the dataset. Such prior knowledge can
  be obtained by manually verifying a small group of
  randomly selected samples.&lt;/li&gt;
          &lt;li&gt;Training on Clean Data: Lastly, we &lt;strong&gt;re-initialize&lt;/strong&gt; the
  parameters of the network, and &lt;strong&gt;re-train&lt;/strong&gt; it on the cleansing dataset ordinarily until achieving stable accuracy
  and loss in the validation set.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;robustness&quot;&gt;Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Peterson_Human_Uncertainty_Makes_Classification_More_Robust_ICCV_2019_paper.pdf&quot;&gt;Human uncertainty makes classification more robust–From Labels to Label Distributions&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;We suggest &lt;strong&gt;an alternative objective: not
just trying to capture the most likely label, but trying to capture the full distribution over labels.&lt;/strong&gt; Although there has been much work scaling the number of images in datasets [18], and investigating label noise
[40, 12, 48], little effort has been put into identifying the
benefits from increasing the richness of (informative) label
distributions for image classification tasks.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Soft Labels&lt;/strong&gt;: One of the core contributions of our work is around using the soft labels provided through human confusion as a replacement for one-hot label encodings.&lt;/li&gt;
      &lt;li&gt;Our approach proposes &lt;strong&gt;utilizing these human disagreements&lt;/strong&gt; to improve the accuracy and robustness of a model, complementing existing work aimed at leveraging “errors” in human labeling&lt;/li&gt;
      &lt;li&gt;As accuracy gains have begun to asymptote at
near-perfect levels [11], there has been &lt;strong&gt;increasing focus on
out-of-training-set performance—in particular, the ability
to generalize to related stimuli [39], and robustness to adversarial examples [29]&lt;/strong&gt;. On these tasks, by contrast, CNNs
tend to perform rather poorly, whereas humans continue to
perform well.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Yamaguchi_Subspace_Structure-Aware_Spectral_Clustering_for_Robust_Subspace_Clustering_ICCV_2019_paper.pdf&quot;&gt;Subspace Structure-aware Spectral Clustering for Robust Subspace Clustering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adversarial-robustness&quot;&gt;Adversarial Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Gowal_Scalable_Verified_Training_for_Provably_Robust_Image_Classification_ICCV_2019_paper.pdf&quot;&gt;Scalable Verified Training for Provably Robust Image Classification&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Train deep neural networks that are provably robust to norm-bounded adversarial perturbations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Improving_Adversarial_Robustness_via_Guided_Complement_Entropy_ICCV_2019_paper.pdf&quot;&gt;Improving Adversarial Robustness via Guided Complement Entropy&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Bilateral_Adversarial_Training_Towards_Fast_Training_of_More_Robust_Models_ICCV_2019_paper.pdf&quot;&gt;Bilateral Adversarial Training: Towards Fast Training of More Robust Models
Against Adversarial Attacks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-metric-learning-&quot;&gt;Deep Metric Learning, …&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">ICML-2019</title><link href="http://localhost:4000/paperlists/2019-12-29-ICML/" rel="alternate" type="text/html" title="ICML-2019" /><published>2019-12-29T00:00:00+00:00</published><updated>2019-12-29T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/ICML</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-29-ICML/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;label-noise&quot;&gt;Label Noise&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v97/arazo19a/arazo19a.pdf&quot;&gt;Unsupervised Label Noise Modeling and Loss Correction&lt;/a&gt; :+1:
    &lt;ul&gt;
      &lt;li&gt;A suitable two-component mixture model as an unsupervised generative model
of sample loss values during training to allow
online estimation of the probability that a sample is mislabelled.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.05040.pdf&quot;&gt;Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels&lt;/a&gt;-&lt;strong&gt;I am skeptical of their conclusion.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;We find that &lt;strong&gt;the test accuracy can be
quantitatively characterized in terms of the noise
ratio in datasets&lt;/strong&gt;. &lt;strong&gt;The test accuracy
is a quadratic function of the noise ratio in the
case of symmetric noise&lt;/strong&gt;, which explains the experimental findings previously published. (&lt;strong&gt;I am not convinced on this!&lt;/strong&gt;)&lt;/li&gt;
      &lt;li&gt;DNNs tend to learn simple patterns
first, then gradually memorize all samples, which justifies
the widely used small-loss criteria: treating samples with
small training loss as clean ones (Han et al., 2018; Jiang
et al., 2018).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v97/song19b/song19b.pdf&quot;&gt;SELFIE: Refurbishing Unclean Samples for Robust Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;robustness&quot;&gt;Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf&quot;&gt;Certified Adversarial Robustness via Randomized Smoothing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;importance-weighting&quot;&gt;Importance Weighting?&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v97/byrd19a/byrd19a.pdf&quot;&gt;What is the Effect of Importance Weighting in Deep Learning?&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Across tasks, architectures and datasets, our results confirm that for standard neural networks, &lt;strong&gt;weighting has a significant effect early in training.&lt;/strong&gt;
  However, &lt;strong&gt;as training progresses the effect dissipates&lt;/strong&gt; and for most weight ratios considered (between 256:1 and 1:256) the effect of importance weighting is indistinguishable from unweighted risk minimization after sufficient training epochs.&lt;/li&gt;
      &lt;li&gt;While L2 regularization restores some of the impact of importance weighting, this has the perplexing consequence of expressing the amount by which importance weights affect the learned model in terms of a seemingly unrelated quantity—the degree of regularization—prompting the question: how does one appropriately choose the L2 regularization given importance weights? Interestingly, dropout regularization, which is often used interchangeably with L2 regularization, does not exhibit any such interaction with importance weighting. Batch normalization also appears to interact with importance weights, although as we will discuss later, the precise mechanism remains unclear.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Related Papers from other conferences
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=BkeStsCcKQ&quot;&gt;ICLR2019-CRITICAL LEARNING PERIODS IN DEEP NETWORKS&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;Our findings, described in Section 2, indicate that the early transient is critical in determining the final solution of the optimization associated with training an artificial neural network.&lt;/li&gt;
          &lt;li&gt;To study this early phase, in Section 3, we use the Fisher Information to quantify the effective connectivity of a network during training, and introduce the notion of Information Plasticity in learning. Information Plasticity is maximal during the memorization phase, and decreases in the reorganization phase. We show that deficit sensitivity during critical periods correlates strongly with the effective connectivity.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v80/katharopoulos18a.html&quot;&gt;ICML2018-Not All Samples Are Created Equal: Deep Learning with Importance Sampling&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;Deep Neural Network training spends most of the computation on &lt;strong&gt;examples that are properly handled, and could be ignored&lt;/strong&gt;. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on &lt;strong&gt;“informative” examples&lt;/strong&gt;, and &lt;strong&gt;reduces the variance of the stochastic gradients&lt;/strong&gt; during training.&lt;/li&gt;
          &lt;li&gt;Our contribution is twofold: first, we derive a &lt;strong&gt;tractable upper bound to the per-sample gradient norm&lt;/strong&gt;, and second we &lt;strong&gt;derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup.&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;Recently, researchers have shifted their focus on using importance sampling to improve and accelerate the training of
  neural networks (Alain et al., 2015; Loshchilov &amp;amp; Hutter, 2015; Schaul et al., 2015). Those works, employ &lt;strong&gt;either the gradient norm or the loss to compute each sample’s importance.&lt;/strong&gt; However, the former is prohibitively expensive to compute and the latter is not a particularly good approximation of the gradient norm.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Firstly we provide an intuitive metric to predict how useful importance sampling is going to be, thus we are able to decide when to switch on importance sampling during training. Secondly, we also provide theoretical guarantees for speedup, when variance reduction is above a threshold.&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">IJCAI-2019</title><link href="http://localhost:4000/paperlists/2019-12-29-IJCAI/" rel="alternate" type="text/html" title="IJCAI-2019" /><published>2019-12-29T00:00:00+00:00</published><updated>2019-12-29T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/IJCAI</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-29-IJCAI/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;robustness&quot;&gt;Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ijcai.org/Proceedings/2019/0654.pdf&quot;&gt;Robustra: Training Provable Robust Neural Networks
over Reference Adversarial Space&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry><entry><title type="html">NeurIPS-2019</title><link href="http://localhost:4000/paperlists/2019-12-29-NeurIPS/" rel="alternate" type="text/html" title="NeurIPS-2019" /><published>2019-12-29T00:00:00+00:00</published><updated>2019-12-29T00:00:00+00:00</updated><id>http://localhost:4000/paperlists/NeurIPS</id><content type="html" xml:base="http://localhost:4000/paperlists/2019-12-29-NeurIPS/">&lt;p class=&quot;message&quot;&gt;:+1: means being highly related to my personal research interest.&lt;/p&gt;

&lt;h2 id=&quot;example-weighting-by-meta-learning-for-robustness&quot;&gt;Example Weighting by Meta Learning for Robustness&lt;/h2&gt;
&lt;ul class=&quot;message&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/8467-meta-weight-net-learning-an-explicit-mapping-for-sample-weighting.pdf&quot;&gt;Meta-Weight-Net: Learning an Explicit Mapping
For Sample Weighting&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;A following work of &lt;a href=&quot;http://127.0.0.1:4000/paperlists/2018-12-30-ICML/#example-weighting-by-meta-learning-gradient-directions&quot;&gt;https://xinshaoamoswang.github.io/paperlists/2018-12-30-ICML/#example-weighting-by-meta-learning-gradient-directions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;We propose to automatically learn an explicit loss-weight function, parameterized by an MLP from
data in a meta-learning manner. Due to the universal approximation capability of this weight net, it can finely fit a wide range of weighting functions including those used in conventional research.&lt;/li&gt;
      &lt;li&gt;The sample weights of those
samples better complying with the meta-data knowledge will be improved, while those violating such meta-knowledge will be suppressed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Xinshao Wang</name><email>xwang at qub dot ac dot uk xwang at qub dot ac dot uk</email></author><summary type="html">:+1: means being highly related to my personal research interest.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/steve-harvey.jpg" /></entry></feed>