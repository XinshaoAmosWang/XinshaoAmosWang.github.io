---
layout: post
title: arXiv-2020
description: >
  
image: /assets/img/blog/steve-harvey.jpg
comment: true
---

0. [Noisy labels ](#noisy-labels)
0. [Active Learning](#active-learning)
0. [Foundation of Deep Learning](#foundation-of-deep-learning)

:+1: means being highly related to my personal research interest. 
{:.message}


## Foundation of Deep Learning 
* [Neural Bayes: A Generic Parameterization Method for Unsupervised Representation Learning](https://arxiv.org/pdf/2002.09046.pdf)

* [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf)
    * Data augmentation: composition of data augmentations plays a critical role in defining effective predictive tasks;
    * Auxiliary transformation:  introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations;
    * Larger batch size and more training steps: contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning.
    * Results: By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy.


* [Instance Cross Entropy for Deep Metric Learning](https://arxiv.org/pdf/1911.09976.pdf) and its application in SimCLR-A Simple Framework for Contrastive Learning of Visual Representations

    * I am very glad to highlight that:  our proposed ICE is simple and effective, which has also been demonstrated in recent work SimCLR, in the context of self-supervised learning: A Simple Framework for Contrastive Learning of Visual Representations

    * Its loss expression NT-Xent (the normalized temperature-scaled cross entropy loss) is a fantastic application of our recently proposed Instance Cross Entropy for Deep Metric Learning,  in the context of self-supervised learnining. I am very excited about this.
        * #InstanceCrossEntropy #TemperatureScaling #RepresentationLearning
    * [Research Gate](https://www.researchgate.net/publication/337485049_Instance_Cross_Entropy_for_Deep_Metric_Learning/comments)
    * [Open Review](https://openreview.net/forum?id=BJeguTEKDB&noteId=txrrkCL-sXhttps://openreview.net/forum?id=BJeguTEKDB&noteId=txrrkCL-sX)
    * [Reddit](https://www.reddit.com/r/MachineLearning/comments/f4x1sh/r_instance_cross_entropy_for_deep_metric_learning/)
* [Unsupervised Embedding Learning via Invariant and Spreading Instance Feature](https://arxiv.org/pdf/1904.03436.pdf)
    * Unsupervised Embedding Learning via Invariant and SpreadingInstance Feature is even closer, also in the context of self-supervised learning, maximising the agreement over augmentations of one instance.
    * [ Instance Cross Entropy for Deep Metric Learning](https://arxiv.org/pdf/1911.09976.pdf) is in the context of supervised discriminative representation learning, maximising the agreement over augmentaions of multiple images i.e. a class.

* [ICLR 2019: Learning deep representations by mutual information estimation and maximization](https://xinshaoamoswang.github.io/Papers/AnomalyAndRegularisation/#iclr-2019-learning-deep-representations-by-mutual-information-estimation-and-maximization) 
{:.message}



## Noisy labels 
* [Learning Adaptive Loss for Robust Learning with Noisy Labels](https://arxiv.org/pdf/2002.06482.pdf)
* [DivideMix: Learning with Noisy Labels as Semi-supervised Learning](https://openreview.net/forum?id=HJgExaVtwr)
* [Group-Teaching: Learning Robust CNNs From Extremely Noisy Labels](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9001093)
* []
{:.message}

## Active Learning
* [Learning Active Learning from Data](https://papers.nips.cc/paper/7010-learning-active-learning-from-data.pdf)
{:.message}

