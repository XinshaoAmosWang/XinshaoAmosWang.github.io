---
layout: post
title: arXiv-2020
description: >
  
image: /assets/img/blog/steve-harvey.jpg
comment: true
---

:+1: means being highly related to my personal research interest. 
{:.message}


## Foundation of Deep Learning 
* [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf)
    * Data augmentation: composition of data augmentations plays a critical role in defining effective predictive tasks;
    * Auxiliary transformation:  introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations;
    * Larger batch size and more training steps: contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning.
    * Results: By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy.

Note that the proposed loss expression **NT-Xent (the normalized
temperature-scaled cross entropy loss)** has been proposed in [Instance Cross Entropy for Deep Metric Learning--ICLR2020 Submission Version](https://openreview.net/pdf?id=BJeguTEKDB), [arXiv Version](https://arxiv.org/pdf/1911.09976.pdf): cross entropy computation and dot product scaling. 
{:.message}




