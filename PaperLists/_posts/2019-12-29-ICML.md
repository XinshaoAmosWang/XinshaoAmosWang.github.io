---
layout: post
title: ICML-2019
description: >
  
image: /assets/img/blog/steve-harvey.jpg
comment: true
---

:+1: means being highly related to my personal research interest. 
{:.message}


## Label Noise 
* [Unsupervised Label Noise Modeling and Loss Correction](http://proceedings.mlr.press/v97/arazo19a/arazo19a.pdf) :+1:
    * A suitable two-component mixture model as an unsupervised generative model
of sample loss values during training to allow
online estimation of the probability that a sample is mislabelled. 
* [Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels](https://arxiv.org/pdf/1905.05040.pdf) 
    * We find that the test accuracy can be
quantitatively characterized in terms of the noise
ratio in datasets. The test accuracy
is a quadratic function of the noise ratio in the
case of symmetric noise, which explains the experimental findings previously published. (I am not convinced on this!)
    * DNNs tend to learn simple patterns
first, then gradually memorize all samples, which justifies
the widely used small-loss criteria: treating samples with
small training loss as clean ones (Han et al., 2018; Jiang
et al., 2018).
* [SELFIE: Refurbishing Unclean Samples for Robust Deep Learning](http://proceedings.mlr.press/v97/song19b/song19b.pdf)
{:.message}

## Robustness
* [Certified Adversarial Robustness via Randomized Smoothing](http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf)

