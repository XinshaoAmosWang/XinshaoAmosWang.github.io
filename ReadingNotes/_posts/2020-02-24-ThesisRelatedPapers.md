---
layout: post
title: Papers Related to My PhD Thesis-Example Weighting in Classification and Distance Metric Learning
description: >
  
#image: /assets/img/blog/steve-harvey.jpg
comment: true
---


0. [Learning to Retrieve](#distance-metric-learning-learning-to-retrieve)
0. [Learning to Classify ](#learning-to-classify)
0. [Others](#others)
{:.message}

## Distance Metric Learning: Learning to Retrieve 
* [Active Ordinal Querying for Tuplewise Similarity Learning](https://arxiv.org/pdf/1910.04115.pdf) 
* [Representation Learning with
Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748.pdf)
{:.message}



## Learning to Classify 
* 
{:.message}



## Others 
* [A general and Adaptive Robust Loss Function-CVPR 2019 Best Paper Finalist]()
* [Unsupervised Embedding Learning via Invariant and Spreading Instance Feature-CVPR 2019]()
* [Regularising Deep Neural Networks by Noise: Its Interpretation and Optimisation-NeurIPS 2017]()
* [Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks-NeurIPS 2017]()
* [Selfie: Refurbishing Unclean Samples for Robust Deep Learning-ICML 2019]()
* [Unsupervised Label Noise Modeling and Loss Correction-ICML 2019]()
* [Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise-NeurIPS 2018]()

## Example Weighting
* [Weighted Machine Learning-Mahdi Hashemi∗, Hassan A. Karimi](https://www.researchgate.net/publication/328731166_Weighted_Machine_Learning)
* [Not All Samples Are Created Equal: Deep Learning with Importance Sampling-Angelos Katharopoulos, Franc¸ois Fleuret, ICML 2018](http://proceedings.mlr.press/v80/katharopoulos18a.html)
    * computing the importance score for the whole
dataset is still prohibitive and would render the method
unsuitable for online learning.
    * In order to solve the problem of computing the importance for the whole dataset, we pre-sample a large batch of data points, compute the sampling distribution for that batch and re-sample a smaller batch with replacement. 
{:.message}


